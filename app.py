import streamlit as st
import pandas as pd
import numpy as np
import io
import json
import xlsxwriter
from docx import Document
from docx.shared import Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.shared import OxmlElement
from docx.oxml.ns import qn
import docx.opc.constants

# ==========================================
# PART 1: é…ç½®åŒºåŸŸ (ä¿®å¤äº†å­—æ®µæ˜ å°„)
# ==========================================

COMMON_METRICS = {
    "spend": ["èŠ±è´¹é‡‘é¢(USD)", "èŠ±è´¹é‡‘é¢ ï¼ˆUSDï¼‰", "èŠ±è´¹é‡‘é¢ (USD)", "èŠ±è´¹é‡‘é¢", "Amount Spent"],
    "roas": ["å¹¿å‘ŠèŠ±è´¹å›æŠ¥ (ROAS) - è´­ç‰©", "å¹¿å‘ŠèŠ±è´¹å›æŠ¥ï¼ˆROASï¼‰-è´­ç‰©", "ROAS", "Purchase ROAS"],
    "purchases": ["è´­ä¹°æ¬¡æ•°", "æˆæ•ˆæ•°é‡", "æˆæ•ˆ", "Purchases"],
    "cpa": ["å•æ¬¡è´­ä¹°è´¹ç”¨", "å•æ¬¡è´­ç‰©æˆæœ¬", "å•æ¬¡æˆæ•ˆæˆæœ¬", "å•æ¬¡æˆæ•ˆè´¹ç”¨", "Cost per Purchase"],
    "ctr": ["é“¾æ¥ç‚¹å‡»ç‡", "é“¾æ¥ç‚¹å‡»ç‡ï¼ˆ%)", "é“¾æ¥ç‚¹å‡»ç‡ï¼ˆ%ï¼‰", "CTR"],
    "cpm": ["åƒæ¬¡å±•ç¤ºè´¹ç”¨", "CPM"],
    "clicks": ["ç‚¹å‡»", "é“¾æ¥ç‚¹å‡»", "Clicks"],
    "impressions": ["æ›å…‰", "å±•ç¤ºæ¬¡æ•°", "Impressions"],
    "purchase_value": ["è´­ä¹°ä»·å€¼", "è´­ç‰©ä»·å€¼", "Purchase Value"],
    "aov": ["å•æ¬¡è´­ä¹°ä»·å€¼", "å•æ¬¡è´­ç‰©ä»·å€¼"]
}

# æ¡†å®šã€Œæ¯ä¸€ä¸ª Sheetã€éœ€è¦æŠ½å–å“ªäº›æŒ‡æ ‡
# âœ… ä¿®å¤ï¼šåœ¨"å—ä¼—ç»„"ä¸­å¢åŠ äº† converting_countries/genders/ages æ˜ å°„
SHEET_MAPPINGS = {
    "æ•´ä½“æ•°æ®": {
        **COMMON_METRICS,
        "date_range": ["æ—¶é—´èŒƒå›´"],
        "clicks_all": ["ç‚¹å‡»"],
        "landing_page_views": ["è½åœ°é¡µæµè§ˆé‡"],
        "add_to_cart": ["åŠ å…¥è´­ç‰©è½¦"],
        "initiate_checkout": ["ç»“è´¦å‘èµ·æ¬¡æ•°"],
        "rate_click_to_lp": ["ç‚¹å‡»-è½åœ°é¡µæµè§ˆè½¬åŒ–ç‡"],
        "rate_lp_to_atc": ["è½åœ°é¡µæµè§ˆ-åŠ è´­è½¬åŒ–ç‡"],
        "rate_atc_to_ic": ["åŠ è´­-ç»“è´¦è½¬åŒ–ç‡"],
        "rate_ic_to_pur": ["ç»“è´¦-è´­ä¹°è½¬åŒ–ç‡"]
    },
    "åˆ†æ—¶æ®µæ•°æ®": {
        **COMMON_METRICS,
        "date_range": ["æ—¶é—´èŒƒå›´"],
        "landing_page_views": ["è½åœ°é¡µæµè§ˆé‡"],
        "add_to_cart": ["åŠ å…¥è´­ç‰©è½¦"],
        "initiate_checkout": ["ç»“è´¦å‘èµ·æ¬¡æ•°"],
        "rate_click_to_lp": ["ç‚¹å‡»-è½åœ°é¡µæµè§ˆè½¬åŒ–ç‡"],
        "rate_lp_to_atc": ["è½åœ°é¡µæµè§ˆ-åŠ è´­è½¬åŒ–ç‡"],
        "rate_atc_to_ic": ["åŠ è´­-ç»“è´¦è½¬åŒ–ç‡"],
        "rate_ic_to_pur": ["ç»“è´¦-è´­ä¹°è½¬åŒ–ç‡"]
    },
    "å¼‚å¸¸æŒ‡æ ‡": {
        "anomaly_metric_name": ["å¼‚å¸¸æŒ‡æ ‡"],
        "mom_change": ["ç¯æ¯”"]
    },
    "å¹¿å‘Šæ¶æ„": {**COMMON_METRICS, "dimension_item": ["å¹¿å‘Šç±»å‹"]},
    "å—ä¼—ç»„": {
        **COMMON_METRICS,
        "dimension_item": ["å¹¿å‘Šç»„", "å¹¿å‘Šç»„Id", "Ad Set Name"],
        "custom_audience_settings": ["è®¾ç½®çš„è‡ªå®šä¹‰å—ä¼—", "Custom Audiences"],
        "converting_keywords": ["äº§ç”Ÿæˆæ•ˆçš„å…³é”®è¯", "Interests", "Keywords"],
        # âœ… æ–°å¢ä»¥ä¸‹ä¸‰è¡Œï¼Œç¡®ä¿ä»Excelä¸­è¯»å–è¿™äº›åˆ—
        "converting_countries": ["äº§ç”Ÿæˆæ•ˆçš„å›½å®¶", "å›½å®¶", "åœ°åŒº", "Country", "Region", "Location"],
        "converting_genders": ["äº§ç”Ÿæˆæ•ˆçš„æ€§åˆ«", "æ€§åˆ«", "Gender"],
        "converting_ages": ["äº§ç”Ÿæˆæ•ˆçš„å¹´é¾„", "å¹´é¾„", "Age", "Age Group"]
    },
    "å—ä¼—ç±»å‹": {**COMMON_METRICS, "dimension_item": ["å—ä¼—ç±»å‹"]},
    "å›½å®¶": {**COMMON_METRICS, "dimension_item": ["å›½å®¶/åœ°åŒº", "å›½å®¶"]},
    "å¹´é¾„": {**COMMON_METRICS, "dimension_item": ["å¹´é¾„"]},
    "æ€§åˆ«": {**COMMON_METRICS, "dimension_item": ["æ€§åˆ«"]},
    "å¹³å°&ç‰ˆä½": {**COMMON_METRICS, "dimension_item": ["å¹³å°&ç‰ˆä½"]},
    "ç´ æ": {
        **COMMON_METRICS,
        "content_item": ["ç´ æ"],
        "cvr_lp_to_pur": ["è½åœ°é¡µæµè§ˆ-è´­ä¹°è½¬åŒ–ç‡"]
    },
    "è½åœ°é¡µ": {
        **COMMON_METRICS,
        "content_item": ["è½åœ°é¡µurl", "è½åœ°é¡µ"],
        "ctr_all": ["æ›å…‰-ç‚¹å‡»è½¬åŒ–ç‡"],
        "rate_lp_to_atc": ["è½åœ°é¡µæµè§ˆ-åŠ è´­è½¬åŒ–ç‡", "è½åœ°é¡µæµè§ˆ-è´­ç‰©è½¬åŒ–ç‡"]
    }
}

GROUP_CONFIG = {
    "Master_Overview": ["æ•´ä½“æ•°æ®", "åˆ†æ—¶æ®µæ•°æ®", "å¼‚å¸¸æŒ‡æ ‡"],
    "Master_Breakdown": ["å¹¿å‘Šæ¶æ„", "å—ä¼—ç»„", "å—ä¼—ç±»å‹", "å›½å®¶", "å¹´é¾„", "æ€§åˆ«", "å¹³å°&ç‰ˆä½"],
    "Master_Creative": ["ç´ æ", "è½åœ°é¡µ"]
}

REPORT_MAPPING = {
    "spend": "èŠ±è´¹ ($)", "roas": "ROAS", "purchases": "è´­ä¹°æ¬¡æ•°", "purchase_value": "è´­ä¹°æ€»ä»·å€¼",
    "cpa": "CPA ($)", "ctr": "CTR (%)", "cpm": "CPM ($)", "aov": "å®¢å•ä»·",
    "impressions": "å±•ç°é‡", "clicks_all": "ç‚¹å‡»é‡ (All)", "clicks": "ç‚¹å‡»é‡ (All)", "ctr_all": "ç‚¹å‡»ç‡ (All)",
    "landing_page_views": "è½åœ°é¡µè®¿é—®é‡", "add_to_cart": "åŠ è´­æ¬¡æ•°", "initiate_checkout": "ç»“è´¦å‘èµ·æ•° (IC)",
    "rate_click_to_lp": "ç‚¹å‡» â†’ è½åœ°é¡µè®¿é—®è½¬åŒ–ç‡", "rate_lp_to_atc": "è½åœ°é¡µ â†’ åŠ è´­è½¬åŒ–ç‡",
    "rate_atc_to_ic": "åŠ è´­ â†’ è´­ä¹°è½¬åŒ–ç‡", "rate_ic_to_pur": "è´­ä¹°è½¬åŒ–ç‡",
    "cvr_purchase": "ç‚¹å‡» â†’ è´­ä¹°è½¬åŒ–ç‡", "cvr_lp_to_pur": "CVR (å…¨ç«™è½¬åŒ–ç‡)",
    "date_range": "æ—¥æœŸ/æ—¶æ®µ", "campaign_type": "æŠ•æ”¾æ¨¡å¼", "adset_name": "å¹¿å‘Šç»„ID", "adset_id": "å¹¿å‘Šç»„ID",
    "custom_audience_settings": "è‡ªå®šä¹‰å—ä¼—æº", "converting_keywords": "é«˜æ½œå…´è¶£è¯", "audience_type": "å—ä¼—ç­–ç•¥",
    "country": "å›½å®¶", "age_group": "å¹´é¾„", "gender": "æ€§åˆ«", "creative_name": "ç´ æåç§°", "placement": "ç‰ˆä½",
    "landing_page_url": "é¡µé¢ URL", "mom_change": "ç¯æ¯”æ³¢åŠ¨", "anomaly_metric_name": "å¼‚å¸¸é¡¹",
    "converting_countries": "äº§ç”Ÿæˆæ•ˆçš„å›½å®¶", "converting_genders": "äº§ç”Ÿæˆæ•ˆçš„æ€§åˆ«", "converting_ages": "äº§ç”Ÿæˆæ•ˆçš„å¹´é¾„"
}

# âœ… å¢å¼ºäº†æ¨¡ç³ŠåŒ¹é…åˆ«å
FIELD_ALIASES = {
    "adset_id": ["adset_id", "ad set id", "adset id", "å¹¿å‘Šç»„ç¼–å·", "å¹¿å‘Šç»„id", "adset_name", "ad set name"],
    "converting_countries": ["converting_countries", "country", "region", "å›½å®¶", "åœ°åŒº", "location"],
    "converting_genders": ["converting_genders", "gender", "æ€§åˆ«"],
    "converting_ages": ["converting_ages", "age", "å¹´é¾„", "age_group"],
    "converting_keywords": ["converting_keywords", "keywords", "interests", "å…´è¶£", "å…³é”®è¯"],
    "spend": ["spend", "amount spent", "cost", "èŠ±è´¹", "æ¶ˆè€—"],
    "purchases": ["purchases", "results", "result", "æˆæ•ˆ", "è´­ä¹°"],
    "roas": ["roas", "return on ad spend", "purchase roas"],
    "purchase_value": ["purchase_value", "conversion value", "value", "æ€»ä»·å€¼", "gmv", "è´­ä¹°æ€»ä»·å€¼"],
    "clicks": ["clicks", "clicks (all)", "ç‚¹å‡»é‡", "clicks_all"],
    "impressions": ["impressions", "å±•ç¤º", "å±•ç°"],
    "ctr_all": ["ctr_all", "ctr (all)", "ç‚¹å‡»ç‡ (all)"]
}


# ==========================================
# PART 2: æ ¸å¿ƒå·¥å…·å‡½æ•° (ä¿æŒä¸å˜ï¼Œç•¥)
# ==========================================

# ==========================================
# PART 2: æ ¸å¿ƒå·¥å…·å‡½æ•° (å·²ä¿®å¤ç™¾åˆ†æ¯”è¯†åˆ«é—®é¢˜)
# ==========================================

def parse_float(value):
    """è¾…åŠ©å‡½æ•°ï¼šæ¸…ç†æ•°æ®å¹¶å°†å­—ç¬¦ä¸²/æ•°å­—å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
    if value is None:
        return 0.0
    try:
        # å¦‚æœå·²ç»æ˜¯æ•°å­—ï¼Œç›´æ¥è¿”å›
        if isinstance(value, (int, float)):
            return float(value)
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè°ƒç”¨ clean_numeric_strict è¿›è¡Œæ ‡å‡†å¤„ç†
        return clean_numeric_strict(value)
    except (ValueError, TypeError):
        return 0.0

def safe_div(numerator, denominator, multiplier=1.0):
    n = parse_float(numerator)
    d = parse_float(denominator)
    if d > 0:
        return (n / d) * multiplier
    else:
        return 0.0

# å®½æ¾æ¸…æ´—ï¼ˆç”¨äºå±•ç¤ºï¼‰
def clean_numeric(val):
    if pd.isna(val): return 0.0
    if isinstance(val, (int, float)): return float(val)
    val_str = str(val).strip().replace('$', '').replace('Â¥', '').replace(',', '')
    
    # âœ… ä¿®å¤ç‚¹ 1ï¼šå¦‚æœæ˜¯ç™¾åˆ†æ•°å­—ç¬¦ä¸²ï¼Œè½¬æ¢åé™¤ä»¥ 100
    if '%' in val_str: 
        val_str = val_str.replace('%', '')
        try: return float(val_str) / 100.0 
        except: return 0.0
        
    try: return float(val_str)
    except: return val

# ä¸¥æ ¼æ¸…æ´—ï¼ˆç”¨äºè®¡ç®—ï¼‰
def clean_numeric_strict(val): 
    if pd.isna(val): return 0.0
    # å¦‚æœå·²ç»æ˜¯æ•°å­—ï¼Œç›´æ¥è¿”å›
    if isinstance(val, (int, float)): return float(val)
    
    val_str = str(val).strip().replace('$', '').replace('Â¥', '').replace(',', '')
    
    # âœ… ä¿®å¤ç‚¹ 2ï¼šå¦‚æœæ˜¯ç™¾åˆ†æ•°å­—ç¬¦ä¸²ï¼ˆå¦‚ "2.31%"ï¼‰ï¼Œå»é™¤%åé™¤ä»¥100è¿˜åŸä¸ºå°æ•°ï¼ˆ0.0231ï¼‰
    if '%' in val_str: 
        val_str = val_str.replace('%', '')
        try: return float(val_str) / 100.0
        except: return 0.0
        
    try: return float(val_str)
    except: return 0.0

# å­—æ®µé²æ£’æ ¸å¿ƒ
def find_column_fuzzy(df, keywords):
    for kw in keywords:
        if kw in df.columns: return kw
    df_cols_norm = {c.lower().replace(' ', '').replace('_', ''): c for c in df.columns}
    for kw in keywords:
        kw_norm = kw.lower().replace(' ', '').replace('_', '')
        if kw_norm in df_cols_norm: return df_cols_norm[kw_norm]
    for col in df.columns:
        col_lower = col.lower()
        for kw in keywords:
            if kw.lower() in col_lower: return col
    return None

# æ ¸å¿ƒæŒ‡æ ‡è®¡ç®— (ä¿æŒä¸å˜)
def calc_metrics_dict(df_chunk):
    res = {}
    if df_chunk.empty: return res
    sums = {}
    targets = ['spend', 'clicks', 'impressions', 'purchases', 'purchase_value',
               'landing_page_views', 'add_to_cart', 'initiate_checkout']
    
    for t in targets:
        aliases = FIELD_ALIASES.get(t, [t])
        if t == 'purchase_value' and 'value' not in aliases: aliases.append('value')
        col = find_column_fuzzy(df_chunk, aliases)
        if col:
             sums[t] = df_chunk[col].apply(clean_numeric_strict).sum()
        else:
             sums[t] = 0.0

    res['spend'] = parse_float(sums.get('spend', 0))
    res['impressions'] = parse_float(sums.get('impressions', 0))
    res['clicks'] = parse_float(sums.get('clicks', 0))
    res['purchases'] = parse_float(sums.get('purchases', 0))
    res['purchase_value'] = parse_float(sums.get('purchase_value', 0))
    res['roas'] = safe_div(sums.get('purchase_value'), sums.get('spend'))
    res['cpm'] = safe_div(sums.get('spend'), sums.get('impressions'), multiplier=1000)
    res['cpc'] = safe_div(sums.get('spend'), sums.get('clicks'))
    res['ctr'] = safe_div(sums.get('clicks'), sums.get('impressions'))
    res['cpa'] = safe_div(sums.get('spend'), sums.get('purchases'))
    res['cvr_purchase'] = safe_div(sums.get('purchases'), sums.get('clicks'))
    res['rate_click_to_lp'] = safe_div(sums.get('landing_page_views'), sums.get('clicks'))
    res['rate_lp_to_atc']   = safe_div(sums.get('add_to_cart'), sums.get('landing_page_views'))
    res['rate_atc_to_ic']   = safe_div(sums.get('initiate_checkout'), sums.get('add_to_cart'))
    res['rate_ic_to_pur']   = safe_div(sums.get('purchases'), sums.get('initiate_checkout'))
    res['aov'] = safe_div(sums.get('purchase_value'), sums.get('purchases'))

    date_col = find_column_fuzzy(df_chunk, ['date', 'time', 'range'])
    if date_col:
        try:
            dates = pd.to_datetime(df_chunk[date_col], errors='coerce').dropna()
            if not dates.empty: res['date_range'] = f"{dates.min():%Y-%m-%d} ~ {dates.max():%Y-%m-%d}"
            else: res['date_range'] = "-"
        except: res['date_range'] = "-"
    else: res['date_range'] = "-"
    return res 

def format_cell(key, val, is_mom=False):
    if isinstance(val, str): return val
    if is_mom:
        if key == 'date_range': return val
        return f"{val:+.2%}"
    k = str(key).lower()
    if 'roas' in k: return f"{val:.2f}"
    if any(x in k for x in ['rate', 'ctr', 'cvr', 'ç‚¹å‡»ç‡', 'è½¬åŒ–ç‡', 'ç€é™†ç‡', 'æ„å‘ç‡', 'æˆåŠŸç‡']): 
        # è¿™é‡Œä¼šä¹˜ä»¥100ï¼Œæ‰€ä»¥è¾“å…¥å¿…é¡»æ˜¯å°æ•° (0.0231 -> 2.31%)
        return f"{val:.2%}" 
    if any(x in k for x in ['spend', 'cpm', 'cpc', 'value', 'aov', 'cpa', 'èŠ±è´¹', 'é‡‘é¢', 'å®¢å•ä»·', 'gmv', 'ä»·å€¼']): return f"{val:,.2f}"
    if any(x in k for x in ['purchases', 'cart', 'click', 'æ¬¡æ•°', 'å•é‡', 'ç‚¹å‡»', 'å±•ç°', 'è®¿é—®é‡', 'å‘èµ·æ•°']): return f"{val:,.0f}"
    return f"{val}"

def extract_benchmark_values(df_bench):
    targets = {'roas': (['roas'], True), 'cpm': (['cpm'], False), 'ctr': (['ctr'], True), 'cpc': (['cpc'], False), 'cpa': (['cpa_purchase', 'cpa'], False)}
    extracted = {}
    for metric, (aliases, higher_better) in targets.items():
        found_col = None
        for alias in aliases:
            found_col = find_column_fuzzy(df_bench, [alias])
            if found_col: break
        if found_col:
            try:
                s = df_bench[found_col].apply(clean_numeric_strict)
                v = s[s>0].mean()
                
                # âœ… ä¿®å¤ç‚¹ 3ï¼šé˜²å¾¡æ€§é€»è¾‘
                # å¦‚æœæ˜¯ CTR/CVR ç­‰æ¯”ç‡ç±»æŒ‡æ ‡ï¼Œä¸”åŸºå‡†å€¼ > 1.0 (ä¾‹å¦‚ç”¨æˆ·å¡«äº† 2.31 è€Œä¸æ˜¯ 0.0231)ï¼Œ
                # ä¸”è¯¥åˆ—ä¸æ˜¯ CPA/CPM/ROAS/CPC è¿™ç§æœ¬èº«å°±å¾ˆå¤§çš„å€¼ï¼Œåˆ™å¼ºåˆ¶é™¤ä»¥100
                if metric in ['ctr'] and v > 1.0:
                    v = v / 100.0
                    
                if not pd.isna(v): extracted[metric] = [v, higher_better]
            except: pass
    return extracted

# ... (add_hyperlink, apply_report_labels, add_df_to_word ä¿æŒä¸å˜)

def add_hyperlink(paragraph, url, text, color="0000FF", underline=True):
    try:
        part = paragraph.part
        r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)
        hyperlink = OxmlElement('w:hyperlink')
        hyperlink.set(qn('r:id'), r_id)
        new_run = OxmlElement('w:r')
        rPr = OxmlElement('w:rPr')
        if color:
            c = OxmlElement('w:color')
            c.set(qn('w:val'), color)
            rPr.append(c)
        if underline:
            u = OxmlElement('w:u')
            u.set(qn('w:val'), 'single')
            rPr.append(u)
        new_run.append(rPr)
        new_run.text = text
        hyperlink.append(new_run)
        paragraph._p.append(hyperlink)
        return hyperlink
    except: return None

def apply_report_labels(df, custom_mapping=None):
    if df.empty: return df
    mapping = REPORT_MAPPING.copy()
    if custom_mapping: mapping.update(custom_mapping)
    return df.rename(columns=mapping)

def add_df_to_word(doc, df, title, level=1):
    if df.empty: return
    doc.add_heading(title, level=level)
    t = doc.add_table(rows=df.shape[0]+1, cols=df.shape[1])
    t.style = 'Table Grid'
    is_creative = "ç´ æ" in title
    is_landing = "è½åœ°é¡µ" in title
    link_col_idx = -1
    for j, col in enumerate(df.columns):
        cell = t.cell(0, j)
        cell.text = str(col)
        if any(x in str(col).lower() for x in ["url", "link", "ç´ æ", "å†…å®¹", "content"]): link_col_idx = j
        for p in cell.paragraphs:
            for r in p.runs:
                r.font.bold = True
                r.font.size = Pt(8)
    for i in range(df.shape[0]):
        label_prefix = "ç´ æ" if is_creative else ("è½åœ°é¡µ" if is_landing else "")
        label_char = chr(65 + (i % 26))
        if i >= 26: label_char += str(i // 26)
        label_text = f"{label_prefix}{label_char}"
        for j in range(df.shape[1]):
            val = df.iat[i, j]
            cell = t.cell(i+1, j)
            if (is_creative or is_landing) and j == link_col_idx:
                try:
                    p = cell.paragraphs[0]
                    url = str(val).strip()
                    if len(url) > 5: add_hyperlink(p, url, label_text)
                    else: cell.text = label_text
                except: cell.text = label_text
            else:
                cell.text = str(val)
                if "ç»“è®º" in str(df.columns[j]):
                    if "âœ…" in str(val): cell.paragraphs[0].runs[0].font.color.rgb = RGBColor(0, 128, 0)
                    if "âš ï¸" in str(val): cell.paragraphs[0].runs[0].font.color.rgb = RGBColor(255, 0, 0)
            for p in cell.paragraphs:
                for r in p.runs: r.font.size = Pt(8)
    doc.add_paragraph("\n")

# ==========================================
# PART 3: ä¸»é€»è¾‘ç±» (Process ETL ä¸­åŒ…å«äº†å…³é”®ä¿®å¤)
# ==========================================

class AdReportProcessor:
    def __init__(self, raw_file, bench_file=None):
        self.raw_file = raw_file
        self.bench_file = bench_file
        self.processed_dfs = {}
        self.merged_dfs = {}
        self.final_json = {}
        self.doc = Document()

    # --- é˜¶æ®µ 1: æ•°æ®æ¸…æ´—ä¸é™ç»´ ---
    def process_etl(self):
        xls = pd.ExcelFile(self.raw_file)
        for sheet_name, mapping in SHEET_MAPPINGS.items():
            if sheet_name in xls.sheet_names:
                df = pd.read_excel(xls, sheet_name=sheet_name)
                final_cols = {}
                # å­—æ®µæ˜ å°„
                for std_col, raw_col_options in mapping.items():
                    matched_col = None
                    for option in raw_col_options:
                        if option in df.columns: matched_col = option; break
                        if not matched_col:
                            for df_col in df.columns:
                                if option.replace(" ", "") == df_col.replace(" ", ""): matched_col = df_col; break
                        if matched_col: break
                    if matched_col: final_cols[std_col] = matched_col

                if final_cols:
                    df_clean = df[list(final_cols.values())].rename(columns={v: k for k, v in final_cols.items()})
                    
                    # âœ… ä¿®å¤é‡ç‚¹ï¼šå°† converting_countries ç­‰åˆ—åŠ å…¥ã€Œä¸è¿›è¡Œæ•°å­—æ¸…æ´—ã€çš„ç™½åå•
                    text_cols = ['date_range', 'anomaly_metric_name', 
                                 'converting_keywords', 'converting_countries', 'converting_genders', 'converting_ages', 
                                 'custom_audience_settings', 'dimension_item', 'content_item']
                    
                    for col in df_clean.columns:
                        if col not in text_cols:
                            df_clean[col] = df_clean[col].apply(clean_numeric)

                    if sheet_name in ["ç´ æ", "è½åœ°é¡µ", "å—ä¼—ç»„"]:
                        if "spend" in df_clean.columns:
                            df_clean = df_clean.sort_values("spend", ascending=False).head(10)

                    df_clean["Source_Sheet"] = sheet_name
                    self.processed_dfs[sheet_name] = df_clean

        # åˆå¹¶ Master Tables
        for master_name, source_sheets in GROUP_CONFIG.items():
            dfs_to_merge = [self.processed_dfs[src] for src in source_sheets if src in self.processed_dfs]
            if dfs_to_merge:
                merged_df = pd.concat(dfs_to_merge, ignore_index=True)
                cols = list(merged_df.columns)
                priority_cols = ['Source_Sheet', 'date_range', 'dimension_item', 'content_item',
                                 'spend', 'roas', 'purchases', 'cpa']
                new_order = [c for c in priority_cols if c in cols] + [c for c in cols if c not in priority_cols]
                self.merged_dfs[master_name] = merged_df[new_order]

    # --- é˜¶æ®µ 2: æŠ¥å‘Šç”Ÿæˆä¸æ¶æ„è¯Šæ–­ ---
    def generate_report(self):
        benchmark_targets = {'roas': [2.0, True], 'cpm': [20.0, False], 'ctr': [0.015, True], 'cpc': [1.5, False], 'cpa': [30.0, False]}
        if self.bench_file:
            try:
                df_b = pd.read_excel(self.bench_file)
                benchmark_targets = extract_benchmark_values(df_b)
            except: pass

        self.doc.add_heading('å¹¿å‘ŠæŠ•æ”¾æ·±åº¦åˆ†ææŠ¥å‘Š', 0).alignment = WD_ALIGN_PARAGRAPH.CENTER
        self.final_json = {"report_title": "å¹¿å‘ŠæŠ•æ”¾æ·±åº¦åˆ†ææŠ¥å‘Š", "generated_at": pd.Timestamp.now().strftime("%Y-%m-%d")}

        # 1. å¤§ç›˜æ€»è§ˆ (ä¿æŒä¸å˜)
        df_ov = pd.DataFrame()
        if "Master_Overview" in self.merged_dfs:
            df_src = self.merged_dfs["Master_Overview"]
            mask = df_src['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["åˆ†æ—¶", "Time"]))
            df_ov = df_src[mask].copy() if not df_src[mask].empty else df_src.copy()

        if not df_ov.empty:
            date_col = find_column_fuzzy(df_ov, ['date', 'time', 'æ—¶é—´'])
            if date_col:
                try:
                    df_ov['temp_date'] = pd.to_datetime(df_ov[date_col], errors='coerce')
                    df_clean = df_ov.dropna(subset=['temp_date']).sort_values('temp_date')
                    dates = df_clean['temp_date'].unique()
                    raw_overall = calc_metrics_dict(df_clean)
                    if len(dates) >= 2:
                        mid_date = dates[len(dates)//2]
                        raw_prev = calc_metrics_dict(df_clean[df_clean['temp_date'] < mid_date])
                        raw_curr = calc_metrics_dict(df_clean[df_clean['temp_date'] >= mid_date])
                        raw_mom = {}
                        for k, v_curr in raw_curr.items():
                            if k == 'date_range': raw_mom[k] = "-"
                            else:
                                v_prev = raw_prev.get(k, 0)
                                raw_mom[k] = (v_curr - v_prev) / v_prev if v_prev > 0 else 0.0
                    else:
                        raw_prev = {k: "-" for k in raw_overall}; raw_curr = raw_overall; raw_mom = {k: "-" for k in raw_overall}

                    col_order = ["date_range", "spend", "roas", "cpa", "cpm", "cpc", "ctr", "cvr_purchase",
                                 "rate_click_to_lp", "rate_lp_to_atc", "rate_ic_to_pur", "aov", "add_to_cart", "purchases", "purchase_value"]
                    final_data = []
                    for label, r in zip(["æ•´ä½“æ•°æ®", "ä¸Šå‘¨æœŸå€¼", "æœ¬å‘¨æœŸ", "ç¯æ¯”"], [raw_overall, raw_prev, raw_curr, raw_mom]):
                        row = {"Label": label}
                        is_m = (label == "ç¯æ¯”")
                        for c in col_order: row[c] = format_cell(c, r.get(c, 0), is_mom=is_m)
                        row['date_range'] = label
                        final_data.append(row)

                    df_f = pd.DataFrame(final_data, columns=col_order)
                    df_f_display = apply_report_labels(df_f)
                    add_df_to_word(self.doc, df_f_display, "1. æ•°æ®å¤§ç›˜æ€»è§ˆ", level=1)
                    self.final_json['1_data_overview'] = df_f.to_dict(orient='records')

                    # 2. Benchmark
                    raw_current = calc_metrics_dict(df_clean)
                    bench_data = []
                    for metric_key in ['roas', 'cpm', 'ctr', 'cpc', 'cpa']:
                        curr_val = raw_current.get(metric_key, 0)
                        bench_val, higher_is_better = benchmark_targets.get(metric_key, [0, True])
                        conclusion = "-"
                        if curr_val != 0:
                            diff = curr_val - bench_val
                            if higher_is_better: conclusion = "âœ… ä¼˜äºå¤§ç›˜" if diff > 0 else ("âš ï¸ ä½äºå¤§ç›˜" if diff < 0 else "æŒå¹³")
                            else: conclusion = "âœ… ä¼˜äºå¤§ç›˜" if diff < 0 else ("âš ï¸ é«˜äºå¤§ç›˜" if diff > 0 else "æŒå¹³")
                        bench_data.append({
                            "æŒ‡æ ‡": REPORT_MAPPING.get(metric_key, metric_key.upper()),
                            "å½“å‰è´¦æˆ·": format_cell(metric_key, curr_val),
                            "è¡Œä¸šåŸºå‡†": format_cell(metric_key, bench_val),
                            "å¯¹æ¯”ç»“è®º": conclusion
                        })
                    df_b = pd.DataFrame(bench_data)
                    add_df_to_word(self.doc, df_b, "2. è¡Œä¸š Benchmark å¯¹æ¯”", level=1)
                    self.final_json['2_industry_benchmark'] = df_b.to_dict(orient='records')
                except Exception as e: st.warning(f"å¤§ç›˜è®¡ç®—è­¦å‘Š: {e}")

        # 3. å—ä¼—ç»„
        self.doc.add_heading("3. å—ä¼—ç»„åˆ†æ", level=1)
        self.final_json['3_audience_analysis'] = {}
        audience_configs = [
            ("3.1 å›½å®¶åˆ†æ", ["å›½å®¶", "Country"], True, "å›½å®¶"),
            ("3.2 æ€§åˆ«åˆ†æ", ["æ€§åˆ«", "Gender"], False, "æ€§åˆ«"),
            ("3.3 å¹´é¾„åˆ†æ", ["å¹´é¾„", "Age"], False, "å¹´é¾„æ®µ"),
            ("3.4 å—ä¼—ç»„åˆ†æè¡¨", ["å—ä¼—", "Audience"], True, "å—ä¼—ç»„åç§°"),
        ]

        if "Master_Breakdown" in self.merged_dfs:
            df_bd = self.merged_dfs["Master_Breakdown"]
            for title, keywords, top10, dim_label in audience_configs:
                mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in keywords))
                df_curr = df_bd[mask].copy()
                if not df_curr.empty:
                    if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan) if 'clicks' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['cpm']): df_curr['cpm'] = (df_curr['spend'] / df_curr['impressions'].replace(0, np.nan)) * 1000 if 'impressions' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan) if 'impressions' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan) if 'purchases' in df_curr else 0

                    req_cols = ["dimension_item", "spend", "ctr", "cpc", "cpm", "cpa", "roas"]
                    # âœ… ç°åœ¨ï¼Œå¦‚æœæ˜¯å—ä¼—è¡¨ï¼Œè¿™äº›å­—æ®µå·²ç»è¢«ä¿ç•™ä¸‹æ¥äº†
                    if "å—ä¼—" in title: req_cols += ["converting_countries", "converting_keywords", "converting_genders", "converting_ages"]

                    rename_map = {}; valid_cols = []
                    for req in req_cols:
                        aliases = FIELD_ALIASES.get(req, [req])
                        found = find_column_fuzzy(df_curr, aliases)
                        if found: valid_cols.append(found); rename_map[found] = req
                        else: 
                            # å¯¹äºæ–‡æœ¬å­—æ®µï¼Œç»™ "-" è€Œä¸æ˜¯ 0.0
                            default_val = "-" if "converting" in req else 0.0
                            df_curr[req] = default_val; valid_cols.append(req)

                    df_final = df_curr[valid_cols].rename(columns=rename_map)
                    if "dimension_item" in df_final.columns:
                         df_final = df_final[~df_final['dimension_item'].astype(str).str.lower().str.contains('unknow', na=False)]

                    if top10 and 'spend' in df_final.columns: df_final = df_final.sort_values('spend', ascending=False).head(10)
                    df_clean = df_final.round(2)
                    df_display = apply_report_labels(df_clean, custom_mapping={'dimension_item': dim_label})
                    add_df_to_word(self.doc, df_display, title, level=2)
                    self.final_json['3_audience_analysis'][title] = df_clean.to_dict(orient='records')

        # 4. ç´ æä¸è½åœ°é¡µ (ä¿æŒä¸å˜)
        if "Master_Creative" in self.merged_dfs:
            df_cr = self.merged_dfs["Master_Creative"]
            for title, keywords, label, json_key in [("4. ç´ æåˆ†æ", ["ç´ æ", "Creative"], "ç´ æåç§°", "4_creative_analysis"), ("6. è½åœ°é¡µåˆ†æ", ["è½åœ°é¡µ", "Landing"], "è½åœ°é¡µ URL", "6_landing_page_analysis")]:
                mask = df_cr['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in keywords))
                df_curr = df_cr[mask].copy()
                if not df_curr.empty:
                    if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan) if 'clicks' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan) if 'purchases' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['ctr']):
                         if 'impressions' in df_curr and 'clicks' in df_curr: df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan)
                         else: df_curr['ctr'] = np.nan
                    if 'cpc' in df_curr.columns and 'cpm' in df_curr.columns:
                        mask_fix = (df_curr['ctr'].isna() | (df_curr['ctr'] == 0)) & (df_curr['cpc'] > 0)
                        if mask_fix.any(): df_curr.loc[mask_fix, 'ctr'] = df_curr.loc[mask_fix, 'cpm'] / (df_curr.loc[mask_fix, 'cpc'] * 1000)
                    df_curr['ctr'] = df_curr['ctr'].fillna(0) * 100 

                    req_cols = ["content_item", "spend", "ctr", "cpc", "cpm", "roas", "cpa"]
                    rename_map = {}; valid_cols = []
                    for req in req_cols:
                        aliases = FIELD_ALIASES.get(req, [req])
                        found = find_column_fuzzy(df_curr, aliases)
                        if found: valid_cols.append(found); rename_map[found] = req
                        else: df_curr[req] = 0.0; valid_cols.append(req)
                    df_final = df_curr[valid_cols].rename(columns=rename_map)
                    if 'spend' in df_final.columns: df_final = df_final.sort_values('spend', ascending=False).head(10)
                    df_clean = df_final.round(2) 
                    df_display = apply_report_labels(df_clean, custom_mapping={'content_item': label})
                    add_df_to_word(self.doc, df_display, title, level=1)
                    self.final_json[json_key] = df_clean.to_dict(orient='records')
                    
        # 5. ç‰ˆä½ (ä¿æŒä¸å˜)
        if "Master_Breakdown" in self.merged_dfs:
             self.doc.add_heading("5. ç‰ˆä½åˆ†æ", level=1)
             df_bd = self.merged_dfs["Master_Breakdown"]
             mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["ç‰ˆä½", "Placement"]))
             df_curr = df_bd[mask].copy()
             if not df_curr.empty:
                 if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan) if 'clicks' in df_curr else 0
                 if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan) if 'purchases' in df_curr else 0
                 if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan) if 'impressions' in df_curr else 0
                 if not find_column_fuzzy(df_curr, ['cpm']): df_curr['cpm'] = (df_curr['spend'] / df_curr['impressions'].replace(0, np.nan)) * 1000 if 'impressions' in df_curr else 0
                 req_cols = ['dimension_item', 'spend', 'ctr', 'cpc', 'cpm', 'roas', 'cpa']
                 rename_map = {}; valid_cols = []
                 for c in req_cols:
                     aliases = FIELD_ALIASES.get(c, [c])
                     f = find_column_fuzzy(df_curr, aliases)
                     if f: valid_cols.append(f); rename_map[f] = c
                     else: df_curr[c] = 0.0; valid_cols.append(c)
                 df_clean = df_curr[valid_cols].rename(columns=rename_map).round(2)
                 df_top5 = df_clean.sort_values('spend', ascending=False).head(5)
                 add_df_to_word(self.doc, apply_report_labels(df_top5, {'dimension_item': 'ç‰ˆä½'}), "5.1 ç‰ˆä½èŠ±è´¹ TOP 5", level=2)
                 mean_ctr = df_clean['ctr'].mean(); mean_cpm = df_clean['cpm'].mean()
                 mask_pot = (df_clean['ctr'] > mean_ctr) & (df_clean['cpm'] < mean_cpm)
                 df_pot = df_clean[mask_pot].sort_values('ctr', ascending=False).head(5)
                 if df_pot.empty: df_pot = df_clean.sort_values('ctr', ascending=False).head(5)
                 add_df_to_word(self.doc, apply_report_labels(df_pot, {'dimension_item': 'ç‰ˆä½'}), "5.2 ç‰ˆä½é«˜æ½œåŠ›", level=2)
                 self.final_json['5_placement_analysis'] = {"top_spend": df_top5.to_dict('records'), "high_potential": df_pot.to_dict('records')}

        # 7. æ¶æ„è¯Šæ–­ (ä¿æŒä¸å˜)
        rows = []
        if "Master_Overview" in self.merged_dfs:
             metrics = calc_metrics_dict(self.merged_dfs["Master_Overview"])
             if not metrics: metrics = {} 
             rows.append({
                "æ¨¡å—": "é¢„ç®—ç»“æ„", 
                "å½“å‰ç»“æ„æ•°æ®è¡¨ç°": (
                    f"æ€»èŠ±è´¹: ${float(str(metrics.get('spend', 0)).replace(',', '')):,.2f}\n"
                    f"CPA: ${float(str(metrics.get('cpa', 0)).replace(',', '')):.2f}\n"
                    f"ROAS: {float(str(metrics.get('roas', 0)).replace(',', '')):.2f}"
                ), 
                "å­˜åœ¨çš„é—®é¢˜": ""
             })
        if "Master_Breakdown" in self.merged_dfs:
            df_bd = self.merged_dfs["Master_Breakdown"]
            mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["å—ä¼—", "Audience"]))
            df_aud = df_bd[mask]
            s_col = find_column_fuzzy(df_aud, ['spend']); active_count = len(df_aud[df_aud[s_col] > 0]) if s_col else 0
            top_share = "0%"
            if not df_aud.empty and s_col:
                total_s = df_aud[s_col].sum()
                if total_s > 0: top_share = f"{df_aud[s_col].max()/total_s:.1%}"
            rows.append({"æ¨¡å—": "å—ä¼—ç»“æ„", "å½“å‰ç»“æ„æ•°æ®è¡¨ç°": f"æ´»è·ƒå—ä¼—ç»„æ•°: {active_count}\nTop1 èŠ±è´¹å æ¯”: {top_share}", "å­˜åœ¨çš„é—®é¢˜": ""})
        if "Master_Creative" in self.merged_dfs:
             df_cr = self.merged_dfs["Master_Creative"]
             mask = df_cr['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["ç´ æ", "Creative"]))
             df_mat = df_cr[mask]
             s_col = find_column_fuzzy(df_mat, ['spend']); active_count = len(df_mat[df_mat[s_col] > 0]) if s_col else 0
             rows.append({"æ¨¡å—": "ç´ æç»“æ„", "å½“å‰ç»“æ„æ•°æ®è¡¨ç°": f"æ´»è·ƒç´ ææ•°: {active_count}", "å­˜åœ¨çš„é—®é¢˜": ""})

        df_struct = pd.DataFrame(rows)
        add_df_to_word(self.doc, df_struct, "7. å¹¿å‘Šæ¶æ„åˆ†æ", level=1)
        if "Master_Overview" in self.merged_dfs:
             self.final_json['7_structure_analysis'] = df_struct.to_dict(orient='records')

# ==========================================
# PART 4: Streamlit UI (ä¿æŒä¸å˜)
# ==========================================
def main():
    st.set_page_config(page_title="Auto-Merge & Analysis V20.10", layout="wide")
    st.title("ğŸ“Šå¹¿å‘Šä¼˜åŒ–æŠ¥å‘Šæ•°æ®ç»ˆè¡¨ç”Ÿäº§")

    st.markdown("""
    **åŠŸèƒ½è¯´æ˜ï¼š**
    1. è¯·æ‚¨ä¸Šä¼ [å‘¨æœŸæ€§å¤ç›˜æŠ¥å‘Š]ã€[è¡Œä¸šbenchmark]ä¸¤ä¸ªæ•°æ®æ–‡ä»¶ã€‚
    2. æœ¬å·¥å…·å°†ä¸ºæ‚¨è¾“å‡ºä¸‰ç§æ–‡ä»¶ï¼ŒJSONæ ¼å¼å¯ç”¨äºå¤§æ¨¡å‹åˆ†æï¼ŒExcelå¯ç”¨äºæ•°æ®é€è§†ï¼ŒWordæ ¼å¼å¯ç”¨äºå®¡æŸ¥ã€‚
    3. å»ºè®®ï¼š**æ‚¨å¯åªé€‰æ‹©ä¸‹è½½JSONæ ¼å¼æ–‡ä»¶**ï¼Œå¦‚æœ‰å¿…è¦å†ä¸‹è½½å…¶ä»–æ ¼å¼æ–‡ä»¶ã€‚
    4. å¦‚æœæœ‰å…¶ä»–é—®é¢˜ï¼Œå¯è”ç³»Keyiã€‚
    """)

    col1, col2 = st.columns(2)
    with col1:
        raw_file = st.file_uploader("1. ä¸Šä¼  [æ•°æ®æŠ¥è¡¨] (Excel)", type=["xlsx", "xls"])
    with col2:
        bench_file = st.file_uploader("2. ä¸Šä¼  [è¡Œä¸šBenchmark]", type=["xlsx", "xls"])

    if st.button("ğŸš€ å¼€å§‹å¤„ç†"):
        if not raw_file:
            st.error("è¯·è‡³å°‘ä¸Šä¼ æ•°æ®æŠ¥è¡¨ï¼")
            return

        processor = AdReportProcessor(raw_file, bench_file)

        try:
            with st.spinner("é˜¶æ®µ 1/2: æ•°æ®æ¸…æ´—ã€Top10æˆªæ–­ã€é™ç»´åˆå¹¶..."):
                processor.process_etl()
                st.success("âœ… é˜¶æ®µ 1 å®Œæˆï¼šMaster Tables å·²ç”Ÿæˆ")

                with st.expander("æŸ¥çœ‹é™ç»´åˆå¹¶åçš„æ•°æ® (Master Tables)"):
                    tabs = st.tabs(processor.merged_dfs.keys())
                    for i, (k, v) in enumerate(processor.merged_dfs.items()):
                        with tabs[i]: st.dataframe(v.head(20))

            with st.spinner("é˜¶æ®µ 2/2: ç”Ÿæˆæ¶æ„è¯Šæ–­ã€WordæŠ¥å‘Š & JSON..."):
                processor.generate_report()
                st.success("âœ… é˜¶æ®µ 2 å®Œæˆï¼šæŠ¥å‘Šå·²ç”Ÿæˆ")

            st.divider()

            c1, c2, c3 = st.columns(3)
            json_str = json.dumps(processor.final_json, indent=4, ensure_ascii=False)
            c1.download_button("ğŸ“¥ ä¸‹è½½ JSON (ç”¨äºå¤§æ¨¡å‹åˆ†æ)", json_str, "Ad_Report_Data.json", "application/json")

            output_xls = io.BytesIO()
            with pd.ExcelWriter(output_xls, engine='xlsxwriter') as writer:
                for name, df in processor.merged_dfs.items(): df.to_excel(writer, sheet_name=name, index=False)
            c2.download_button("ğŸ“¥ ä¸‹è½½ Excel (ç”¨äºæ•°æ®é€è§†)", output_xls.getvalue(), "Merged_Ad_Report_Final.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

            output_doc = io.BytesIO()
            processor.doc.save(output_doc)
            c3.download_button("ğŸ“¥ ä¸‹è½½ Word (ç”¨äºæ•°æ®å®¡æŸ¥)", output_doc.getvalue(), "Ad_Report_Final_V20_10.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document")

        except Exception as e:
            st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            st.exception(e)

if __name__ == "__main__":
    main()
