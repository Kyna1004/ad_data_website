# -*- coding: utf-8 -*-
"""ÂπøÂëä‰ºòÂåñÊä•Âëä agent ÁΩëÈ°µÁâà.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11rV4Yh5BPiPouSl4Dx3fffvWc0Munpuh

# **ÂπøÂëä‰ºòÂåñÊä•ÂëäÁΩëÈ°µ**
"""

import streamlit as st
import pandas as pd
import numpy as np
import io
import json
import xlsxwriter
from docx import Document
from docx.shared import Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.shared import OxmlElement
from docx.oxml.ns import qn
import docx.opc.constants

# ==========================================
# PART 1: ÈÖçÁΩÆÂå∫Âüü
# ==========================================

#ÂÆö‰πâ‰∏Ä‰∏™„ÄåÊ†áÂáÜÂ≠óÊÆµÂêç‚ÜíÂéüÂßãË°®‰∏≠ÂèØËÉΩÂá∫Áé∞ÁöÑÂàóÂêç„Äç
COMMON_METRICS = {
    "spend": ["Ëä±Ë¥πÈáëÈ¢ù(USD)", "Ëä±Ë¥πÈáëÈ¢ù ÔºàUSDÔºâ", "Ëä±Ë¥πÈáëÈ¢ù (USD)", "Ëä±Ë¥πÈáëÈ¢ù"],
    "roas": ["ÂπøÂëäËä±Ë¥πÂõûÊä• (ROAS) - Ë¥≠Áâ©", "ÂπøÂëäËä±Ë¥πÂõûÊä•ÔºàROASÔºâ-Ë¥≠Áâ©", "ROAS"],
    "purchases": ["Ë¥≠‰π∞Ê¨°Êï∞", "ÊàêÊïàÊï∞Èáè", "ÊàêÊïà"],
    "cpa": ["ÂçïÊ¨°Ë¥≠‰π∞Ë¥πÁî®", "ÂçïÊ¨°Ë¥≠Áâ©ÊàêÊú¨", "ÂçïÊ¨°ÊàêÊïàÊàêÊú¨", "ÂçïÊ¨°ÊàêÊïàË¥πÁî®"],
    "ctr": ["ÈìæÊé•ÁÇπÂáªÁéá", "ÈìæÊé•ÁÇπÂáªÁéáÔºà%)", "ÈìæÊé•ÁÇπÂáªÁéáÔºà%Ôºâ"],
    "cpm": ["ÂçÉÊ¨°Â±ïÁ§∫Ë¥πÁî®", "CPM"],
    "clicks": ["ÁÇπÂáª", "ÈìæÊé•ÁÇπÂáª"],
    "impressions": ["ÊõùÂÖâ", "Â±ïÁ§∫Ê¨°Êï∞"],
    "purchase_value": ["Ë¥≠‰π∞‰ª∑ÂÄº", "Ë¥≠Áâ©‰ª∑ÂÄº"],
    "aov": ["ÂçïÊ¨°Ë¥≠‰π∞‰ª∑ÂÄº", "ÂçïÊ¨°Ë¥≠Áâ©‰ª∑ÂÄº"]
}

#Ê°ÜÂÆö„ÄåÊØè‰∏Ä‰∏™ Sheet„ÄçÈúÄË¶ÅÊäΩÂèñÂì™‰∫õÊåáÊ†á
SHEET_MAPPINGS = {
    "Êï¥‰ΩìÊï∞ÊçÆ": {
        **COMMON_METRICS,
        "date_range": ["Êó∂Èó¥ËåÉÂõ¥"],
        "clicks_all": ["ÁÇπÂáª"],
        "landing_page_views": ["ËêΩÂú∞È°µÊµèËßàÈáè"],
        "add_to_cart": ["Âä†ÂÖ•Ë¥≠Áâ©ËΩ¶"],
        "initiate_checkout": ["ÁªìË¥¶ÂèëËµ∑Ê¨°Êï∞"],
        "rate_click_to_lp": ["ÁÇπÂáª-ËêΩÂú∞È°µÊµèËßàËΩ¨ÂåñÁéá"],
        "rate_lp_to_atc": ["ËêΩÂú∞È°µÊµèËßà-Âä†Ë¥≠ËΩ¨ÂåñÁéá"],
        "rate_atc_to_ic": ["Âä†Ë¥≠-ÁªìË¥¶ËΩ¨ÂåñÁéá"],
        "rate_ic_to_pur": ["ÁªìË¥¶-Ë¥≠‰π∞ËΩ¨ÂåñÁéá"]
    },
    "ÂàÜÊó∂ÊÆµÊï∞ÊçÆ": {
        **COMMON_METRICS,
        "date_range": ["Êó∂Èó¥ËåÉÂõ¥"],
        "landing_page_views": ["ËêΩÂú∞È°µÊµèËßàÈáè"],
        "add_to_cart": ["Âä†ÂÖ•Ë¥≠Áâ©ËΩ¶"],
        "initiate_checkout": ["ÁªìË¥¶ÂèëËµ∑Ê¨°Êï∞"],
        "rate_click_to_lp": ["ÁÇπÂáª-ËêΩÂú∞È°µÊµèËßàËΩ¨ÂåñÁéá"],
        "rate_lp_to_atc": ["ËêΩÂú∞È°µÊµèËßà-Âä†Ë¥≠ËΩ¨ÂåñÁéá"],
        "rate_atc_to_ic": ["Âä†Ë¥≠-ÁªìË¥¶ËΩ¨ÂåñÁéá"],
        "rate_ic_to_pur": ["ÁªìË¥¶-Ë¥≠‰π∞ËΩ¨ÂåñÁéá"]
    },
    "ÂºÇÂ∏∏ÊåáÊ†á": {
        "anomaly_metric_name": ["ÂºÇÂ∏∏ÊåáÊ†á"],
        "mom_change": ["ÁéØÊØî"]
    },
    "ÂπøÂëäÊû∂ÊûÑ": {**COMMON_METRICS, "dimension_item": ["ÂπøÂëäÁ±ªÂûã"]},
    "Âèó‰ºóÁªÑ": {
        **COMMON_METRICS,
        "dimension_item": ["ÂπøÂëäÁªÑ", "ÂπøÂëäÁªÑId"],
        "custom_audience_settings": ["ËÆæÁΩÆÁöÑËá™ÂÆö‰πâÂèó‰ºó"],
        "converting_keywords": ["‰∫ßÁîüÊàêÊïàÁöÑÂÖ≥ÈîÆËØç"]
    },
    "Âèó‰ºóÁ±ªÂûã": {**COMMON_METRICS, "dimension_item": ["Âèó‰ºóÁ±ªÂûã"]},
    "ÂõΩÂÆ∂": {**COMMON_METRICS, "dimension_item": ["ÂõΩÂÆ∂/Âú∞Âå∫", "ÂõΩÂÆ∂"]},
    "Âπ¥ÈæÑ": {**COMMON_METRICS, "dimension_item": ["Âπ¥ÈæÑ"]},
    "ÊÄßÂà´": {**COMMON_METRICS, "dimension_item": ["ÊÄßÂà´"]},
    "Âπ≥Âè∞&Áâà‰Ωç": {**COMMON_METRICS, "dimension_item": ["Âπ≥Âè∞&Áâà‰Ωç"]},
    "Á¥†Êùê": {
        **COMMON_METRICS,
        "content_item": ["Á¥†Êùê"],
        "cvr_lp_to_pur": ["ËêΩÂú∞È°µÊµèËßà-Ë¥≠‰π∞ËΩ¨ÂåñÁéá"]
    },
    "ËêΩÂú∞È°µ": {
        **COMMON_METRICS,
        "content_item": ["ËêΩÂú∞È°µurl", "ËêΩÂú∞È°µ"],
        "ctr_all": ["ÊõùÂÖâ-ÁÇπÂáªËΩ¨ÂåñÁéá"],
        "rate_lp_to_atc": ["ËêΩÂú∞È°µÊµèËßà-Âä†Ë¥≠ËΩ¨ÂåñÁéá", "ËêΩÂú∞È°µÊµèËßà-Ë¥≠Áâ©ËΩ¨ÂåñÁéá"]
    }
}

#ÈôçÁª¥Êï∞ÊçÆË°®ÁöÑÁªìÊûÑ
GROUP_CONFIG = {
    "Master_Overview": ["Êï¥‰ΩìÊï∞ÊçÆ", "ÂàÜÊó∂ÊÆµÊï∞ÊçÆ", "ÂºÇÂ∏∏ÊåáÊ†á"],
    "Master_Breakdown": ["ÂπøÂëäÊû∂ÊûÑ", "Âèó‰ºóÁªÑ", "Âèó‰ºóÁ±ªÂûã", "ÂõΩÂÆ∂", "Âπ¥ÈæÑ", "ÊÄßÂà´", "Âπ≥Âè∞&Áâà‰Ωç"],
    "Master_Creative": ["Á¥†Êùê", "ËêΩÂú∞È°µ"]
}

#Â≠óÊÆµÁöÑÂ±ïÁ§∫ÂêçÁß∞
REPORT_MAPPING = {
    "spend": "Ëä±Ë¥π ($)", "roas": "ROAS", "purchases": "Ë¥≠‰π∞Ê¨°Êï∞", "purchase_value": "Ë¥≠‰π∞ÊÄª‰ª∑ÂÄº",
    "cpa": "CPA ($)", "ctr": "CTR (%)", "cpm": "CPM ($)", "aov": "ÂÆ¢Âçï‰ª∑",
    "impressions": "Â±ïÁé∞Èáè", "clicks_all": "ÁÇπÂáªÈáè (All)", "clicks": "ÁÇπÂáªÈáè (All)", "ctr_all": "ÁÇπÂáªÁéá (All)",
    "landing_page_views": "ËêΩÂú∞È°µËÆøÈóÆÈáè", "add_to_cart": "Âä†Ë¥≠Ê¨°Êï∞", "initiate_checkout": "ÁªìË¥¶ÂèëËµ∑Êï∞ (IC)",
    "rate_click_to_lp": "ÁÇπÂáª ‚Üí ËêΩÂú∞È°µËÆøÈóÆËΩ¨ÂåñÁéá", "rate_lp_to_atc": "ËêΩÂú∞È°µ ‚Üí Âä†Ë¥≠ËΩ¨ÂåñÁéá",
    "rate_atc_to_ic": "Âä†Ë¥≠ ‚Üí Ë¥≠‰π∞ËΩ¨ÂåñÁéá", "rate_ic_to_pur": "Ë¥≠‰π∞ËΩ¨ÂåñÁéá",
    "cvr_purchase": "ÁÇπÂáª ‚Üí Ë¥≠‰π∞ËΩ¨ÂåñÁéá", "cvr_lp_to_pur": "CVR (ÂÖ®Á´ôËΩ¨ÂåñÁéá)",
    "date_range": "Êó•Êúü/Êó∂ÊÆµ", "campaign_type": "ÊäïÊîæÊ®°Âºè", "adset_name": "ÂπøÂëäÁªÑID", "adset_id": "ÂπøÂëäÁªÑID",
    "custom_audience_settings": "Ëá™ÂÆö‰πâÂèó‰ºóÊ∫ê", "converting_keywords": "È´òÊΩúÂÖ¥Ë∂£ËØç", "audience_type": "Âèó‰ºóÁ≠ñÁï•",
    "country": "ÂõΩÂÆ∂", "age_group": "Âπ¥ÈæÑ", "gender": "ÊÄßÂà´", "creative_name": "Á¥†ÊùêÂêçÁß∞", "placement": "Áâà‰Ωç",
    "landing_page_url": "È°µÈù¢ URL", "mom_change": "ÁéØÊØîÊ≥¢Âä®", "anomaly_metric_name": "ÂºÇÂ∏∏È°π",
    "converting_countries": "‰∫ßÁîüÊàêÊïàÁöÑÂõΩÂÆ∂", "converting_genders": "‰∫ßÁîüÊàêÊïàÁöÑÊÄßÂà´", "converting_ages": "‰∫ßÁîüÊàêÊïàÁöÑÂπ¥ÈæÑ"
}

#Áî®‰∫éÊ®°Á≥äËØÜÂà´Â≠óÊÆµÔºàÈò≤Ê≠¢ÂàóÂêç‰∏çËßÑËåÉÔºâ
FIELD_ALIASES = {
    "adset_id": ["adset_id", "ad set id", "adset id", "ÂπøÂëäÁªÑÁºñÂè∑", "ÂπøÂëäÁªÑid", "adset_name", "ad set name"],
    "converting_countries": ["converting_countries", "country", "region", "ÂõΩÂÆ∂", "Âú∞Âå∫"],
    "converting_genders": ["converting_genders", "gender", "ÊÄßÂà´"],
    "converting_ages": ["converting_ages", "age", "Âπ¥ÈæÑ", "age_group"],
    "converting_keywords": ["converting_keywords", "keywords", "interests", "ÂÖ¥Ë∂£", "ÂÖ≥ÈîÆËØç"],
    "spend": ["spend", "amount spent", "cost", "Ëä±Ë¥π", "Ê∂àËÄó"],
    "purchases": ["purchases", "results", "result", "ÊàêÊïà", "Ë¥≠‰π∞"],
    "roas": ["roas", "return on ad spend", "purchase roas"],
    "purchase_value": ["purchase_value", "conversion value", "value", "ÊÄª‰ª∑ÂÄº", "gmv", "Ë¥≠‰π∞ÊÄª‰ª∑ÂÄº"],
    "clicks": ["clicks", "clicks (all)", "ÁÇπÂáªÈáè", "clicks_all"],
    "impressions": ["impressions", "Â±ïÁ§∫", "Â±ïÁé∞"],
    "ctr_all": ["ctr_all", "ctr (all)", "ÁÇπÂáªÁéá (all)"]
}


# ==========================================
# PART 2: Ê†∏ÂøÉÂ∑•ÂÖ∑ÂáΩÊï∞
# ==========================================

#ÂÆΩÊùæÊ∏ÖÊ¥ó
def clean_numeric(val):
    if pd.isna(val): return 0.0
    if isinstance(val, (int, float)): return float(val)
    val_str = str(val).strip().replace('$', '').replace('¬•', '').replace(',', '')
    if '%' in val_str: val_str = val_str.replace('%', '')
    try: return float(val_str)
    except: return val

#‰∏•Ê†ºÊ∏ÖÊ¥ó
def clean_numeric_strict(val): # ËÑöÊú¨2ÁöÑ‰∏•Ê†ºÊ∏ÖÊ¥óÔºåÁî®‰∫éËÆ°ÁÆó
    if pd.isna(val): return 0.0
    val_str = str(val).strip().replace('$', '').replace('¬•', '').replace(',', '')
    if '%' in val_str: val_str = val_str.replace('%', '')
    try: return float(val_str)
    except: return 0.0

#Â≠óÊÆµÈ≤ÅÊ£íÊ†∏ÂøÉ
def find_column_fuzzy(df, keywords):
    for kw in keywords:
        if kw in df.columns: return kw
    df_cols_norm = {c.lower().replace(' ', '').replace('_', ''): c for c in df.columns}
    for kw in keywords:
        kw_norm = kw.lower().replace(' ', '').replace('_', '')
        if kw_norm in df_cols_norm: return df_cols_norm[kw_norm]
    for col in df.columns:
        col_lower = col.lower()
        for kw in keywords:
            if kw.lower() in col_lower: return col
    return None

#Ê†∏ÂøÉÊåáÊ†áËÆ°ÁÆó
def calc_metrics_dict(df_chunk):
    res = {}
    if df_chunk.empty: return res
    sums = {}
    targets = ['spend', 'clicks', 'impressions', 'purchases', 'purchase_value',
               'landing_page_views', 'add_to_cart', 'initiate_checkout']
    for t in targets:
        aliases = FIELD_ALIASES.get(t, [t])
        if t == 'purchase_value' and 'value' not in aliases: aliases.append('value')
        col = find_column_fuzzy(df_chunk, aliases)
        # ‰ΩøÁî®‰∏•Ê†ºÊ∏ÖÊ¥óËøõË°åËÆ°ÁÆó
        if col:
             sums[t] = df_chunk[col].apply(clean_numeric_strict).sum()
        else:
             sums[t] = 0.0

    eps = 1e-9
    res['spend'] = sums['spend']
    res['roas'] = sums['purchase_value'] / (sums['spend'] + eps)
    res['cpm'] = (sums['spend'] / (sums['impressions'] + eps)) * 1000
    res['cpc'] = sums['spend'] / (sums['clicks'] + eps)
    res['ctr'] = sums['clicks'] / (sums['impressions'] + eps)
    res['cpa'] = sums['spend'] / (sums['purchases'] + eps)
    res['cvr_purchase'] = sums['purchases'] / (sums['clicks'] + eps)
    res['rate_click_to_lp'] = sums['landing_page_views'] / (sums['clicks'] + eps)
    res['rate_lp_to_atc'] = sums['add_to_cart'] / (sums['landing_page_views'] + eps)
    res['rate_atc_to_ic'] = sums['initiate_checkout'] / (sums['add_to_cart'] + eps)
    res['rate_ic_to_pur'] = sums['purchases'] / (sums['initiate_checkout'] + eps)
    res['aov'] = sums['purchase_value'] / (sums['purchases'] + eps)

    # ËæÖÂä©‰ø°ÊÅØ
    date_col = find_column_fuzzy(df_chunk, ['date', 'time', 'range'])
    if date_col:
        try:
            dates = pd.to_datetime(df_chunk[date_col], errors='coerce').dropna()
            if not dates.empty: res['date_range'] = f"{dates.min():%Y-%m-%d} ~ {dates.max():%Y-%m-%d}"
            else: res['date_range'] = "-"
        except: res['date_range'] = "-"
    else: res['date_range'] = "-"
    return res

def format_cell(key, val, is_mom=False):
    if isinstance(val, str): return val
    if is_mom:
        if key == 'date_range': return val
        return f"{val:+.2%}"
    k = str(key).lower()
    if 'roas' in k: return f"{val:.2f}"
    if any(x in k for x in ['rate', 'ctr', 'cvr', 'ÁÇπÂáªÁéá', 'ËΩ¨ÂåñÁéá', 'ÁùÄÈôÜÁéá', 'ÊÑèÂêëÁéá', 'ÊàêÂäüÁéá']): return f"{val:.2%}"
    if any(x in k for x in ['spend', 'cpm', 'cpc', 'value', 'aov', 'cpa', 'Ëä±Ë¥π', 'ÈáëÈ¢ù', 'ÂÆ¢Âçï‰ª∑', 'gmv', '‰ª∑ÂÄº']): return f"{val:,.2f}"
    if any(x in k for x in ['purchases', 'cart', 'click', 'Ê¨°Êï∞', 'ÂçïÈáè', 'ÁÇπÂáª', 'Â±ïÁé∞', 'ËÆøÈóÆÈáè', 'ÂèëËµ∑Êï∞']): return f"{val:,.0f}"
    return f"{val}"

def extract_benchmark_values(df_bench):
    targets = {'roas': (['roas'], True), 'cpm': (['cpm'], False), 'ctr': (['ctr'], True), 'cpc': (['cpc'], False), 'cpa': (['cpa_purchase', 'cpa'], False)}
    extracted = {}
    for metric, (aliases, higher_better) in targets.items():
        found_col = None
        for alias in aliases:
            found_col = find_column_fuzzy(df_bench, [alias])
            if found_col: break
        if found_col:
            try:
                s = df_bench[found_col].apply(clean_numeric_strict)
                v = s[s>0].mean()
                if not pd.isna(v): extracted[metric] = [v, higher_better]
            except: pass
    return extracted

#Ë∂ÖÈìæÊé•
def add_hyperlink(paragraph, url, text, color="0000FF", underline=True):
    try:
        part = paragraph.part
        r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)
        hyperlink = OxmlElement('w:hyperlink')
        hyperlink.set(qn('r:id'), r_id)
        new_run = OxmlElement('w:r')
        rPr = OxmlElement('w:rPr')
        if color:
            c = OxmlElement('w:color')
            c.set(qn('w:val'), color)
            rPr.append(c)
        if underline:
            u = OxmlElement('w:u')
            u.set(qn('w:val'), 'single')
            rPr.append(u)
        new_run.append(rPr)
        new_run.text = text
        hyperlink.append(new_run)
        paragraph._p.append(hyperlink)
        return hyperlink
    except: return None

def apply_report_labels(df, custom_mapping=None):
    if df.empty: return df
    mapping = REPORT_MAPPING.copy()
    if custom_mapping: mapping.update(custom_mapping)
    return df.rename(columns=mapping)

def add_df_to_word(doc, df, title, level=1):
    if df.empty: return
    doc.add_heading(title, level=level)
    t = doc.add_table(rows=df.shape[0]+1, cols=df.shape[1])
    t.style = 'Table Grid'

    is_creative = "Á¥†Êùê" in title
    is_landing = "ËêΩÂú∞È°µ" in title
    link_col_idx = -1

    for j, col in enumerate(df.columns):
        cell = t.cell(0, j)
        cell.text = str(col)
        if any(x in str(col).lower() for x in ["url", "link", "Á¥†Êùê", "ÂÜÖÂÆπ", "content"]):
            link_col_idx = j
        for p in cell.paragraphs:
            for r in p.runs:
                r.font.bold = True
                r.font.size = Pt(8)

    for i in range(df.shape[0]):
        label_prefix = "Á¥†Êùê" if is_creative else ("ËêΩÂú∞È°µ" if is_landing else "")
        label_char = chr(65 + (i % 26))
        if i >= 26: label_char += str(i // 26)
        label_text = f"{label_prefix}{label_char}"

        for j in range(df.shape[1]):
            val = df.iat[i, j]
            cell = t.cell(i+1, j)

            if (is_creative or is_landing) and j == link_col_idx:
                try:
                    p = cell.paragraphs[0]
                    url = str(val).strip()
                    if len(url) > 5: add_hyperlink(p, url, label_text)
                    else: cell.text = label_text
                except: cell.text = label_text
            else:
                cell.text = str(val)
                if "ÁªìËÆ∫" in str(df.columns[j]):
                    if "‚úÖ" in str(val): cell.paragraphs[0].runs[0].font.color.rgb = RGBColor(0, 128, 0)
                    if "‚ö†Ô∏è" in str(val): cell.paragraphs[0].runs[0].font.color.rgb = RGBColor(255, 0, 0)

            for p in cell.paragraphs:
                for r in p.runs: r.font.size = Pt(8)
    doc.add_paragraph("\n")

# ==========================================
# PART 3: ‰∏ªÈÄªËæëÁ±ª
# ==========================================

class AdReportProcessor:
    def __init__(self, raw_file, bench_file=None):
        self.raw_file = raw_file
        self.bench_file = bench_file
        self.processed_dfs = {} # Phase 1 results
        self.merged_dfs = {}    # Phase 1 results (Master)
        self.final_json = {}    # Phase 2 results
        self.doc = Document()   # Phase 2 Word Doc

    # --- Èò∂ÊÆµ 1: Êï∞ÊçÆÊ∏ÖÊ¥ó‰∏éÈôçÁª¥ (ËÑöÊú¨ 1 ÈÄªËæë) ---
    def process_etl(self):
        xls = pd.ExcelFile(self.raw_file)

        # 1. Ê∏ÖÊ¥óÂêÑ Sheet
        for sheet_name, mapping in SHEET_MAPPINGS.items():
            if sheet_name in xls.sheet_names:
                df = pd.read_excel(xls, sheet_name=sheet_name)
                final_cols = {}

                # Â≠óÊÆµÊò†Â∞Ñ
                for std_col, raw_col_options in mapping.items():
                    matched_col = None
                    for option in raw_col_options:
                        if option in df.columns:
                            matched_col = option; break
                        if not matched_col:
                            for df_col in df.columns:
                                if option.replace(" ", "") == df_col.replace(" ", ""):
                                    matched_col = df_col; break
                        if matched_col: break
                    if matched_col: final_cols[std_col] = matched_col

                if final_cols:
                    df_clean = df[list(final_cols.values())].rename(columns={v: k for k, v in final_cols.items()})
                    text_cols = ['date_range', 'anomaly_metric_name', 'converting_keywords',
                                 'custom_audience_settings', 'dimension_item', 'content_item']
                    for col in df_clean.columns:
                        if col not in text_cols:
                            df_clean[col] = df_clean[col].apply(clean_numeric)

                    # üéØ [ËÑöÊú¨1Ê†∏ÂøÉÈÄªËæë] ‰øÆÂ§çÈáçÁÇπÔºöÁ¥†Êùê‰∏éËêΩÂú∞È°µÁã¨Á´ãÂÅö Top10ÔºàÊåâ spendÔºâ
                    if sheet_name in ["Á¥†Êùê", "ËêΩÂú∞È°µ","Âèó‰ºóÁªÑ"]:
                        if "spend" in df_clean.columns:
                            df_clean = df_clean.sort_values("spend", ascending=False).head(10)

                    df_clean["Source_Sheet"] = sheet_name
                    self.processed_dfs[sheet_name] = df_clean

        # 2. ÂêàÂπ∂ Master Tables
        for master_name, source_sheets in GROUP_CONFIG.items():
            dfs_to_merge = [self.processed_dfs[src] for src in source_sheets if src in self.processed_dfs]
            if dfs_to_merge:
                merged_df = pd.concat(dfs_to_merge, ignore_index=True)
                cols = list(merged_df.columns)
                priority_cols = ['Source_Sheet', 'date_range', 'dimension_item', 'content_item',
                                 'spend', 'roas', 'purchases', 'cpa']
                new_order = [c for c in priority_cols if c in cols] + [c for c in cols if c not in priority_cols]
                self.merged_dfs[master_name] = merged_df[new_order]

    # --- Èò∂ÊÆµ 2: Êä•ÂëäÁîüÊàê‰∏éÊû∂ÊûÑËØäÊñ≠ (ËÑöÊú¨ 2 ÈÄªËæë) ---
    def generate_report(self):
        # ÂáÜÂ§á Benchmark
        benchmark_targets = {'roas': [2.0, True], 'cpm': [20.0, False], 'ctr': [0.015, True], 'cpc': [1.5, False], 'cpa': [30.0, False]}
        if self.bench_file:
            try:
                df_b = pd.read_excel(self.bench_file)
                benchmark_targets = extract_benchmark_values(df_b)
            except: pass

        self.doc.add_heading('ÂπøÂëäÊäïÊîæÊ∑±Â∫¶ÂàÜÊûêÊä•Âëä', 0).alignment = WD_ALIGN_PARAGRAPH.CENTER
        self.final_json = {"report_title": "ÂπøÂëäÊäïÊîæÊ∑±Â∫¶ÂàÜÊûêÊä•Âëä", "generated_at": pd.Timestamp.now().strftime("%Y-%m-%d")}

        # 1. Â§ßÁõòÊÄªËßà
        df_ov = pd.DataFrame()
        if "Master_Overview" in self.merged_dfs:
            df_src = self.merged_dfs["Master_Overview"]
            mask = df_src['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["ÂàÜÊó∂", "Time"]))
            df_ov = df_src[mask].copy() if not df_src[mask].empty else df_src.copy()

        if not df_ov.empty:
            # ÈÄªËæëÔºögenerate_overview_struct
            date_col = find_column_fuzzy(df_ov, ['date', 'time', 'Êó∂Èó¥'])
            if date_col:
                try:
                    df_ov['temp_date'] = pd.to_datetime(df_ov[date_col], errors='coerce')
                    df_clean = df_ov.dropna(subset=['temp_date']).sort_values('temp_date')
                    dates = df_clean['temp_date'].unique()
                    raw_overall = calc_metrics_dict(df_clean)

                    if len(dates) >= 2:
                        mid_date = dates[len(dates)//2]
                        raw_prev = calc_metrics_dict(df_clean[df_clean['temp_date'] < mid_date])
                        raw_curr = calc_metrics_dict(df_clean[df_clean['temp_date'] >= mid_date])
                        raw_mom = {}
                        for k, v_curr in raw_curr.items():
                            if k == 'date_range': raw_mom[k] = "-"
                            else:
                                v_prev = raw_prev.get(k, 0)
                                raw_mom[k] = (v_curr - v_prev) / v_prev if v_prev > 0 else 0.0
                    else:
                        raw_prev = {k: "-" for k in raw_overall}; raw_curr = raw_overall; raw_mom = {k: "-" for k in raw_overall}

                    col_order = ["date_range", "spend", "roas", "cpa", "cpm", "cpc", "ctr", "cvr_purchase",
                                 "rate_click_to_lp", "rate_lp_to_atc", "rate_ic_to_pur", "aov", "add_to_cart", "purchases", "purchase_value"]
                    final_data = []
                    for label, r in zip(["Êï¥‰ΩìÊï∞ÊçÆ", "‰∏äÂë®ÊúüÂÄº", "Êú¨Âë®Êúü", "ÁéØÊØî"], [raw_overall, raw_prev, raw_curr, raw_mom]):
                        row = {"Label": label}
                        is_m = (label == "ÁéØÊØî")
                        for c in col_order: row[c] = format_cell(c, r.get(c, 0), is_mom=is_m)
                        row['date_range'] = label
                        final_data.append(row)

                    df_f = pd.DataFrame(final_data, columns=col_order)
                    df_f_display = apply_report_labels(df_f)
                    add_df_to_word(self.doc, df_f_display, "1. Êï∞ÊçÆÂ§ßÁõòÊÄªËßà", level=1)
                    self.final_json['1_data_overview'] = df_f.to_dict(orient='records')

                    # 2. Benchmark
                    raw_current = calc_metrics_dict(df_clean)
                    bench_data = []
                    for metric_key in ['roas', 'cpm', 'ctr', 'cpc', 'cpa']:
                        curr_val = raw_current.get(metric_key, 0)
                        bench_val, higher_is_better = benchmark_targets.get(metric_key, [0, True])
                        conclusion = "-"
                        if curr_val != 0:
                            diff = curr_val - bench_val
                            if higher_is_better: conclusion = "‚úÖ ‰ºò‰∫éÂ§ßÁõò" if diff > 0 else ("‚ö†Ô∏è ‰Ωé‰∫éÂ§ßÁõò" if diff < 0 else "ÊåÅÂπ≥")
                            else: conclusion = "‚úÖ ‰ºò‰∫éÂ§ßÁõò" if diff < 0 else ("‚ö†Ô∏è È´ò‰∫éÂ§ßÁõò" if diff > 0 else "ÊåÅÂπ≥")

                        bench_data.append({
                            "ÊåáÊ†á": REPORT_MAPPING.get(metric_key, metric_key.upper()),
                            "ÂΩìÂâçË¥¶Êà∑": format_cell(metric_key, curr_val),
                            "Ë°å‰∏öÂü∫ÂáÜ": format_cell(metric_key, bench_val),
                            "ÂØπÊØîÁªìËÆ∫": conclusion
                        })
                    df_b = pd.DataFrame(bench_data)
                    add_df_to_word(self.doc, df_b, "2. Ë°å‰∏ö Benchmark ÂØπÊØî", level=1)
                    self.final_json['2_industry_benchmark'] = df_b.to_dict(orient='records')
                except Exception as e:
                    st.warning(f"Â§ßÁõòËÆ°ÁÆóË≠¶Âëä: {e}")

        # 3. Âèó‰ºóÁªÑ
        self.doc.add_heading("3. Âèó‰ºóÁªÑÂàÜÊûê", level=1)
        self.final_json['3_audience_analysis'] = {}
        audience_configs = [
            ("3.1 ÂõΩÂÆ∂ÂàÜÊûê", ["ÂõΩÂÆ∂", "Country"], True, "ÂõΩÂÆ∂"),
            ("3.2 ÊÄßÂà´ÂàÜÊûê", ["ÊÄßÂà´", "Gender"], False, "ÊÄßÂà´"),
            ("3.3 Âπ¥ÈæÑÂàÜÊûê", ["Âπ¥ÈæÑ", "Age"], False, "Âπ¥ÈæÑÊÆµ"),
            ("3.4 Âèó‰ºóÁªÑÂàÜÊûêË°®", ["Âèó‰ºó", "Audience"], True, "Âèó‰ºóÁªÑÂêçÁß∞"),
        ]

        if "Master_Breakdown" in self.merged_dfs:
            df_bd = self.merged_dfs["Master_Breakdown"]
            for title, keywords, top10, dim_label in audience_configs:
                mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in keywords))
                df_curr = df_bd[mask].copy()
                if not df_curr.empty:
                    # ËÆ°ÁÆóÁº∫Â§±ÊåáÊ†á
                    if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan) if 'clicks' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['cpm']): df_curr['cpm'] = (df_curr['spend'] / df_curr['impressions'].replace(0, np.nan)) * 1000 if 'impressions' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan) if 'impressions' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan) if 'purchases' in df_curr else 0

                    req_cols = ["dimension_item", "spend", "ctr", "cpc", "cpm", "cpa", "roas"]
                    if "Âèó‰ºó" in title: req_cols += ["converting_countries", "converting_keywords"]

                    rename_map = {}; valid_cols = []
                    for req in req_cols:
                        aliases = FIELD_ALIASES.get(req, [req])
                        found = find_column_fuzzy(df_curr, aliases)
                        if found: valid_cols.append(found); rename_map[found] = req
                        else: df_curr[req] = 0.0 if "converting" not in req else "-"; valid_cols.append(req)

                    df_final = df_curr[valid_cols].rename(columns=rename_map)
                    # ËøáÊª§ unknown
                    if "dimension_item" in df_final.columns:
                         df_final = df_final[~df_final['dimension_item'].astype(str).str.lower().str.contains('unknow', na=False)]

                    if top10 and 'spend' in df_final.columns: df_final = df_final.sort_values('spend', ascending=False).head(10)
                    df_clean = df_final.round(2)
                    df_display = apply_report_labels(df_clean, custom_mapping={'dimension_item': dim_label})
                    add_df_to_word(self.doc, df_display, title, level=2)
                    self.final_json['3_audience_analysis'][title] = df_clean.to_dict(orient='records')

        # 4. Á¥†Êùê‰∏éËêΩÂú∞È°µ (‰ªé Master_Creative)
        if "Master_Creative" in self.merged_dfs:
            df_cr = self.merged_dfs["Master_Creative"]
            for title, keywords, label, json_key in [("4. Á¥†ÊùêÂàÜÊûê", ["Á¥†Êùê", "Creative"], "Á¥†ÊùêÂêçÁß∞", "4_creative_analysis"), ("6. ËêΩÂú∞È°µÂàÜÊûê", ["ËêΩÂú∞È°µ", "Landing"], "ËêΩÂú∞È°µ URL", "6_landing_page_analysis")]:
                mask = df_cr['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in keywords))
                df_curr = df_cr[mask].copy()
                if not df_curr.empty:
                    # ËÆ°ÁÆóÊåáÊ†á
                    if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan) if 'clicks' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan) if 'purchases' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan) if 'impressions' in df_curr else 0

                    req_cols = ["content_item", "spend", "ctr", "cpc", "cpm", "roas", "cpa"]
                    rename_map = {}; valid_cols = []
                    for req in req_cols:
                        aliases = FIELD_ALIASES.get(req, [req])
                        found = find_column_fuzzy(df_curr, aliases)
                        if found: valid_cols.append(found); rename_map[found] = req
                        else: df_curr[req] = 0.0; valid_cols.append(req)

                    df_final = df_curr[valid_cols].rename(columns=rename_map)
                    # Â∑≤ÁªèÂú® Phase 1 ÂÅöËøá Top 10 ‰∫ÜÔºåËøôÈáåÂèØ‰ª•Áõ¥Êé•Áî®ÔºåÊàñËÄÖÂÜç‰øùÈô©‰∏ÄÊ¨°
                    if 'spend' in df_final.columns: df_final = df_final.sort_values('spend', ascending=False).head(10)

                    df_clean = df_final.round(2)
                    df_display = apply_report_labels(df_clean, custom_mapping={'content_item': label})
                    add_df_to_word(self.doc, df_display, title, level=1)
                    self.final_json[json_key] = df_clean.to_dict(orient='records')

        # 5. Áâà‰Ωç (Master_Breakdown)
        if "Master_Breakdown" in self.merged_dfs:
             self.doc.add_heading("5. Áâà‰ΩçÂàÜÊûê", level=1)
             df_bd = self.merged_dfs["Master_Breakdown"]
             mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["Áâà‰Ωç", "Placement"]))
             df_curr = df_bd[mask].copy()
             if not df_curr.empty:
                 if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan) if 'clicks' in df_curr else 0
                 if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan) if 'purchases' in df_curr else 0
                 if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan) if 'impressions' in df_curr else 0
                 if not find_column_fuzzy(df_curr, ['cpm']): df_curr['cpm'] = (df_curr['spend'] / df_curr['impressions'].replace(0, np.nan)) * 1000 if 'impressions' in df_curr else 0

                 req_cols = ['dimension_item', 'spend', 'ctr', 'cpc', 'cpm', 'roas', 'cpa']
                 rename_map = {}; valid_cols = []
                 for c in req_cols:
                     aliases = FIELD_ALIASES.get(c, [c])
                     f = find_column_fuzzy(df_curr, aliases)
                     if f: valid_cols.append(f); rename_map[f] = c
                     else: df_curr[c] = 0.0; valid_cols.append(c)

                 df_clean = df_curr[valid_cols].rename(columns=rename_map).round(2)

                 # 5.1 Spend Top 5
                 df_top5 = df_clean.sort_values('spend', ascending=False).head(5)
                 add_df_to_word(self.doc, apply_report_labels(df_top5, {'dimension_item': 'Áâà‰Ωç'}), "5.1 Áâà‰ΩçËä±Ë¥π TOP 5", level=2)

                 # 5.2 High Potential
                 mean_ctr = df_clean['ctr'].mean(); mean_cpm = df_clean['cpm'].mean()
                 mask_pot = (df_clean['ctr'] > mean_ctr) & (df_clean['cpm'] < mean_cpm)
                 df_pot = df_clean[mask_pot].sort_values('ctr', ascending=False).head(5)
                 if df_pot.empty: df_pot = df_clean.sort_values('ctr', ascending=False).head(5)
                 add_df_to_word(self.doc, apply_report_labels(df_pot, {'dimension_item': 'Áâà‰Ωç'}), "5.2 Áâà‰ΩçÈ´òÊΩúÂäõ", level=2)

                 self.final_json['5_placement_analysis'] = {"top_spend": df_top5.to_dict('records'), "high_potential": df_pot.to_dict('records')}

        # 7. Êû∂ÊûÑËØäÊñ≠
        rows = []
        # È¢ÑÁÆó
        if "Master_Overview" in self.merged_dfs:
             metrics = calc_metrics_dict(self.merged_dfs["Master_Overview"])
             rows.append({"Ê®°Âùó": "È¢ÑÁÆóÁªìÊûÑ", "ÂΩìÂâçÁªìÊûÑÊï∞ÊçÆË°®Áé∞": f"ÊÄªËä±Ë¥π: ${metrics.get('spend',0):,.2f}\nCPA: ${metrics.get('cpa',0):.2f}\nROAS: {metrics.get('roas',0):.2f}", "Â≠òÂú®ÁöÑÈóÆÈ¢ò": ""})

        # Âèó‰ºóÁªìÊûÑ
        if "Master_Breakdown" in self.merged_dfs:
            df_bd = self.merged_dfs["Master_Breakdown"]
            mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["Âèó‰ºó", "Audience"]))
            df_aud = df_bd[mask]
            s_col = find_column_fuzzy(df_aud, ['spend']); active_count = len(df_aud[df_aud[s_col] > 0]) if s_col else 0
            top_share = "0%"
            if not df_aud.empty and s_col:
                total_s = df_aud[s_col].sum()
                if total_s > 0: top_share = f"{df_aud[s_col].max()/total_s:.1%}"
            rows.append({"Ê®°Âùó": "Âèó‰ºóÁªìÊûÑ", "ÂΩìÂâçÁªìÊûÑÊï∞ÊçÆË°®Áé∞": f"Ê¥ªË∑ÉÂèó‰ºóÁªÑÊï∞: {active_count}\nTop1 Ëä±Ë¥πÂç†ÊØî: {top_share}", "Â≠òÂú®ÁöÑÈóÆÈ¢ò": ""})

        # Á¥†ÊùêÁªìÊûÑ
        if "Master_Creative" in self.merged_dfs:
             df_cr = self.merged_dfs["Master_Creative"]
             mask = df_cr['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["Á¥†Êùê", "Creative"]))
             df_mat = df_cr[mask]
             s_col = find_column_fuzzy(df_mat, ['spend']); active_count = len(df_mat[df_mat[s_col] > 0]) if s_col else 0
             rows.append({"Ê®°Âùó": "Á¥†ÊùêÁªìÊûÑ", "ÂΩìÂâçÁªìÊûÑÊï∞ÊçÆË°®Áé∞": f"Ê¥ªË∑ÉÁ¥†ÊùêÊï∞: {active_count}", "Â≠òÂú®ÁöÑÈóÆÈ¢ò": ""})

        df_struct = pd.DataFrame(rows)
        add_df_to_word(self.doc, df_struct, "7. ÂπøÂëäÊû∂ÊûÑÂàÜÊûê", level=1)
        self.final_json['7_structure_audit'] = df_struct.to_dict(orient='records')


# ==========================================
# PART 4: Streamlit UI
# ==========================================
def main():
    st.set_page_config(page_title="Auto-Merge & Analysis V20.10", layout="wide")
    st.title("üìä ÂπøÂëäÊï∞ÊçÆÊ∏ÖÊ¥ó‰∏éÈôçÁª¥ÂêàÂπ∂ (Auto-Merge V4.2 + V20.10)")

    st.markdown("""
    **ÂäüËÉΩËØ¥ÊòéÔºö**
    1. **ETLÈò∂ÊÆµ (Auto-Merge V4.2)**ÔºöÂÆåÂÖ®Â§çÂàªËÑöÊú¨1ÈÄªËæëÔºåÂåÖÊã¨Â≠óÊÆµÊò†Â∞Ñ„ÄÅÊï∞ÂÄºÊ∏ÖÊ¥ó„ÄÅ‰ª•Âèä**Á¥†Êùê/ËêΩÂú∞È°µ Top10 Êà™Êñ≠**„ÄÇ
    2. **Êä•ÂëäÈò∂ÊÆµ (V20.10)**ÔºöÂ§çÂàªËÑöÊú¨2ÈÄªËæëÔºåÁîüÊàêÊû∂ÊûÑËØäÊñ≠Ë°®„ÄÅWord Êä•ÂëäÂèä Gemini ÂàÜÊûêÁî® JSON„ÄÇ
    """)

    col1, col2 = st.columns(2)
    with col1:
        raw_file = st.file_uploader("1. ‰∏ä‰º† [Êï∞ÊçÆÊä•Ë°®] (Excel)", type=["xlsx", "xls"])
    with col2:
        bench_file = st.file_uploader("2. ‰∏ä‰º† [Ë°å‰∏öBenchmark] (Excel, ÂèØÈÄâ)", type=["xlsx", "xls"])

    if st.button("üöÄ ÂºÄÂßãÊâßË°åÂÖ®ÊµÅÁ®ã"):
        if not raw_file:
            st.error("ËØ∑Ëá≥Â∞ë‰∏ä‰º†Êï∞ÊçÆÊä•Ë°®ÔºÅ")
            return

        processor = AdReportProcessor(raw_file, bench_file)

        try:
            with st.spinner("Èò∂ÊÆµ 1/2: Êï∞ÊçÆÊ∏ÖÊ¥ó„ÄÅTop10Êà™Êñ≠„ÄÅÈôçÁª¥ÂêàÂπ∂..."):
                processor.process_etl()
                st.success("‚úÖ Èò∂ÊÆµ 1 ÂÆåÊàêÔºöMaster Tables Â∑≤ÁîüÊàê")

                # Preview Master Data
                with st.expander("Êü•ÁúãÈôçÁª¥ÂêàÂπ∂ÂêéÁöÑÊï∞ÊçÆ (Master Tables)"):
                    tabs = st.tabs(processor.merged_dfs.keys())
                    for i, (k, v) in enumerate(processor.merged_dfs.items()):
                        with tabs[i]: st.dataframe(v.head(20))

            with st.spinner("Èò∂ÊÆµ 2/2: ÁîüÊàêÊû∂ÊûÑËØäÊñ≠„ÄÅWordÊä•Âëä & JSON..."):
                processor.generate_report()
                st.success("‚úÖ Èò∂ÊÆµ 2 ÂÆåÊàêÔºöÊä•ÂëäÂ∑≤ÁîüÊàê")

            st.divider()

            # --- Downloads ---
            c1, c2, c3 = st.columns(3)

            # 1. JSON (Gemini)
            json_str = json.dumps(processor.final_json, indent=4, ensure_ascii=False)
            c1.download_button("üì• ‰∏ãËΩΩ JSON (GeminiÁî®)", json_str, "Ad_Report_Data.json", "application/json")

            # 2. Excel (Merged Data)
            output_xls = io.BytesIO()
            with pd.ExcelWriter(output_xls, engine='xlsxwriter') as writer:
                for name, df in processor.merged_dfs.items(): df.to_excel(writer, sheet_name=name, index=False)
            c2.download_button("üì• ‰∏ãËΩΩ Excel (ÂêàÂπ∂ÂêéÊï∞ÊçÆ)", output_xls.getvalue(), "Merged_Ad_Report_Final.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

            # 3. Word (Report)
            output_doc = io.BytesIO()
            processor.doc.save(output_doc)
            c3.download_button("üì• ‰∏ãËΩΩ Word (ÊúÄÁªàÊä•Âëä)", output_doc.getvalue(), "Ad_Report_Final_V20_10.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document")

        except Exception as e:
            st.error(f"Â§ÑÁêÜËøáÁ®ã‰∏≠ÂèëÁîüÈîôËØØ: {str(e)}")
            st.exception(e)

if __name__ == "__main__":
    main()

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import pandas as pd
# import numpy as np
# import io
# import json
# import xlsxwriter
# from docx import Document
# from docx.shared import Pt, RGBColor
# from docx.enum.text import WD_ALIGN_PARAGRAPH
# from docx.oxml.shared import OxmlElement
# from docx.oxml.ns import qn
# import docx.opc.constants
# 
# # ==========================================
# # PART 1: ÈÖçÁΩÆÂå∫Âüü
# # ==========================================
# # ËøôÊòØ‰∏Ä‰∏™ÂÖ®Â±ÄÁöÑ‚ÄúÂ≠óÂÖ∏ÁøªËØëÊú¨‚ÄùÔºåÂÜ≥ÂÆö‰∫ÜÊâÄÊúâÊñá‰ª∂‰∏≠ÂàóÂêçÂè´‰ªÄ‰πà
# REPORT_MAPPING = {
#     # Ê†∏ÂøÉÊåáÊ†á
#     "spend": "Ëä±Ë¥π ($)",
#     "roas": "ROAS",
#     "purchases": "Ë¥≠‰π∞Ê¨°Êï∞",
#     "purchase_value": "Ë¥≠‰π∞ÊÄª‰ª∑ÂÄº ($)",
#     "cpa": "CPA (ÂçïÊ¨°ÊàêÊïàË¥πÁî® $)",
#     "ctr": "CTR (ÁÇπÂáªÁéá %)",
#     "cpm": "CPM (ÂçÉÊ¨°Â±ïÁ§∫Ë¥π $)",
#     "cpc": "CPC (ÂçïÊ¨°ÁÇπÂáªË¥π $)",
#     "aov": "ÂÆ¢Âçï‰ª∑ ($)",
#     "impressions": "Â±ïÁé∞Èáè",
#     "clicks": "ÁÇπÂáªÈáè (All)",
#     "clicks_all": "ÁÇπÂáªÈáè (All)",
#     "ctr_all": "ÁÇπÂáªÁéá (All %)",
# 
#     # ÊºèÊñóÊåáÊ†á
#     "landing_page_views": "ËêΩÂú∞È°µËÆøÈóÆÈáè",
#     "add_to_cart": "Âä†Ë¥≠Ê¨°Êï∞",
#     "initiate_checkout": "ÁªìË¥¶ÂèëËµ∑Êï∞ (IC)",
#     "rate_click_to_lp": "ÁÇπÂáª‚ÜíËêΩÂú∞È°µËΩ¨ÂåñÁéá (%)",
#     "rate_lp_to_atc": "ËêΩÂú∞È°µ‚ÜíÂä†Ë¥≠ËΩ¨ÂåñÁéá (%)",
#     "rate_atc_to_ic": "Âä†Ë¥≠‚ÜíICËΩ¨ÂåñÁéá (%)",
#     "rate_ic_to_pur": "IC‚ÜíË¥≠‰π∞ËΩ¨ÂåñÁéá (%)",
#     "cvr_purchase": "CVR (ÂÖ®Á´ôËΩ¨ÂåñÁéá %)",
#     "cvr_lp_to_pur": "LP‚ÜíË¥≠‰π∞ËΩ¨ÂåñÁéá (%)",
# 
#     # Áª¥Â∫¶Â≠óÊÆµ
#     "date_range": "Êó•Êúü/Êó∂ÊÆµ",
#     "dimension_item": "Áª¥Â∫¶ÂêçÁß∞",
#     "content_item": "ÂÜÖÂÆπÂêçÁß∞",
#     "Source_Sheet": "Êï∞ÊçÆÊù•Ê∫êË°®",
# 
#     # Âèó‰ºó/ÂõΩÂÆ∂/Á¥†ÊùêÁ≠âÁªÜÂàÜÂ≠óÊÆµ
#     "campaign_type": "ÊäïÊîæÊ®°Âºè",
#     "adset_name": "ÂπøÂëäÁªÑÂêçÁß∞",
#     "adset_id": "ÂπøÂëäÁªÑID",
#     "custom_audience_settings": "Ëá™ÂÆö‰πâÂèó‰ºóÊ∫ê",
#     "converting_keywords": "È´òÊΩúÂÖ¥Ë∂£ËØç",
#     "audience_type": "Âèó‰ºóÁ≠ñÁï•",
#     "country": "ÂõΩÂÆ∂",
#     "converting_countries": "‰∏ªË¶ÅÊàêÊïàÂõΩÂÆ∂",
#     "converting_genders": "‰∏ªË¶ÅÊàêÊïàÊÄßÂà´",
#     "converting_ages": "‰∏ªË¶ÅÊàêÊïàÂπ¥ÈæÑ",
#     "age_group": "Âπ¥ÈæÑÊÆµ",
#     "gender": "ÊÄßÂà´",
#     "creative_name": "Á¥†ÊùêÂêçÁß∞",
#     "placement": "Áâà‰Ωç",
#     "landing_page_url": "È°µÈù¢ URL",
# 
#     # ËØäÊñ≠Â≠óÊÆµ
#     "mom_change": "ÁéØÊØîÊ≥¢Âä®",
#     "anomaly_metric_name": "ÂºÇÂ∏∏È°π",
#     "Label": "Êï∞ÊçÆÊ†áÁ≠æ",
#     "metric": "ÊåáÊ†áÂêçÁß∞",
#     "current_value": "ÂΩìÂâçÊï∞ÂÄº",
#     "benchmark_value": "Â§ßÁõòÂü∫ÂáÜ",
#     "diff": "Â∑ÆÂÄº",
#     "status": "Áä∂ÊÄÅËØÑ‰ª∑",
#     "Module": "Ê®°Âùó",
#     "Stats": "ÁªüËÆ°Êï∞ÊçÆ",
#     "Health": "ÂÅ•Â∫∑Â∫¶"
# }
# 
# # ËæìÂÖ•Êñá‰ª∂ÁöÑÂàóÂêçËØÜÂà´ÈÖçÁΩÆ (‰øùÊåÅ‰∏çÂèò)
# COMMON_METRICS = {
#     "spend": ["Ëä±Ë¥πÈáëÈ¢ù(USD)", "Ëä±Ë¥πÈáëÈ¢ù ÔºàUSDÔºâ", "Ëä±Ë¥πÈáëÈ¢ù (USD)", "Ëä±Ë¥πÈáëÈ¢ù", "Amount Spent"],
#     "roas": ["ÂπøÂëäËä±Ë¥πÂõûÊä• (ROAS) - Ë¥≠Áâ©", "ÂπøÂëäËä±Ë¥πÂõûÊä•ÔºàROASÔºâ-Ë¥≠Áâ©", "ROAS", "Purchase ROAS"],
#     "purchases": ["Ë¥≠‰π∞Ê¨°Êï∞", "ÊàêÊïàÊï∞Èáè", "ÊàêÊïà", "Purchases", "Results"],
#     "cpa": ["ÂçïÊ¨°Ë¥≠‰π∞Ë¥πÁî®", "ÂçïÊ¨°Ë¥≠Áâ©ÊàêÊú¨", "ÂçïÊ¨°ÊàêÊïàÊàêÊú¨", "ÂçïÊ¨°ÊàêÊïàË¥πÁî®", "Cost per Purchase"],
#     "ctr": ["ÈìæÊé•ÁÇπÂáªÁéá", "ÈìæÊé•ÁÇπÂáªÁéáÔºà%)", "ÈìæÊé•ÁÇπÂáªÁéáÔºà%Ôºâ", "CTR"],
#     "cpm": ["ÂçÉÊ¨°Â±ïÁ§∫Ë¥πÁî®", "CPM"],
#     "clicks": ["ÁÇπÂáª", "ÈìæÊé•ÁÇπÂáª", "Clicks (All)"],
#     "impressions": ["ÊõùÂÖâ", "Â±ïÁ§∫Ê¨°Êï∞", "Impressions"],
#     "purchase_value": ["Ë¥≠‰π∞‰ª∑ÂÄº", "Ë¥≠Áâ©‰ª∑ÂÄº", "Purchase Conversion Value"],
#     "aov": ["ÂçïÊ¨°Ë¥≠‰π∞‰ª∑ÂÄº", "ÂçïÊ¨°Ë¥≠Áâ©‰ª∑ÂÄº"]
# }
# 
# SHEET_MAPPINGS = {
#     "Êï¥‰ΩìÊï∞ÊçÆ": {
#         **COMMON_METRICS,
#         "date_range": ["Êó∂Èó¥ËåÉÂõ¥", "Date", "Day"],
#         "clicks_all": ["ÁÇπÂáª", "Clicks"],
#         "landing_page_views": ["ËêΩÂú∞È°µÊµèËßàÈáè", "Landing Page Views"],
#         "add_to_cart": ["Âä†ÂÖ•Ë¥≠Áâ©ËΩ¶", "Adds to Cart"],
#         "initiate_checkout": ["ÁªìË¥¶ÂèëËµ∑Ê¨°Êï∞", "Initiate Checkout"],
#         "rate_click_to_lp": ["ÁÇπÂáª-ËêΩÂú∞È°µÊµèËßàËΩ¨ÂåñÁéá"],
#         "rate_lp_to_atc": ["ËêΩÂú∞È°µÊµèËßà-Âä†Ë¥≠ËΩ¨ÂåñÁéá"],
#         "rate_atc_to_ic": ["Âä†Ë¥≠-ÁªìË¥¶ËΩ¨ÂåñÁéá"],
#         "rate_ic_to_pur": ["ÁªìË¥¶-Ë¥≠‰π∞ËΩ¨ÂåñÁéá"]
#     },
#     "ÂàÜÊó∂ÊÆµÊï∞ÊçÆ": {
#         **COMMON_METRICS,
#         "date_range": ["Êó∂Èó¥ËåÉÂõ¥", "Time of Day", "Hourly"],
#         "landing_page_views": ["ËêΩÂú∞È°µÊµèËßàÈáè"],
#         "add_to_cart": ["Âä†ÂÖ•Ë¥≠Áâ©ËΩ¶"],
#         "initiate_checkout": ["ÁªìË¥¶ÂèëËµ∑Ê¨°Êï∞"],
#     },
#     "ÂºÇÂ∏∏ÊåáÊ†á": {
#         "anomaly_metric_name": ["ÂºÇÂ∏∏ÊåáÊ†á"],
#         "mom_change": ["ÁéØÊØî"]
#     },
#     "ÂπøÂëäÊû∂ÊûÑ": {**COMMON_METRICS, "dimension_item": ["ÂπøÂëäÁ±ªÂûã", "Campaign Name"]},
#     "Âèó‰ºóÁªÑ": {
#         **COMMON_METRICS,
#         "dimension_item": ["ÂπøÂëäÁªÑ", "ÂπøÂëäÁªÑId", "Ad Set Name"],
#         "custom_audience_settings": ["ËÆæÁΩÆÁöÑËá™ÂÆö‰πâÂèó‰ºó"],
#         "converting_keywords": ["‰∫ßÁîüÊàêÊïàÁöÑÂÖ≥ÈîÆËØç"]
#     },
#     "Âèó‰ºóÁ±ªÂûã": {**COMMON_METRICS, "dimension_item": ["Âèó‰ºóÁ±ªÂûã"]},
#     "ÂõΩÂÆ∂": {**COMMON_METRICS, "dimension_item": ["ÂõΩÂÆ∂/Âú∞Âå∫", "ÂõΩÂÆ∂", "Country", "Region"]},
#     "Âπ¥ÈæÑ": {**COMMON_METRICS, "dimension_item": ["Âπ¥ÈæÑ", "Age"]},
#     "ÊÄßÂà´": {**COMMON_METRICS, "dimension_item": ["ÊÄßÂà´", "Gender"]},
#     "Âπ≥Âè∞&Áâà‰Ωç": {**COMMON_METRICS, "dimension_item": ["Âπ≥Âè∞&Áâà‰Ωç", "Placement", "Platform"]},
#     "Á¥†Êùê": {
#         **COMMON_METRICS,
#         "content_item": ["Á¥†Êùê", "Ad Name", "Creative Name"],
#         "cvr_lp_to_pur": ["ËêΩÂú∞È°µÊµèËßà-Ë¥≠‰π∞ËΩ¨ÂåñÁéá"]
#     },
#     "ËêΩÂú∞È°µ": {
#         **COMMON_METRICS,
#         "content_item": ["ËêΩÂú∞È°µurl", "ËêΩÂú∞È°µ", "Website URL"],
#         "ctr_all": ["ÊõùÂÖâ-ÁÇπÂáªËΩ¨ÂåñÁéá"],
#         "rate_lp_to_atc": ["ËêΩÂú∞È°µÊµèËßà-Âä†Ë¥≠ËΩ¨ÂåñÁéá", "ËêΩÂú∞È°µÊµèËßà-Ë¥≠Áâ©ËΩ¨ÂåñÁéá"]
#     }
# }
# 
# GROUP_CONFIG = {
#     "Master_Overview": ["Êï¥‰ΩìÊï∞ÊçÆ", "ÂàÜÊó∂ÊÆµÊï∞ÊçÆ", "ÂºÇÂ∏∏ÊåáÊ†á"],
#     "Master_Breakdown": ["ÂπøÂëäÊû∂ÊûÑ", "Âèó‰ºóÁªÑ", "Âèó‰ºóÁ±ªÂûã", "ÂõΩÂÆ∂", "Âπ¥ÈæÑ", "ÊÄßÂà´", "Âπ≥Âè∞&Áâà‰Ωç"],
#     "Master_Creative": ["Á¥†Êùê", "ËêΩÂú∞È°µ"]
# }
# 
# FIELD_ALIASES = {
#     "adset_id": ["adset_id", "ad set id", "adset id", "ÂπøÂëäÁªÑÁºñÂè∑", "ÂπøÂëäÁªÑid", "adset_name", "ad set name"],
#     "converting_countries": ["converting_countries", "country", "region", "ÂõΩÂÆ∂", "Âú∞Âå∫"],
#     "converting_genders": ["converting_genders", "gender", "ÊÄßÂà´"],
#     "converting_ages": ["converting_ages", "age", "Âπ¥ÈæÑ", "age_group"],
#     "converting_keywords": ["converting_keywords", "keywords", "interests", "ÂÖ¥Ë∂£", "ÂÖ≥ÈîÆËØç"],
#     "spend": ["spend", "amount spent", "cost", "Ëä±Ë¥π", "Ê∂àËÄó"],
#     "purchases": ["purchases", "results", "result", "ÊàêÊïà", "Ë¥≠‰π∞"],
#     "roas": ["roas", "return on ad spend", "purchase roas"],
#     "purchase_value": ["purchase_value", "conversion value", "value", "ÊÄª‰ª∑ÂÄº", "gmv", "Ë¥≠‰π∞ÊÄª‰ª∑ÂÄº"],
#     "clicks": ["clicks", "clicks (all)", "ÁÇπÂáªÈáè", "clicks_all"],
#     "impressions": ["impressions", "Â±ïÁ§∫", "Â±ïÁé∞"],
#     "ctr_all": ["ctr_all", "ctr (all)", "ÁÇπÂáªÁéá (all)"]
# }
# 
# # ==========================================
# # PART 2: Ê†∏ÂøÉÂ∑•ÂÖ∑ÂáΩÊï∞
# # ==========================================
# 
# def clean_numeric(val):
#     if pd.isna(val): return 0.0
#     if isinstance(val, (int, float)): return float(val)
#     val_str = str(val).strip().replace('$', '').replace('¬•', '').replace(',', '')
#     if '%' in val_str:
#         try: return float(val_str.replace('%', '')) / 100.0
#         except: return 0.0
#     try: return float(val_str)
#     except: return 0.0
# 
# def find_column_fuzzy(df, keywords):
#     for kw in keywords:
#         if kw in df.columns: return kw
#     df_cols_norm = {c.lower().replace(' ', '').replace('_', ''): c for c in df.columns}
#     for kw in keywords:
#         kw_norm = kw.lower().replace(' ', '').replace('_', '')
#         if kw_norm in df_cols_norm: return df_cols_norm[kw_norm]
#     for col in df.columns:
#         col_lower = col.lower()
#         for kw in keywords:
#             if kw.lower() in col_lower: return col
#     return None
# 
# def calc_metrics_dict(df_chunk):
#     res = {}
#     if df_chunk.empty: return res
#     sums = {}
#     targets = ['spend', 'clicks', 'impressions', 'purchases', 'purchase_value',
#                'landing_page_views', 'add_to_cart', 'initiate_checkout']
# 
#     for t in targets:
#         aliases = FIELD_ALIASES.get(t, [t])
#         if t == 'purchase_value' and 'value' not in aliases: aliases.append('value')
#         col = find_column_fuzzy(df_chunk, aliases)
#         if col:
#              sums[t] = df_chunk[col].apply(clean_numeric).sum()
#         else:
#              sums[t] = 0.0
# 
#     eps = 1e-9
#     res['spend'] = float(sums['spend'])
#     res['roas'] = float(sums['purchase_value'] / (sums['spend'] + eps))
#     res['cpm'] = float((sums['spend'] / (sums['impressions'] + eps)) * 1000)
#     res['cpc'] = float(sums['spend'] / (sums['clicks'] + eps))
#     res['ctr'] = float(sums['clicks'] / (sums['impressions'] + eps))
#     res['cpa'] = float(sums['spend'] / sums['purchases'] if sums['purchases'] > 0 else 0.0)
#     res['cvr_purchase'] = float(sums['purchases'] / (sums['clicks'] + eps))
#     res['rate_click_to_lp'] = float(sums['landing_page_views'] / (sums['clicks'] + eps))
#     res['rate_lp_to_atc'] = float(sums['add_to_cart'] / (sums['landing_page_views'] + eps))
#     res['rate_atc_to_ic'] = float(sums['initiate_checkout'] / (sums['add_to_cart'] + eps))
#     res['rate_ic_to_pur'] = float(sums['purchases'] / (sums['initiate_checkout'] + eps))
#     res['aov'] = float(sums['purchase_value'] / (sums['purchases'] + eps))
#     res['purchases'] = int(sums['purchases'])
#     res['add_to_cart'] = int(sums['add_to_cart'])
#     res['purchase_value'] = float(sums['purchase_value'])
# 
#     date_col = find_column_fuzzy(df_chunk, ['date', 'time', 'range'])
#     if date_col:
#         try:
#             dates = pd.to_datetime(df_chunk[date_col], errors='coerce').dropna()
#             if not dates.empty: res['date_range'] = f"{dates.min():%Y-%m-%d} ~ {dates.max():%Y-%m-%d}"
#             else: res['date_range'] = "-"
#         except: res['date_range'] = "-"
#     else: res['date_range'] = "-"
#     return res
# 
# def format_cell_for_display(key, val, is_mom=False):
#     """Ê†ºÂºèÂåñÊï∞ÂÄºÔºöÁî®‰∫é Word ÊòæÁ§∫"""
#     if isinstance(val, str): return val
#     if is_mom:
#         if key == 'date_range': return val
#         return f"{val:+.2%}"
# 
#     k = str(key).lower()
#     if 'roas' in k: return f"{val:.2f}"
#     if any(x in k for x in ['rate', 'ctr', 'cvr', 'ÁÇπÂáªÁéá', 'ËΩ¨ÂåñÁéá']): return f"{val:.2%}"
#     if any(x in k for x in ['spend', 'cpm', 'cpc', 'value', 'aov', 'cpa', 'Ëä±Ë¥π', 'ÈáëÈ¢ù', 'ÂÆ¢Âçï‰ª∑', 'gmv']): return f"{val:,.2f}"
#     if any(x in k for x in ['purchases', 'cart', 'click', 'Ê¨°Êï∞', 'ÂçïÈáè', 'Â±ïÁé∞', 'ËÆøÈóÆÈáè', 'ÂèëËµ∑Êï∞']): return f"{val:,.0f}"
#     return f"{val}"
# 
# def extract_benchmark_values(df_bench):
#     targets = {'roas': (['roas'], True), 'cpm': (['cpm'], False), 'ctr': (['ctr'], True), 'cpc': (['cpc'], False), 'cpa': (['cpa_purchase', 'cpa'], False)}
#     extracted = {}
#     for metric, (aliases, higher_better) in targets.items():
#         found_col = None
#         for alias in aliases:
#             found_col = find_column_fuzzy(df_bench, [alias])
#             if found_col: break
#         if found_col:
#             try:
#                 s = df_bench[found_col].apply(clean_numeric)
#                 v = s[s>0].mean()
#                 if not pd.isna(v): extracted[metric] = [float(v), higher_better]
#             except: pass
#     return extracted
# 
# def remap_json_keys(obj, mapping):
#     """ÈÄíÂΩíÂ∞Ü JSON ÂØπË±°‰∏≠ÁöÑËã±Êñá Key ÊõøÊç¢‰∏∫‰∏≠ÊñáÂ±ïÁ§∫Âêç"""
#     if isinstance(obj, dict):
#         new_dict = {}
#         for k, v in obj.items():
#             # Â∞ùËØïËé∑ÂèñÊò†Â∞ÑÔºåÂ¶ÇÊûúÊ≤°ÊúâÂàô‰øùÊåÅÂéüÊ†∑
#             new_key = mapping.get(k, k)
#             new_dict[new_key] = remap_json_keys(v, mapping)
#         return new_dict
#     elif isinstance(obj, list):
#         return [remap_json_keys(i, mapping) for i in obj]
#     else:
#         return obj
# 
# # --- Word Doc Helpers ---
# def add_hyperlink(paragraph, url, text, color="0000FF", underline=True):
#     try:
#         part = paragraph.part
#         r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)
#         hyperlink = OxmlElement('w:hyperlink')
#         hyperlink.set(qn('r:id'), r_id)
#         new_run = OxmlElement('w:r')
#         rPr = OxmlElement('w:rPr')
#         if color:
#             c = OxmlElement('w:color')
#             c.set(qn('w:val'), color)
#             rPr.append(c)
#         if underline:
#             u = OxmlElement('w:u')
#             u.set(qn('w:val'), 'single')
#             rPr.append(u)
#         new_run.append(rPr)
#         new_run.text = text
#         hyperlink.append(new_run)
#         paragraph._p.append(hyperlink)
#         return hyperlink
#     except: return None
# 
# def apply_report_labels(df, custom_mapping=None):
#     if df.empty: return df
#     mapping = REPORT_MAPPING.copy()
#     if custom_mapping: mapping.update(custom_mapping)
#     return df.rename(columns=mapping)
# 
# def add_df_to_word(doc, df, title, level=1):
#     if df.empty: return
#     doc.add_heading(title, level=level)
#     t = doc.add_table(rows=df.shape[0]+1, cols=df.shape[1])
#     t.style = 'Table Grid'
# 
#     is_creative = "Á¥†Êùê" in title
#     is_landing = "ËêΩÂú∞È°µ" in title
#     link_col_idx = -1
# 
#     for j, col in enumerate(df.columns):
#         cell = t.cell(0, j)
#         cell.text = str(col)
#         if any(x in str(col).lower() for x in ["url", "link", "Á¥†Êùê", "ÂÜÖÂÆπ", "content"]):
#             link_col_idx = j
#         for p in cell.paragraphs:
#             for r in p.runs:
#                 r.font.bold = True
#                 r.font.size = Pt(8)
# 
#     for i in range(df.shape[0]):
#         label_prefix = "Á¥†Êùê" if is_creative else ("ËêΩÂú∞È°µ" if is_landing else "")
#         label_char = chr(65 + (i % 26))
#         if i >= 26: label_char += str(i // 26)
#         label_text = f"{label_prefix}{label_char}"
# 
#         for j in range(df.shape[1]):
#             val = df.iat[i, j]
#             cell = t.cell(i+1, j)
# 
#             if (is_creative or is_landing) and j == link_col_idx:
#                 try:
#                     p = cell.paragraphs[0]
#                     url = str(val).strip()
#                     if len(url) > 5 and ("http" in url or "www" in url):
#                         add_hyperlink(p, url, label_text)
#                     else:
#                         cell.text = str(val)
#                 except: cell.text = str(val)
#             else:
#                 cell.text = str(val)
#                 if "ÁªìËÆ∫" in str(df.columns[j]) or "ËØÑ‰ª∑" in str(df.columns[j]):
#                     if "‚úÖ" in str(val): cell.paragraphs[0].runs[0].font.color.rgb = RGBColor(0, 128, 0)
#                     if "‚ö†Ô∏è" in str(val): cell.paragraphs[0].runs[0].font.color.rgb = RGBColor(255, 0, 0)
# 
#             for p in cell.paragraphs:
#                 for r in p.runs: r.font.size = Pt(8)
#     doc.add_paragraph("\n")
# 
# # ==========================================
# # PART 3: ‰∏ªÈÄªËæëÁ±ª
# # ==========================================
# 
# class AdReportProcessor:
#     def __init__(self, raw_file, bench_file=None):
#         self.raw_file = raw_file
#         self.bench_file = bench_file
#         self.processed_dfs = {}
#         self.merged_dfs = {}
#         self.final_json = {}
#         self.doc = Document()
# 
#     def process_etl(self):
#         xls = pd.ExcelFile(self.raw_file)
# 
#         for sheet_name, mapping in SHEET_MAPPINGS.items():
#             if sheet_name in xls.sheet_names:
#                 df = pd.read_excel(xls, sheet_name=sheet_name)
#                 final_cols = {}
# 
#                 for std_col, raw_col_options in mapping.items():
#                     matched_col = None
#                     for option in raw_col_options:
#                         if option in df.columns:
#                             matched_col = option; break
#                         if not matched_col:
#                             for df_col in df.columns:
#                                 if option.replace(" ", "") == df_col.replace(" ", ""):
#                                     matched_col = df_col; break
#                         if matched_col: break
#                     if matched_col: final_cols[std_col] = matched_col
# 
#                 if final_cols:
#                     df_clean = df[list(final_cols.values())].rename(columns={v: k for k, v in final_cols.items()})
#                     text_cols = ['date_range', 'anomaly_metric_name', 'converting_keywords',
#                                  'custom_audience_settings', 'dimension_item', 'content_item']
#                     for col in df_clean.columns:
#                         if col not in text_cols:
#                             df_clean[col] = df_clean[col].apply(clean_numeric)
# 
#                     if "spend" in df_clean.columns:
#                          df_clean = df_clean.sort_values("spend", ascending=False)
# 
#                     df_clean["Source_Sheet"] = sheet_name
#                     self.processed_dfs[sheet_name] = df_clean
# 
#         for master_name, source_sheets in GROUP_CONFIG.items():
#             dfs_to_merge = [self.processed_dfs[src] for src in source_sheets if src in self.processed_dfs]
#             if dfs_to_merge:
#                 merged_df = pd.concat(dfs_to_merge, ignore_index=True)
#                 cols = list(merged_df.columns)
#                 priority_cols = ['Source_Sheet', 'date_range', 'dimension_item', 'content_item',
#                                  'spend', 'roas', 'purchases', 'cpa', 'clicks', 'impressions']
#                 new_order = [c for c in priority_cols if c in cols] + [c for c in cols if c not in priority_cols]
#                 self.merged_dfs[master_name] = merged_df[new_order]
# 
#     def generate_report(self):
#         benchmark_targets = {'roas': [2.0, True], 'cpm': [20.0, False], 'ctr': [0.015, True], 'cpc': [1.5, False], 'cpa': [30.0, False]}
#         if self.bench_file:
#             try:
#                 df_b = pd.read_excel(self.bench_file)
#                 benchmark_targets = extract_benchmark_values(df_b)
#             except: pass
# 
#         self.doc.add_heading('ÂπøÂëäÊäïÊîæÊ∑±Â∫¶ÂàÜÊûêÊä•Âëä', 0).alignment = WD_ALIGN_PARAGRAPH.CENTER
# 
#         self.final_json = {
#             "report_meta": {
#                 "title": "ÂπøÂëäÊäïÊîæÊ∑±Â∫¶ÂàÜÊûêÊä•Âëä",
#                 "generated_at": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S"),
#             },
#             "data": {}
#         }
# 
#         # --- 1. Êï∞ÊçÆÂ§ßÁõò ---
#         df_ov = pd.DataFrame()
#         if "Master_Overview" in self.merged_dfs:
#             df_src = self.merged_dfs["Master_Overview"]
#             mask = df_src['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["ÂàÜÊó∂", "Time"]))
#             df_ov = df_src[mask].copy() if not df_src[mask].empty else df_src.copy()
# 
#         if not df_ov.empty:
#             date_col = find_column_fuzzy(df_ov, ['date', 'time', 'Êó∂Èó¥'])
#             if date_col:
#                 df_ov['temp_date'] = pd.to_datetime(df_ov[date_col], errors='coerce')
#                 df_clean = df_ov.dropna(subset=['temp_date']).sort_values('temp_date')
#                 dates = df_clean['temp_date'].unique()
# 
#                 raw_overall = calc_metrics_dict(df_clean)
#                 raw_prev = {}; raw_curr = {}; raw_mom = {}
# 
#                 if len(dates) >= 2:
#                     mid_date = dates[len(dates)//2]
#                     raw_prev = calc_metrics_dict(df_clean[df_clean['temp_date'] < mid_date])
#                     raw_curr = calc_metrics_dict(df_clean[df_clean['temp_date'] >= mid_date])
# 
#                     for k, v_curr in raw_curr.items():
#                         if k == 'date_range': raw_mom[k] = "-"
#                         elif isinstance(v_curr, (int, float)):
#                             v_prev = raw_prev.get(k, 0)
#                             raw_mom[k] = (v_curr - v_prev) / v_prev if v_prev > 0 else 0.0
#                         else: raw_mom[k] = 0
#                 else:
#                     raw_curr = raw_overall
# 
#                 self.final_json['data']['overview'] = {
#                     "total": raw_overall,
#                     "current_period": raw_curr,
#                     "previous_period": raw_prev,
#                     "mom_growth": raw_mom
#                 }
# 
#                 col_order = ["date_range", "spend", "roas", "cpa", "cpm", "cpc", "ctr", "cvr_purchase",
#                              "rate_click_to_lp", "rate_lp_to_atc", "rate_ic_to_pur", "aov", "add_to_cart", "purchases"]
# 
#                 final_data_word = []
#                 for label, r in zip(["Êï¥‰ΩìÊï∞ÊçÆ", "‰∏äÂë®ÊúüÂÄº", "Êú¨Âë®Êúü", "ÁéØÊØî"], [raw_overall, raw_prev, raw_curr, raw_mom]):
#                     row = {"Label": label}
#                     is_m = (label == "ÁéØÊØî")
#                     for c in col_order:
#                         val = r.get(c, 0)
#                         row[c] = format_cell_for_display(c, val, is_mom=is_m)
#                     row['date_range'] = label
#                     final_data_word.append(row)
# 
#                 df_f_display = pd.DataFrame(final_data_word, columns=col_order)
#                 add_df_to_word(self.doc, apply_report_labels(df_f_display), "1. Êï∞ÊçÆÂ§ßÁõòÊÄªËßà", level=1)
# 
#                 # --- Benchmark ---
#                 bench_json = []
#                 bench_word_data = []
#                 comparison_base = raw_curr if raw_curr else raw_overall
# 
#                 for metric_key in ['roas', 'cpm', 'ctr', 'cpc', 'cpa']:
#                     curr_val = comparison_base.get(metric_key, 0)
#                     bench_val, higher_is_better = benchmark_targets.get(metric_key, [0, True])
# 
#                     diff = curr_val - bench_val
#                     status = "flat"
#                     conclusion_word = "-"
# 
#                     if curr_val != 0:
#                         if higher_is_better:
#                             if diff > 0: status = "better"; conclusion_word = "‚úÖ ‰ºò‰∫éÂ§ßÁõò"
#                             elif diff < 0: status = "worse"; conclusion_word = "‚ö†Ô∏è ‰Ωé‰∫éÂ§ßÁõò"
#                             else: status = "flat"; conclusion_word = "ÊåÅÂπ≥"
#                         else:
#                             if diff < 0: status = "better"; conclusion_word = "‚úÖ ‰ºò‰∫éÂ§ßÁõò"
#                             elif diff > 0: status = "worse"; conclusion_word = "‚ö†Ô∏è È´ò‰∫éÂ§ßÁõò"
#                             else: status = "flat"; conclusion_word = "ÊåÅÂπ≥"
# 
#                     bench_json.append({
#                         "metric": metric_key,
#                         "current_value": curr_val,
#                         "benchmark_value": bench_val,
#                         "diff": diff,
#                         "status": status
#                     })
# 
#                     bench_word_data.append({
#                         "metric": REPORT_MAPPING.get(metric_key, metric_key.upper()),
#                         "current_value": format_cell_for_display(metric_key, curr_val),
#                         "benchmark_value": format_cell_for_display(metric_key, bench_val),
#                         "status": conclusion_word
#                     })
# 
#                 self.final_json['data']['benchmark_comparison'] = bench_json
#                 # Word ÈáåËøôÈáåÂ∑≤ÁªèÊâãÂä®Êò†Â∞ÑËøá‰∫ÜÔºåÊâÄ‰ª•ËøôÈáåÁî® apply_report_labels Â§ÑÁêÜÂâ©‰∏ãÁöÑ
#                 add_df_to_word(self.doc, apply_report_labels(pd.DataFrame(bench_word_data)), "2. Ë°å‰∏ö Benchmark ÂØπÊØî", level=1)
# 
#         # --- 3. Âèó‰ºóÁªÑ ---
#         self.doc.add_heading("3. Âèó‰ºóÁªÑÂàÜÊûê", level=1)
#         self.final_json['data']['audience_analysis'] = {}
# 
#         audience_configs = [
#             ("3.1 ÂõΩÂÆ∂ÂàÜÊûê", ["ÂõΩÂÆ∂", "Country"], True, "ÂõΩÂÆ∂", "country_breakdown"),
#             ("3.2 ÊÄßÂà´ÂàÜÊûê", ["ÊÄßÂà´", "Gender"], False, "ÊÄßÂà´", "gender_breakdown"),
#             ("3.3 Âπ¥ÈæÑÂàÜÊûê", ["Âπ¥ÈæÑ", "Age"], False, "Âπ¥ÈæÑÊÆµ", "age_breakdown"),
#             ("3.4 Âèó‰ºóÁªÑÂàÜÊûêË°®", ["Âèó‰ºó", "Audience"], True, "Âèó‰ºóÁªÑÂêçÁß∞", "adset_breakdown"),
#         ]
# 
#         if "Master_Breakdown" in self.merged_dfs:
#             df_bd = self.merged_dfs["Master_Breakdown"]
#             for title, keywords, top10, dim_label, json_key in audience_configs:
#                 mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in keywords))
#                 df_curr = df_bd[mask].copy()
# 
#                 if not df_curr.empty:
#                     if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan)
#                     if not find_column_fuzzy(df_curr, ['cpm']): df_curr['cpm'] = (df_curr['spend'] / df_curr['impressions'].replace(0, np.nan)) * 1000
#                     if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan)
#                     if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan)
#                     df_curr = df_curr.fillna(0)
# 
#                     req_cols = ["dimension_item", "spend", "ctr", "cpc", "cpm", "cpa", "roas", "purchases"]
#                     if "Âèó‰ºó" in title: req_cols += ["converting_countries", "converting_keywords"]
# 
#                     rename_map = {}; valid_cols = []
#                     for req in req_cols:
#                         aliases = FIELD_ALIASES.get(req, [req])
#                         found = find_column_fuzzy(df_curr, aliases)
#                         if found: valid_cols.append(found); rename_map[found] = req
#                         else: df_curr[req] = 0.0 if "converting" not in req else "-"; valid_cols.append(req)
# 
#                     df_std = df_curr[valid_cols].rename(columns=rename_map)
# 
#                     if "dimension_item" in df_std.columns:
#                           df_std = df_std[~df_std['dimension_item'].astype(str).str.lower().str.contains('unknow', na=False)]
#                     if top10 and 'spend' in df_std.columns:
#                         df_std = df_std.sort_values('spend', ascending=False).head(10)
# 
#                     # [JSON] Raw Data
#                     self.final_json['data']['audience_analysis'][json_key] = df_std.to_dict(orient='records')
# 
#                     # [Word]
#                     df_display = df_std.copy()
#                     for c in df_display.columns:
#                         if c in ['dimension_item', 'converting_countries', 'converting_keywords']: continue
#                         df_display[c] = df_display[c].apply(lambda x: format_cell_for_display(c, x))
# 
#                     df_display = apply_report_labels(df_display, custom_mapping={'dimension_item': dim_label})
#                     add_df_to_word(self.doc, df_display, title, level=2)
# 
#         # --- 4. Á¥†Êùê ---
#         if "Master_Creative" in self.merged_dfs:
#             df_cr = self.merged_dfs["Master_Creative"]
#             configs = [
#                 ("4. Á¥†ÊùêÂàÜÊûê", ["Á¥†Êùê", "Creative"], "Á¥†ÊùêÂêçÁß∞", "creative_analysis"),
#                 ("6. ËêΩÂú∞È°µÂàÜÊûê", ["ËêΩÂú∞È°µ", "Landing"], "ËêΩÂú∞È°µ URL", "landing_page_analysis")
#             ]
#             for title, keywords, label, json_key in configs:
#                 mask = df_cr['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in keywords))
#                 df_curr = df_cr[mask].copy()
#                 if not df_curr.empty:
#                     if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan)
#                     if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan)
#                     if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan)
#                     df_curr = df_curr.fillna(0)
# 
#                     req_cols = ["content_item", "spend", "ctr", "cpc", "cpm", "roas", "cpa"]
#                     rename_map = {}; valid_cols = []
#                     for req in req_cols:
#                         aliases = FIELD_ALIASES.get(req, [req])
#                         found = find_column_fuzzy(df_curr, aliases)
#                         if found: valid_cols.append(found); rename_map[found] = req
#                         else: df_curr[req] = 0.0; valid_cols.append(req)
# 
#                     df_std = df_curr[valid_cols].rename(columns=rename_map)
#                     if 'spend' in df_std.columns: df_std = df_std.sort_values('spend', ascending=False).head(10)
# 
#                     self.final_json['data'][json_key] = df_std.to_dict(orient='records')
# 
#                     df_display = df_std.copy()
#                     for c in df_display.columns:
#                          if c == 'content_item': continue
#                          df_display[c] = df_display[c].apply(lambda x: format_cell_for_display(c, x))
# 
#                     df_display = apply_report_labels(df_display, custom_mapping={'content_item': label})
#                     add_df_to_word(self.doc, df_display, title, level=1)
# 
#         # --- 5. Áâà‰Ωç ---
#         if "Master_Breakdown" in self.merged_dfs:
#              self.doc.add_heading("5. Áâà‰ΩçÂàÜÊûê", level=1)
#              df_bd = self.merged_dfs["Master_Breakdown"]
#              mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["Áâà‰Ωç", "Placement"]))
#              df_curr = df_bd[mask].copy()
# 
#              if not df_curr.empty:
#                  if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan)
#                  if not find_column_fuzzy(df_curr, ['cpm']): df_curr['cpm'] = (df_curr['spend'] / df_curr['impressions'].replace(0, np.nan)) * 1000
#                  df_curr = df_curr.fillna(0)
# 
#                  req_cols = ['dimension_item', 'spend', 'ctr', 'cpm', 'roas', 'cpa']
#                  rename_map = {}; valid_cols = []
#                  for c in req_cols:
#                       aliases = FIELD_ALIASES.get(c, [c])
#                       f = find_column_fuzzy(df_curr, aliases)
#                       if f: valid_cols.append(f); rename_map[f] = c
#                       else: df_curr[c] = 0.0; valid_cols.append(c)
# 
#                  df_std = df_curr[valid_cols].rename(columns=rename_map)
# 
#                  df_top5 = df_std.sort_values('spend', ascending=False).head(5)
#                  mean_ctr = df_std['ctr'].mean()
#                  mean_cpm = df_std['cpm'].mean()
#                  df_pot = df_std[(df_std['ctr'] > mean_ctr) & (df_std['cpm'] < mean_cpm)].sort_values('ctr', ascending=False).head(5)
# 
#                  self.final_json['data']['placement_analysis'] = {
#                      "top_spend": df_top5.to_dict('records'),
#                      "high_potential": df_pot.to_dict('records')
#                  }
# 
#                  for d, sub_title in [(df_top5, "5.1 Áâà‰ΩçËä±Ë¥π TOP 5"), (df_pot, "5.2 Áâà‰ΩçÈ´òÊΩúÂäõ")]:
#                      if not d.empty:
#                          d_disp = d.copy()
#                          for c in d_disp.columns:
#                              if c == 'dimension_item': continue
#                              d_disp[c] = d_disp[c].apply(lambda x: format_cell_for_display(c, x))
#                          add_df_to_word(self.doc, apply_report_labels(d_disp, {'dimension_item': 'Áâà‰Ωç'}), sub_title, level=2)
# 
#         # --- 7. Êû∂ÊûÑ ---
#         rows = []
#         if "Master_Breakdown" in self.merged_dfs:
#             df_bd = self.merged_dfs["Master_Breakdown"]
#             mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["Âèó‰ºó", "Audience"]))
#             df_aud = df_bd[mask]
#             if not df_aud.empty and 'spend' in df_aud.columns:
#                 active = len(df_aud[df_aud['spend'] > 0])
#                 total_s = df_aud['spend'].sum()
#                 top_s = df_aud['spend'].max()
#                 conc = top_s/total_s if total_s > 0 else 0
#                 rows.append({
#                     "Module": "Audience Structure",
#                     "Stats": f"Active Adsets: {active}",
#                     "Health": f"Top1 Share: {conc:.1%}"
#                 })
# 
#         self.final_json['data']['structure_audit'] = rows
#         if rows:
#             df_struct = pd.DataFrame(rows)
#             # Apply mapping here for Word
#             add_df_to_word(self.doc, apply_report_labels(df_struct), "7. ÂπøÂëäÊû∂ÊûÑÁÆÄÊûê", level=1)
# 
# 
# # ==========================================
# # PART 4: Streamlit UI
# # ==========================================
# def main():
#     st.set_page_config(page_title="Auto-Merge & Analysis V20.10", layout="wide")
#     st.title("ÂπøÂëä‰ºòÂåñÊä•ÂëäÊï∞ÊçÆÁªàË°®Áîü‰∫ß")
# 
#     st.info("ËØ∑ÊÇ®‰∏ä‰º†[Âë®ÊúüÊÄßÂ§çÁõòÊä•Âëä]„ÄÅ[Ë°å‰∏öbenchmark]‰∏§‰∏™Êï∞ÊçÆÊñá‰ª∂„ÄÇÊú¨Â∑•ÂÖ∑Â∞Ü‰∏∫ÊÇ®ËæìÂá∫‰∏âÁßçÊñá‰ª∂ÔºåJSONÊ†ºÂºèÂèØÁî®‰∫éÂ§ßÊ®°ÂûãÂàÜÊûêÔºåExcelÂèØÁî®‰∫éÊï∞ÊçÆÈÄèËßÜÔºåWordÊ†ºÂºèÂèØÁî®‰∫éÂÆ°Êü•„ÄÇÂª∫ËÆÆÔºöÊÇ®ÂèØÂè™ÈÄâÊã©‰∏ãËΩΩJSONÊ†ºÂºèÊñá‰ª∂ÔºåÂ¶ÇÊúâÂøÖË¶ÅÂÜç‰∏ãËΩΩÂÖ∂‰ªñÊ†ºÂºèÊñá‰ª∂„ÄÇ")
# 
#     col1, col2 = st.columns(2)
#     with col1:
#         raw_file = st.file_uploader("1.ËØ∑Âú®Ê≠§Â§Ñ‰∏ä‰º† [Âë®ÊúüÊÄßÂ§çÁõòÊä•Âëä]", type=["xlsx", "xls"])
#     with col2:
#         bench_file = st.file_uploader("2.ËØ∑Âú®Ê≠§Â§Ñ‰∏ä‰º† [Ë°å‰∏öBenchmark]", type=["xlsx", "xls"])
# 
#     if st.button("üöÄ ÂºÄÂßãÂ§ÑÁêÜ"):
#         if not raw_file:
#             st.error("ËØ∑Ëá≥Â∞ë‰∏ä‰º†Êï∞ÊçÆÊä•Ë°®ÔºÅ")
#             return
# 
#         processor = AdReportProcessor(raw_file, bench_file)
# 
#         try:
#             with st.spinner("Êï∞ÊçÆÊ∏ÖÊ¥ó„ÄÅÂêàÂπ∂„ÄÅÊåáÊ†áËÆ°ÁÆó‰∏≠..."):
#                 processor.process_etl()
#                 processor.generate_report()
#                 st.success("‚úÖ Êä•ÂëäÁîüÊàêÂÆåÊØïÔºÅ")
# 
#             st.divider()
#             c1, c2, c3 = st.columns(3)
# 
#             # 1. JSON (Key Êò†Â∞Ñ‰∏∫Â±ïÁ§∫Âêç)
#             # ‰ΩøÁî® remap_json_keys ÈÄíÂΩíÊõøÊç¢
#             final_json_display = remap_json_keys(processor.final_json, REPORT_MAPPING)
#             json_str = json.dumps(final_json_display, indent=4, ensure_ascii=False, default=str)
# 
#             c1.download_button(
#                 label="üì• ‰∏ãËΩΩ JSON (Áî®‰∫éÂ§ßÊ®°ÂûãÂàÜÊûê)",
#                 data=json_str,
#                 file_name="Ad_Report_Data_Display.json",
#                 mime="application/json"
#             )
# 
#             # 2. Excel (ÂàóÂêç Êò†Â∞Ñ‰∏∫Â±ïÁ§∫Âêç)
#             output_xls = io.BytesIO()
#             with pd.ExcelWriter(output_xls, engine='xlsxwriter') as writer:
#                 for name, df in processor.merged_dfs.items():
#                     # Âú®ÂÜôÂÖ• Excel ÂâçÔºåËøõË°åÂàóÂêçÊõøÊç¢
#                     df_display = df.rename(columns=REPORT_MAPPING)
#                     df_display.to_excel(writer, sheet_name=name, index=False)
# 
#             c2.download_button(
#                 label="üì• ‰∏ãËΩΩ Excel (Áî®‰∫éÊï∞ÊçÆÈÄèËßÜ)",
#                 data=output_xls.getvalue(),
#                 file_name="Merged_Ad_Data_Display.xlsx",
#                 mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
#             )
# 
#             # 3. Word (Êú¨Ë∫´Â∑≤ÊòØÂ±ïÁ§∫Âêç)
#             output_doc = io.BytesIO()
#             processor.doc.save(output_doc)
#             c3.download_button(
#                 label="üì• ‰∏ãËΩΩ Word (Áî®‰∫éÊï∞ÊçÆÂÆ°Êü•)",
#                 data=output_doc.getvalue(),
#                 file_name="Ad_Report_Final_V20_10.docx",
#                 mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
#             )
# 
#         except Exception as e:
#             st.error(f"Â§ÑÁêÜËøáÁ®ã‰∏≠ÂèëÁîüÈîôËØØ: {str(e)}")
#             st.exception(e)
# 
# if __name__ == "__main__":
#     main()

!pip install streamlit pyngrok xlsxwriter python-docx

import subprocess
import time

# 1. ÂêéÂè∞ÂêØÂä® Streamlit (Á°Æ‰øù app.py Â∑≤Â≠òÂú®)
process = subprocess.Popen(["streamlit", "run", "app.py"])

# 2. ‰ΩøÁî® Pinggy ÂàõÂª∫ÈößÈÅì (Êó†ÈúÄÂÆâË£ÖÔºåÂà©Áî®Á≥ªÁªüËá™Â∏¶ SSH)
print("üöÄ Ê≠£Âú®ÁîüÊàê Pinggy ÈìæÊé•ÔºåËØ∑Á®çÁ≠â 5 Áßí...")
# Â∞ÜÊó•ÂøóËæìÂá∫Âà∞ pinggy.log ‰ª•‰æøÊäìÂèñÈìæÊé•
!nohup ssh -p 443 -R0:localhost:8501 -o StrictHostKeyChecking=no -o ServerAliveInterval=30 a.pinggy.io > pinggy.log 2>&1 &

time.sleep(5)

# 3. ÊòæÁ§∫ÈìæÊé•
print("\nüëá ËØ∑ÁÇπÂáª‰∏ãÊñπÊòæÁ§∫ÁöÑÈìæÊé• (ÈÄöÂ∏∏‰ª• https://...pinggy.link ÁªìÂ∞æ):")
!grep -o "https://[^ ]*\.pinggy\.link" pinggy.log
