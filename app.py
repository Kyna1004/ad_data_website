import streamlit as st
import pandas as pd
import numpy as np
import io
import json
import xlsxwriter
from docx import Document
from docx.shared import Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.shared import OxmlElement
from docx.oxml.ns import qn
import docx.opc.constants
import time

# ==========================================
# PART 1: é…ç½®åŒºåŸŸ (å·²å¢å¼º 'add_to_cart' æ˜ å°„)
# ==========================================

COMMON_METRICS = {
    "spend": ["èŠ±è´¹é‡‘é¢(USD)", "èŠ±è´¹é‡‘é¢ ï¼ˆUSDï¼‰", "èŠ±è´¹é‡‘é¢ (USD)", "èŠ±è´¹é‡‘é¢", "Amount Spent", "Cost"],
    "roas": ["å¹¿å‘ŠèŠ±è´¹å›æŠ¥ (ROAS) - è´­ç‰©", "å¹¿å‘ŠèŠ±è´¹å›æŠ¥ï¼ˆROASï¼‰-è´­ç‰©", "ROAS", "Purchase ROAS", "Return on Ad Spend"],
    "purchases": ["è´­ä¹°æ¬¡æ•°", "æˆæ•ˆæ•°é‡", "æˆæ•ˆ", "Purchases", "Results", "Website Purchases"],
    "cpa": ["å•æ¬¡è´­ä¹°è´¹ç”¨", "å•æ¬¡è´­ç‰©æˆæœ¬", "å•æ¬¡æˆæ•ˆæˆæœ¬", "å•æ¬¡æˆæ•ˆè´¹ç”¨", "Cost per Purchase", "Cost per Result"],
    "ctr": ["é“¾æ¥ç‚¹å‡»ç‡", "é“¾æ¥ç‚¹å‡»ç‡ï¼ˆ%)", "é“¾æ¥ç‚¹å‡»ç‡ï¼ˆ%ï¼‰", "CTR", "Link CTR"],
    "cpm": ["åƒæ¬¡å±•ç¤ºè´¹ç”¨", "CPM", "Cost per 1,000 Impressions"],
    "clicks": ["ç‚¹å‡»", "é“¾æ¥ç‚¹å‡»", "Clicks", "Link Clicks"],
    "impressions": ["æ›å…‰", "å±•ç¤ºæ¬¡æ•°", "Impressions"],
    "purchase_value": ["è´­ä¹°ä»·å€¼", "è´­ç‰©ä»·å€¼", "Purchase Value", "Conversion Value"],
    "aov": ["å•æ¬¡è´­ä¹°ä»·å€¼", "å•æ¬¡è´­ç‰©ä»·å€¼"]
}

SHEET_MAPPINGS = {
    "æ•´ä½“æ•°æ®": {
        **COMMON_METRICS,
        "date_range": ["æ—¶é—´èŒƒå›´", "Date Range", "Time"],
        "clicks_all": ["ç‚¹å‡»", "ç‚¹å‡»(å…¨éƒ¨)", "Clicks (All)"],
        "landing_page_views": ["è½åœ°é¡µæµè§ˆé‡", "è½åœ°é¡µ", "Landing Page Views", "Landing"],
        # âœ… ä¿®æ”¹ç‚¹ï¼šå¢åŠ äº†æ›´å¤šå¸¸è§çš„åŠ è´­åˆ—ååˆ«å
        "add_to_cart": ["åŠ å…¥è´­ç‰©è½¦", "åŠ è´­", "Add to Cart", "Website Adds to Cart", "ç½‘ç«™åŠ è´­", "Adds to Cart"], 
        "initiate_checkout": ["ç»“è´¦å‘èµ·æ¬¡æ•°", "ç»“è´¦", "Initiate Checkout", "Website Initiated Checkouts", "ç½‘ç«™ç»“è´¦å‘èµ·"],
        "rate_click_to_lp": ["ç‚¹å‡»-è½åœ°é¡µæµè§ˆè½¬åŒ–ç‡"],
        "rate_lp_to_atc": ["è½åœ°é¡µæµè§ˆ-åŠ è´­è½¬åŒ–ç‡"],
        "rate_atc_to_ic": ["åŠ è´­-ç»“è´¦è½¬åŒ–ç‡"],
        "rate_ic_to_pur": ["ç»“è´¦-è´­ä¹°è½¬åŒ–ç‡"]
    },
    "åˆ†æ—¶æ®µæ•°æ®": {
        **COMMON_METRICS,
        "date_range": ["æ—¶é—´èŒƒå›´", "Day", "Date"],
        "landing_page_views": ["è½åœ°é¡µæµè§ˆé‡", "Landing Page Views"],
        # âœ… ä¿®æ”¹ç‚¹ï¼šç¡®ä¿è¿™é‡ŒåŒ…å«ã€åŠ å…¥è´­ç‰©è½¦ã€‘ä»¥åŠå…¶ä»–å˜ä½“
        "add_to_cart": ["åŠ å…¥è´­ç‰©è½¦", "åŠ è´­", "Add to Cart", "Website Adds to Cart", "ç½‘ç«™åŠ è´­", "Adds to Cart"],
        "initiate_checkout": ["ç»“è´¦å‘èµ·æ¬¡æ•°", "Initiate Checkout"],
        "rate_click_to_lp": ["ç‚¹å‡»-è½åœ°é¡µæµè§ˆè½¬åŒ–ç‡"],
        "rate_lp_to_atc": ["è½åœ°é¡µæµè§ˆ-åŠ è´­è½¬åŒ–ç‡"],
        "rate_atc_to_ic": ["åŠ è´­-ç»“è´¦è½¬åŒ–ç‡"],
        "rate_ic_to_pur": ["ç»“è´¦-è´­ä¹°è½¬åŒ–ç‡"]
    },
    "å¼‚å¸¸æŒ‡æ ‡": {
        "anomaly_metric_name": ["å¼‚å¸¸æŒ‡æ ‡"],
        "mom_change": ["ç¯æ¯”"]
    },
    "å¹¿å‘Šæ¶æ„": {**COMMON_METRICS, "dimension_item": ["å¹¿å‘Šç±»å‹"]},
    "å—ä¼—ç»„": {
        **COMMON_METRICS,
        "dimension_item": ["å¹¿å‘Šç»„", "å¹¿å‘Šç»„Id", "Ad Set Name"],
        "custom_audience_settings": ["è®¾ç½®çš„è‡ªå®šä¹‰å—ä¼—", "Custom Audiences"],
        "converting_keywords": ["äº§ç”Ÿæˆæ•ˆçš„å…³é”®è¯", "Interests", "Keywords"],
        "converting_countries": ["äº§ç”Ÿæˆæ•ˆçš„å›½å®¶", "å›½å®¶", "åœ°åŒº", "Country", "Region", "Location"],
        "converting_genders": ["äº§ç”Ÿæˆæ•ˆçš„æ€§åˆ«", "æ€§åˆ«", "Gender"],
        "converting_ages": ["äº§ç”Ÿæˆæ•ˆçš„å¹´é¾„", "å¹´é¾„", "Age", "Age Group"]
    },
    "å—ä¼—ç±»å‹": {**COMMON_METRICS, "dimension_item": ["å—ä¼—ç±»å‹"]},
    "å›½å®¶": {**COMMON_METRICS, "dimension_item": ["å›½å®¶/åœ°åŒº", "å›½å®¶"]},
    "å¹´é¾„": {**COMMON_METRICS, "dimension_item": ["å¹´é¾„"]},
    "æ€§åˆ«": {**COMMON_METRICS, "dimension_item": ["æ€§åˆ«"]},
    "å¹³å°&ç‰ˆä½": {**COMMON_METRICS, "dimension_item": ["å¹³å°&ç‰ˆä½"]},
    "ç´ æ": {
        **COMMON_METRICS,
        "content_item": ["ç´ æ", "Ad Name", "Creative Name"],
        "cvr_lp_to_pur": ["è½åœ°é¡µæµè§ˆ-è´­ä¹°è½¬åŒ–ç‡"]
    },
    "è½åœ°é¡µ": {
        **COMMON_METRICS,
        "content_item": ["è½åœ°é¡µurl", "è½åœ°é¡µ", "Website URL"],
        "ctr_all": ["æ›å…‰-ç‚¹å‡»è½¬åŒ–ç‡"],
        "rate_lp_to_atc": ["è½åœ°é¡µæµè§ˆ-åŠ è´­è½¬åŒ–ç‡", "è½åœ°é¡µæµè§ˆ-è´­ç‰©è½¬åŒ–ç‡"]
    }
}

GROUP_CONFIG = {
    "Master_Overview": ["æ•´ä½“æ•°æ®", "åˆ†æ—¶æ®µæ•°æ®", "å¼‚å¸¸æŒ‡æ ‡"],
    "Master_Breakdown": ["å¹¿å‘Šæ¶æ„", "å—ä¼—ç»„", "å—ä¼—ç±»å‹", "å›½å®¶", "å¹´é¾„", "æ€§åˆ«", "å¹³å°&ç‰ˆä½"],
    "Master_Creative": ["ç´ æ", "è½åœ°é¡µ"]
}

REPORT_MAPPING = {
    "spend": "èŠ±è´¹ ($)", "roas": "ROAS", "purchases": "è´­ä¹°æ¬¡æ•°", "purchase_value": "è´­ä¹°æ€»ä»·å€¼",
    "cpa": "CPA ($)", "ctr": "CTR (%)", "cpm": "CPM ($)", "aov": "å®¢å•ä»·",
    "impressions": "å±•ç°é‡", "clicks_all": "ç‚¹å‡»é‡ (All)", "clicks": "ç‚¹å‡»é‡ (All)", "ctr_all": "ç‚¹å‡»ç‡ (All)",
    "landing_page_views": "è½åœ°é¡µè®¿é—®é‡", "add_to_cart": "åŠ è´­æ¬¡æ•°", "initiate_checkout": "ç»“è´¦å‘èµ·æ•° (IC)",
    "rate_click_to_lp": "ç‚¹å‡» â†’ è½åœ°é¡µè®¿é—®è½¬åŒ–ç‡", "rate_lp_to_atc": "è½åœ°é¡µ â†’ åŠ è´­è½¬åŒ–ç‡",
    "rate_atc_to_ic": "åŠ è´­ â†’ è´­ä¹°è½¬åŒ–ç‡", "rate_ic_to_pur": "è´­ä¹°è½¬åŒ–ç‡",
    "cvr_purchase": "ç‚¹å‡» â†’ è´­ä¹°è½¬åŒ–ç‡", "cvr_lp_to_pur": "CVR (å…¨ç«™è½¬åŒ–ç‡)",
    "date_range": "æ—¥æœŸ/æ—¶æ®µ", "campaign_type": "æŠ•æ”¾æ¨¡å¼", "adset_name": "å¹¿å‘Šç»„ID", "adset_id": "å¹¿å‘Šç»„ID",
    "custom_audience_settings": "è‡ªå®šä¹‰å—ä¼—æº", "converting_keywords": "é«˜æ½œå…´è¶£è¯", "audience_type": "å—ä¼—ç­–ç•¥",
    "country": "å›½å®¶", "age_group": "å¹´é¾„", "gender": "æ€§åˆ«", "creative_name": "ç´ æåç§°", "placement": "ç‰ˆä½",
    "landing_page_url": "é¡µé¢ URL", "mom_change": "ç¯æ¯”æ³¢åŠ¨", "anomaly_metric_name": "å¼‚å¸¸é¡¹",
    "converting_countries": "äº§ç”Ÿæˆæ•ˆçš„å›½å®¶", "converting_genders": "äº§ç”Ÿæˆæ•ˆçš„æ€§åˆ«", "converting_ages": "äº§ç”Ÿæˆæ•ˆçš„å¹´é¾„"
}

FIELD_ALIASES = {
    "adset_id": ["adset_id", "ad set id", "adset id", "å¹¿å‘Šç»„ç¼–å·", "å¹¿å‘Šç»„id", "adset_name", "ad set name"],
    "converting_countries": ["converting_countries", "country", "region", "å›½å®¶", "åœ°åŒº", "location"],
    "converting_genders": ["converting_genders", "gender", "æ€§åˆ«"],
    "converting_ages": ["converting_ages", "age", "å¹´é¾„", "age_group"],
    "converting_keywords": ["converting_keywords", "keywords", "interests", "å…´è¶£", "å…³é”®è¯"],
    "spend": ["spend", "amount spent", "cost", "èŠ±è´¹", "æ¶ˆè€—"],
    "purchases": ["purchases", "results", "result", "æˆæ•ˆ", "è´­ä¹°"],
    "roas": ["roas", "return on ad spend", "purchase roas"],
    "purchase_value": ["purchase_value", "conversion value", "value", "æ€»ä»·å€¼", "gmv", "è´­ä¹°æ€»ä»·å€¼"],
    "clicks": ["clicks", "clicks (all)", "ç‚¹å‡»é‡", "clicks_all"],
    "impressions": ["impressions", "å±•ç¤º", "å±•ç°"],
    "ctr_all": ["ctr_all", "ctr (all)", "ç‚¹å‡»ç‡ (all)"],
    # âœ… ä¿®æ”¹ç‚¹ï¼šå¢åŠ  "ç½‘ç«™åŠ è´­", "adds to cart" ä»¥é˜²ä¸‡ä¸€
    "add_to_cart": ["add_to_cart", "åŠ å…¥è´­ç‰©è½¦", "åŠ è´­", "cart", "website adds to cart", "ç½‘ç«™åŠ è´­", "adds to cart"], 
    "initiate_checkout": ["initiate_checkout", "ç»“è´¦å‘èµ·æ¬¡æ•°", "ç»“è´¦", "checkout"],
    "landing_page_views": ["landing_page_views", "è½åœ°é¡µæµè§ˆé‡", "è½åœ°é¡µ", "landing"]
}

# ==========================================
# PART 2: æ ¸å¿ƒå·¥å…·å‡½æ•°
# ==========================================

def parse_float(value):
    if value is None: return 0.0
    try:
        if isinstance(value, (int, float)): return float(value)
        return clean_numeric_strict(value)
    except: return 0.0

def safe_div(numerator, denominator, multiplier=1.0):
    n = parse_float(numerator)
    d = parse_float(denominator)
    if d > 0: return (n / d) * multiplier
    else: return 0.0

def clean_numeric(val):
    if pd.isna(val): return 0.0
    if isinstance(val, (int, float)): return float(val)
    val_str = str(val).strip().replace('$', '').replace('Â¥', '').replace(',', '')
    if '%' in val_str: 
        val_str = val_str.replace('%', '')
        try: return float(val_str) / 100.0 
        except: return 0.0
    try: return float(val_str)
    except: return val # Return original if not number (for text columns)

def clean_numeric_strict(val): 
    if pd.isna(val): return 0.0
    if isinstance(val, (int, float)): return float(val)
    val_str = str(val).strip().replace('$', '').replace('Â¥', '').replace(',', '')
    if '%' in val_str: 
        val_str = val_str.replace('%', '')
        try: return float(val_str) / 100.0
        except: return 0.0
    try: return float(val_str)
    except: return 0.0

def find_column_fuzzy(df, keywords):
    # 1. ç²¾ç¡®åŒ¹é…
    for kw in keywords:
        if kw in df.columns: return kw
    
    # 2. å½’ä¸€åŒ–åŒ¹é… (å»ç©ºæ ¼ã€è½¬å°å†™)
    df_cols_norm = {c.lower().replace(' ', '').replace('_', ''): c for c in df.columns}
    for kw in keywords:
        kw_norm = kw.lower().replace(' ', '').replace('_', '')
        if kw_norm in df_cols_norm: return df_cols_norm[kw_norm]
    
    # 3. åŒ…å«åŒ¹é… (Contains)
    for col in df.columns:
        col_lower = col.lower()
        for kw in keywords:
            if kw.lower() in col_lower: return col
    return None

def calc_metrics_dict(df_chunk):
    res = {}
    if df_chunk.empty: return res
    sums = {}
    # ç¡®ä¿è¿™é‡ŒåŒ…å« add_to_cart
    targets = ['spend', 'clicks', 'impressions', 'purchases', 'purchase_value',
               'landing_page_views', 'add_to_cart', 'initiate_checkout']
    
    for t in targets:
        aliases = FIELD_ALIASES.get(t, [t])
        if t == 'purchase_value' and 'value' not in aliases: aliases.append('value')
        col = find_column_fuzzy(df_chunk, aliases)
        if col:
             # ç›´æ¥è¯»å–åˆ—å€¼å¹¶æ±‚å’Œ (å¯¹äºå•è¡Œå°±æ˜¯ç›´æ¥è¯»å–)
             sums[t] = df_chunk[col].apply(clean_numeric_strict).sum()
        else:
             sums[t] = 0.0

    res['spend'] = parse_float(sums.get('spend', 0))
    res['impressions'] = parse_float(sums.get('impressions', 0))
    res['clicks'] = parse_float(sums.get('clicks', 0))
    res['purchases'] = parse_float(sums.get('purchases', 0))
    res['purchase_value'] = parse_float(sums.get('purchase_value', 0))
    # âœ… è¿™é‡Œç›´æ¥è¯»å–ï¼Œä¸è¿›è¡Œå…¬å¼è®¡ç®—
    res['add_to_cart'] = parse_float(sums.get('add_to_cart', 0))
    res['initiate_checkout'] = parse_float(sums.get('initiate_checkout', 0))
    res['landing_page_views'] = parse_float(sums.get('landing_page_views', 0))
    
    res['roas'] = safe_div(sums.get('purchase_value'), sums.get('spend'))
    res['cpm'] = safe_div(sums.get('spend'), sums.get('impressions'), multiplier=1000)
    res['cpc'] = safe_div(sums.get('spend'), sums.get('clicks'))
    res['ctr'] = safe_div(sums.get('clicks'), sums.get('impressions'))
    res['cpa'] = safe_div(sums.get('spend'), sums.get('purchases'))
    res['cvr_purchase'] = safe_div(sums.get('purchases'), sums.get('clicks'))
    
    res['rate_click_to_lp'] = safe_div(sums.get('landing_page_views'), sums.get('clicks'))
    res['rate_lp_to_atc']   = safe_div(sums.get('add_to_cart'), sums.get('landing_page_views'))
    res['rate_atc_to_ic']   = safe_div(sums.get('initiate_checkout'), sums.get('add_to_cart'))
    res['rate_ic_to_pur']   = safe_div(sums.get('purchases'), sums.get('initiate_checkout'))
    
    res['aov'] = safe_div(sums.get('purchase_value'), sums.get('purchases'))

    date_col = find_column_fuzzy(df_chunk, ['date', 'time', 'range'])
    if date_col:
        try:
            dates = pd.to_datetime(df_chunk[date_col], errors='coerce').dropna()
            if not dates.empty: res['date_range'] = f"{dates.min():%Y-%m-%d} ~ {dates.max():%Y-%m-%d}"
            else: res['date_range'] = "-"
        except: res['date_range'] = "-"
    else: res['date_range'] = "-"
    return res 

def format_cell(key, val, is_mom=False):
    if isinstance(val, str): return val
    if is_mom:
        if key == 'date_range': return val
        return f"{val:+.2%}"
    k = str(key).lower()
    if 'roas' in k: return f"{val:.2f}"
    if any(x in k for x in ['rate', 'ctr', 'cvr', 'ç‚¹å‡»ç‡', 'è½¬åŒ–ç‡', 'ç€é™†ç‡', 'æ„å‘ç‡', 'æˆåŠŸç‡']): 
        return f"{val:.2%}" 
    if any(x in k for x in ['spend', 'cpm', 'cpc', 'value', 'aov', 'cpa', 'èŠ±è´¹', 'é‡‘é¢', 'å®¢å•ä»·', 'gmv', 'ä»·å€¼']): return f"{val:,.2f}"
    if any(x in k for x in ['purchases', 'cart', 'click', 'æ¬¡æ•°', 'å•é‡', 'ç‚¹å‡»', 'å±•ç°', 'è®¿é—®é‡', 'å‘èµ·æ•°']): return f"{val:,.0f}"
    return f"{val}"

def extract_benchmark_values(df_bench):
    targets = {'roas': (['roas'], True), 'cpm': (['cpm'], False), 'ctr': (['ctr'], True), 'cpc': (['cpc'], False), 'cpa': (['cpa_purchase', 'cpa'], False)}
    extracted = {}
    for metric, (aliases, higher_better) in targets.items():
        found_col = None
        for alias in aliases:
            found_col = find_column_fuzzy(df_bench, [alias])
            if found_col: break
        if found_col:
            try:
                s = df_bench[found_col].apply(clean_numeric_strict)
                v = s[s>0].mean()
                if metric in ['ctr'] and v > 1.0: v = v / 100.0
                if not pd.isna(v): extracted[metric] = [v, higher_better]
            except: pass
    return extracted

def add_hyperlink(paragraph, url, text, color="0000FF", underline=True):
    try:
        part = paragraph.part
        r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)
        hyperlink = OxmlElement('w:hyperlink')
        hyperlink.set(qn('r:id'), r_id)
        new_run = OxmlElement('w:r')
        rPr = OxmlElement('w:rPr')
        if color:
            c = OxmlElement('w:color')
            c.set(qn('w:val'), color)
            rPr.append(c)
        if underline:
            u = OxmlElement('w:u')
            u.set(qn('w:val'), 'single')
            rPr.append(u)
        new_run.append(rPr)
        new_run.text = text
        hyperlink.append(new_run)
        paragraph._p.append(hyperlink)
        return hyperlink
    except: return None

def apply_report_labels(df, custom_mapping=None):
    if df.empty: return df
    mapping = REPORT_MAPPING.copy()
    if custom_mapping: mapping.update(custom_mapping)
    return df.rename(columns=mapping)

def add_df_to_word(doc, df, title, level=1):
    if df.empty: return
    doc.add_heading(title, level=level)
    t = doc.add_table(rows=df.shape[0]+1, cols=df.shape[1])
    t.style = 'Table Grid'
    is_creative = "ç´ æ" in title
    is_landing = "è½åœ°é¡µ" in title
    link_col_idx = -1
    for j, col in enumerate(df.columns):
        cell = t.cell(0, j)
        cell.text = str(col)
        if any(x in str(col).lower() for x in ["url", "link", "ç´ æ", "å†…å®¹", "content"]): link_col_idx = j
        for p in cell.paragraphs:
            for r in p.runs:
                r.font.bold = True
                r.font.size = Pt(8)
    for i in range(df.shape[0]):
        label_prefix = "ç´ æ" if is_creative else ("è½åœ°é¡µ" if is_landing else "")
        label_char = chr(65 + (i % 26))
        if i >= 26: label_char += str(i // 26)
        label_text = f"{label_prefix}{label_char}"
        for j in range(df.shape[1]):
            val = df.iat[i, j]
            cell = t.cell(i+1, j)
            if (is_creative or is_landing) and j == link_col_idx:
                try:
                    p = cell.paragraphs[0]
                    url = str(val).strip()
                    if len(url) > 5: add_hyperlink(p, url, label_text)
                    else: cell.text = label_text
                except: cell.text = label_text
            else:
                cell.text = str(val)
                if "ç»“è®º" in str(df.columns[j]):
                    if "âœ…" in str(val): cell.paragraphs[0].runs[0].font.color.rgb = RGBColor(0, 128, 0)
                    if "âš ï¸" in str(val): cell.paragraphs[0].runs[0].font.color.rgb = RGBColor(255, 0, 0)
            for p in cell.paragraphs:
                for r in p.runs: r.font.size = Pt(8)
    doc.add_paragraph("\n")

# ==========================================
# PART 3: ä¸»é€»è¾‘ç±»
# ==========================================

class AdReportProcessor:
    def __init__(self, raw_file, bench_file=None):
        self.raw_file = raw_file
        self.bench_file = bench_file
        self.processed_dfs = {}
        self.merged_dfs = {}
        self.final_json = {}
        self.doc = Document()

    def find_sheet_fuzzy(self, target, actual_sheets):
        for actual in actual_sheets:
            if target.strip().lower() == actual.strip().lower():
                return actual
        for actual in actual_sheets:
            if target in actual:
                return actual
        return None

    def process_etl(self):
        xls = pd.ExcelFile(self.raw_file)
        
        for config_sheet_name, mapping in SHEET_MAPPINGS.items():
            actual_sheet_name = self.find_sheet_fuzzy(config_sheet_name, xls.sheet_names)
            
            if actual_sheet_name:
                df = pd.read_excel(xls, sheet_name=actual_sheet_name)
                # å½’ä¸€åŒ–åˆ—åï¼Œæ–¹ä¾¿åŒ¹é…
                df.columns = [str(c).strip() for c in df.columns]
                
                final_cols = {}
                for std_col, raw_col_options in mapping.items():
                    matched_col = None
                    # 1. ç²¾ç¡®/Case-Insensitive åŒ¹é…
                    for option in raw_col_options:
                        # æŸ¥æ‰¾åŸå§‹åˆ—ä¸­æ˜¯å¦å­˜åœ¨è¯¥åˆ«å (å¿½ç•¥å¤§å°å†™)
                        for raw_col in df.columns:
                            if option.lower() == raw_col.lower():
                                matched_col = raw_col
                                break
                        if matched_col: break
                        
                        # å¦‚æœè¿˜æ²¡æ‰¾åˆ°ï¼Œå°è¯•å»ç©ºæ ¼åŒ¹é…
                        if not matched_col:
                            for raw_col in df.columns:
                                if option.lower().replace(" ", "") == raw_col.lower().replace(" ", ""):
                                    matched_col = raw_col
                                    break
                        if matched_col: break
                    
                    if matched_col: 
                        final_cols[std_col] = matched_col
                
                # åˆ›å»ºæ¸…æ´—åçš„ DataFrame
                if final_cols:
                    df_clean = df[list(final_cols.values())].rename(columns={v: k for k, v in final_cols.items()})
                else:
                    df_clean = pd.DataFrame() # å¦‚æœå®Œå…¨æ²¡åŒ¹é…åˆ°
                
                # âœ… æ ¸å¿ƒä¿®æ­£ï¼šå¼ºåˆ¶è¡¥å…¨ç¼ºå¤±çš„æ ‡å‡†åˆ—ï¼Œç¡®ä¿åç»­é€»è¾‘èƒ½æ‰¾åˆ° add_to_cart
                for expected_col in mapping.keys():
                    if expected_col not in df_clean.columns:
                        # å¦‚æœæºæ–‡ä»¶ä¸­æ²¡æ‰¾åˆ°è¿™åˆ—ï¼Œå°±åˆ›å»ºå®ƒå¹¶å¡«0
                        df_clean[expected_col] = 0.0

                # æ•°å€¼æ¸…æ´—
                text_cols = ['date_range', 'anomaly_metric_name', 
                             'converting_keywords', 'converting_countries', 'converting_genders', 'converting_ages', 
                             'custom_audience_settings', 'dimension_item', 'content_item']
                
                for col in df_clean.columns:
                    if col not in text_cols:
                        df_clean[col] = df_clean[col].apply(clean_numeric)

                if config_sheet_name in ["ç´ æ", "è½åœ°é¡µ", "å—ä¼—ç»„"]:
                    if "spend" in df_clean.columns:
                        df_clean = df_clean.sort_values("spend", ascending=False).head(10)

                df_clean["Source_Sheet"] = config_sheet_name
                self.processed_dfs[config_sheet_name] = df_clean

        for master_name, source_sheets in GROUP_CONFIG.items():
            dfs_to_merge = [self.processed_dfs[src] for src in source_sheets if src in self.processed_dfs]
            if dfs_to_merge:
                merged_df = pd.concat(dfs_to_merge, ignore_index=True)
                cols = list(merged_df.columns)
                priority_cols = ['Source_Sheet', 'date_range', 'dimension_item', 'content_item',
                                 'spend', 'roas', 'purchases', 'cpa']
                new_order = [c for c in priority_cols if c in cols] + [c for c in cols if c not in priority_cols]
                self.merged_dfs[master_name] = merged_df[new_order]

    def generate_report(self):
        benchmark_targets = {'roas': [2.0, True], 'cpm': [20.0, False], 'ctr': [0.015, True], 'cpc': [1.5, False], 'cpa': [30.0, False]}
        if self.bench_file:
            try:
                df_b = pd.read_excel(self.bench_file)
                benchmark_targets = extract_benchmark_values(df_b)
            except: pass

        self.doc.add_heading('å¹¿å‘ŠæŠ•æ”¾æ·±åº¦åˆ†ææŠ¥å‘Š', 0).alignment = WD_ALIGN_PARAGRAPH.CENTER
        self.final_json = {"report_title": "å¹¿å‘ŠæŠ•æ”¾æ·±åº¦åˆ†ææŠ¥å‘Š", "generated_at": pd.Timestamp.now().strftime("%Y-%m-%d")}

        # 1. å¤§ç›˜æ€»è§ˆ
        df_ov = pd.DataFrame()
        if "Master_Overview" in self.merged_dfs:
            df_src = self.merged_dfs["Master_Overview"]
            mask = df_src['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["åˆ†æ—¶", "Time"]))
            df_ov = df_src[mask].copy() if not df_src[mask].empty else df_src.copy()

        if not df_ov.empty:
            date_col = find_column_fuzzy(df_ov, ['date', 'time', 'æ—¶é—´'])
            if date_col:
                try:
                    df_ov['temp_date'] = pd.to_datetime(df_ov[date_col], errors='coerce')
                    df_clean = df_ov.dropna(subset=['temp_date']).sort_values('temp_date')
                    dates = df_clean['temp_date'].unique()
                    
                    # 1.1 åŸºäºåˆ†æ—¶æ•°æ®çš„åŸºç¡€è®¡ç®—
                    raw_overall = calc_metrics_dict(df_clean)
                    
                    # ======================================================
                    # âœ… [æ ¸å¿ƒé€»è¾‘ä¿®æ­£] è¦†ç›–æ•°æ®é€»è¾‘å¢å¼º
                    # ======================================================
                    if "Master_Overview" in self.merged_dfs:
                         df_all = self.merged_dfs["Master_Overview"]
                         mask_summary = df_all['Source_Sheet'] == "æ•´ä½“æ•°æ®"
                         df_summary = df_all[mask_summary]
                         
                         if not df_summary.empty:
                             summary_row = df_summary.iloc[0]
                             override_metrics = ['add_to_cart', 'initiate_checkout', 'purchases', 'landing_page_views', 'impressions', 'clicks']
                             
                             for m in override_metrics:
                                 # åªè¦åˆ—å­˜åœ¨ï¼Œå°±å°è¯•è¯»å–
                                 if m in summary_row:
                                     val = clean_numeric_strict(summary_row[m])
                                     # åªæœ‰å€¼å¤§äº0æ‰è¦†ç›–ï¼Œé˜²æ­¢åæ•°æ®
                                     if val > 0:
                                         raw_overall[m] = val
                             
                             # ğŸš¨ é‡æ–°è®¡ç®—è½¬åŒ–ç‡ (å› ä¸ºåˆ†å­åˆ†æ¯å˜äº†)
                             raw_overall['rate_click_to_lp'] = safe_div(raw_overall.get('landing_page_views'), raw_overall.get('clicks'))
                             raw_overall['rate_lp_to_atc']   = safe_div(raw_overall.get('add_to_cart'), raw_overall.get('landing_page_views'))
                             raw_overall['rate_atc_to_ic']   = safe_div(raw_overall.get('initiate_checkout'), raw_overall.get('add_to_cart'))
                             raw_overall['rate_ic_to_pur']   = safe_div(raw_overall.get('purchases'), raw_overall.get('initiate_checkout'))
                             raw_overall['cvr_purchase'] = safe_div(raw_overall.get('purchases'), raw_overall.get('clicks'))
                    # ======================================================

                    if len(dates) >= 2:
                        mid_date = dates[len(dates)//2]
                        raw_prev = calc_metrics_dict(df_clean[df_clean['temp_date'] < mid_date])
                        raw_curr = calc_metrics_dict(df_clean[df_clean['temp_date'] >= mid_date])
                        raw_mom = {}
                        for k, v_curr in raw_curr.items():
                            if k == 'date_range': raw_mom[k] = "-"
                            else:
                                v_prev = raw_prev.get(k, 0)
                                raw_mom[k] = (v_curr - v_prev) / v_prev if v_prev > 0 else 0.0
                    else:
                        raw_prev = {k: "-" for k in raw_overall}; raw_curr = raw_overall; raw_mom = {k: "-" for k in raw_overall}

                    col_order = ["date_range", "spend", "roas", "cpa", "cpm", "cpc", "ctr", "cvr_purchase",
                                 "rate_click_to_lp", "rate_lp_to_atc", "rate_ic_to_pur", "aov", "add_to_cart", "purchases", "purchase_value"]
                    final_data = []
                    for label, r in zip(["æ•´ä½“æ•°æ®", "ä¸Šå‘¨æœŸå€¼", "æœ¬å‘¨æœŸ", "ç¯æ¯”"], [raw_overall, raw_prev, raw_curr, raw_mom]):
                        row = {"Label": label}
                        is_m = (label == "ç¯æ¯”")
                        for c in col_order: row[c] = format_cell(c, r.get(c, 0), is_mom=is_m)
                        row['date_range'] = label
                        final_data.append(row)

                    df_f = pd.DataFrame(final_data, columns=col_order)
                    df_f_display = apply_report_labels(df_f)
                    add_df_to_word(self.doc, df_f_display, "1. æ•°æ®å¤§ç›˜æ€»è§ˆ", level=1)
                    self.final_json['1_data_overview'] = df_f_display.to_dict(orient='records')

                    # 2. Benchmark
                    raw_current = raw_overall.copy()
                    bench_data = []
                    for metric_key in ['roas', 'cpm', 'ctr', 'cpc', 'cpa']:
                        curr_val = raw_current.get(metric_key, 0)
                        bench_val, higher_is_better = benchmark_targets.get(metric_key, [0, True])
                        conclusion = "-"
                        if curr_val != 0:
                            diff = curr_val - bench_val
                            if higher_is_better: conclusion = "âœ… ä¼˜äºå¤§ç›˜" if diff > 0 else ("âš ï¸ ä½äºå¤§ç›˜" if diff < 0 else "æŒå¹³")
                            else: conclusion = "âœ… ä¼˜äºå¤§ç›˜" if diff < 0 else ("âš ï¸ é«˜äºå¤§ç›˜" if diff > 0 else "æŒå¹³")
                        bench_data.append({
                            "æŒ‡æ ‡": REPORT_MAPPING.get(metric_key, metric_key.upper()),
                            "å½“å‰è´¦æˆ·": format_cell(metric_key, curr_val),
                            "è¡Œä¸šåŸºå‡†": format_cell(metric_key, bench_val),
                            "å¯¹æ¯”ç»“è®º": conclusion
                        })
                    df_b = pd.DataFrame(bench_data)
                    add_df_to_word(self.doc, df_b, "2. è¡Œä¸š Benchmark å¯¹æ¯”", level=1)
                    self.final_json['2_industry_benchmark'] = df_b.to_dict(orient='records')
                except Exception as e: st.warning(f"å¤§ç›˜è®¡ç®—è­¦å‘Š: {e}")

        # 3. å—ä¼—ç»„
        self.generate_audience_section()
        # 4. ç´ æä¸è½åœ°é¡µ
        self.generate_creative_section()
        # 5. ç‰ˆä½
        self.generate_placement_section()
        # 7. æ¶æ„è¯Šæ–­
        self.generate_structure_section()

    def generate_audience_section(self):
        self.doc.add_heading("3. å—ä¼—ç»„åˆ†æ", level=1)
        self.final_json['3_audience_analysis'] = {}
        audience_configs = [
            ("3.1 å›½å®¶åˆ†æ", ["å›½å®¶", "Country"], True, "å›½å®¶"),
            ("3.2 æ€§åˆ«åˆ†æ", ["æ€§åˆ«", "Gender"], False, "æ€§åˆ«"),
            ("3.3 å¹´é¾„åˆ†æ", ["å¹´é¾„", "Age"], False, "å¹´é¾„æ®µ"),
            ("3.4 å—ä¼—ç»„åˆ†æè¡¨", ["å—ä¼—", "Audience"], True, "å—ä¼—ç»„åç§°"),
        ]
        if "Master_Breakdown" in self.merged_dfs:
            df_bd = self.merged_dfs["Master_Breakdown"]
            for title, keywords, top10, dim_label in audience_configs:
                mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in keywords))
                df_curr = df_bd[mask].copy()
                if not df_curr.empty:
                    self.process_sub_table(df_curr, title, top10, dim_label, '3_audience_analysis')

    def generate_creative_section(self):
        if "Master_Creative" in self.merged_dfs:
            df_cr = self.merged_dfs["Master_Creative"]
            for title, keywords, label, json_key in [("4. ç´ æåˆ†æ", ["ç´ æ", "Creative"], "ç´ æåç§°", "4_creative_analysis"), ("6. è½åœ°é¡µåˆ†æ", ["è½åœ°é¡µ", "Landing"], "è½åœ°é¡µ URL", "6_landing_page_analysis")]:
                mask = df_cr['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in keywords))
                df_curr = df_cr[mask].copy()
                if not df_curr.empty:
                      # ç®€å•çš„CPC/CTRè¡¥å…¨é€»è¾‘ï¼ŒåŒåŸä»£ç 
                      if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan) if 'clicks' in df_curr else 0
                      if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan) if 'purchases' in df_curr else 0
                      if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = (df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan)) * 100 if 'impressions' in df_curr else 0
                      else: df_curr['ctr'] = df_curr['ctr'] * 100
                      
                      req_cols = ["content_item", "spend", "ctr", "cpc", "cpm", "roas", "cpa"]
                      df_final = self.standardize_cols(df_curr, req_cols)
                      if 'spend' in df_final.columns: df_final = df_final.sort_values('spend', ascending=False).head(10)
                      df_display = apply_report_labels(df_final.round(2), custom_mapping={'content_item': label})
                      add_df_to_word(self.doc, df_display, title, level=1)
                      self.final_json[json_key] = df_display.to_dict(orient='records')

    def generate_placement_section(self):
         if "Master_Breakdown" in self.merged_dfs:
             self.doc.add_heading("5. ç‰ˆä½åˆ†æ", level=1)
             df_bd = self.merged_dfs["Master_Breakdown"]
             mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["ç‰ˆä½", "Placement"]))
             df_curr = df_bd[mask].copy()
             if not df_curr.empty:
                  req_cols = ['dimension_item', 'spend', 'ctr', 'cpc', 'cpm', 'roas', 'cpa']
                  # ç®€å•è¡¥å…¨è®¡ç®—
                  if 'clicks' in df_curr and 'impressions' in df_curr: df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0,np.nan)
                  
                  df_clean = self.standardize_cols(df_curr, req_cols).round(2)
                  df_top5 = df_clean.sort_values('spend', ascending=False).head(5)
                  add_df_to_word(self.doc, apply_report_labels(df_top5, {'dimension_item': 'ç‰ˆä½'}), "5.1 ç‰ˆä½èŠ±è´¹ TOP 5", level=2)
                  self.final_json['5_placement_analysis'] = {"top_spend": df_top5.to_dict('records')}

    def generate_structure_section(self):
        rows = []
        if "Master_Overview" in self.merged_dfs:
             metrics = calc_metrics_dict(self.merged_dfs["Master_Overview"])
             rows.append({"æ¨¡å—": "é¢„ç®—ç»“æ„", "å½“å‰ç»“æ„æ•°æ®è¡¨ç°": f"æ€»èŠ±è´¹: ${metrics.get('spend',0):,.2f}", "å­˜åœ¨çš„é—®é¢˜": ""})
        df_struct = pd.DataFrame(rows)
        add_df_to_word(self.doc, df_struct, "7. å¹¿å‘Šæ¶æ„åˆ†æ", level=1)
        self.final_json['7_structure_analysis'] = df_struct.to_dict(orient='records')

    def standardize_cols(self, df, req_cols):
        rename_map = {}; valid_cols = []
        for req in req_cols:
            aliases = FIELD_ALIASES.get(req, [req])
            found = find_column_fuzzy(df, aliases)
            if found: valid_cols.append(found); rename_map[found] = req
            else: df[req] = 0.0; valid_cols.append(req)
        return df[valid_cols].rename(columns=rename_map)

    def process_sub_table(self, df, title, top10, dim_label, json_section):
        req_cols = ["dimension_item", "spend", "ctr", "cpc", "cpm", "cpa", "roas"]
        if "å—ä¼—" in title: req_cols += ["converting_countries", "converting_keywords"]
        df_final = self.standardize_cols(df, req_cols)
        if top10 and 'spend' in df_final.columns: df_final = df_final.sort_values('spend', ascending=False).head(10)
        df_display = apply_report_labels(df_final.round(2), custom_mapping={'dimension_item': dim_label})
        add_df_to_word(self.doc, df_display, title, level=2)
        if json_section not in self.final_json: self.final_json[json_section] = {}
        self.final_json[json_section][title] = df_display.to_dict(orient='records')

# ==========================================
# PART 4: Streamlit UI
# ==========================================
def main():
    st.set_page_config(page_title="Auto-ad-data", layout="wide")
    st.title("å¹¿å‘Šæ•°æ®è‡ªåŠ¨åŒ–æ¸…æ´—ç³»ç»Ÿ")
    
    raw_file = st.file_uploader("1.ä¸Šä¼ ã€å‘¨æœŸæ€§å¤ç›˜æŠ¥å‘Šã€‘", type=["xlsx", "xls"])
    bench_file = st.file_uploader("2.ä¸Šä¼ ã€è¡Œä¸š Benchmarkã€‘", type=["xlsx", "xls"])
    
    if st.button("å¼€å§‹ç”Ÿæˆæ•°æ®è¡¨") and raw_file:
        processor = AdReportProcessor(raw_file, bench_file)
        try:
            with st.spinner("æ•°æ®å¤„ç†ä¸­..."):
                processor.process_etl()
                processor.generate_report()
            st.success("å¤„ç†å®Œæˆï¼")
            
            # ä¸‹è½½æŒ‰é’®é€»è¾‘
            json_str = json.dumps(processor.final_json, indent=4, ensure_ascii=False)
            st.download_button("ğŸ“¥ ä¸‹è½½ JSON", json_str, "report.json", "application/json")
            
            output_doc = io.BytesIO()
            processor.doc.save(output_doc)
            st.download_button("ğŸ“¥ ä¸‹è½½ Word", output_doc.getvalue(), "report.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document")
            
        except Exception as e:
            st.error(f"å‘ç”Ÿé”™è¯¯: {str(e)}")
            st.exception(e)

if __name__ == "__main__":
    main()
