import streamlit as st
import pandas as pd
import numpy as np
import io
import json
import xlsxwriter
from docx import Document
from docx.shared import Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.shared import OxmlElement
from docx.oxml.ns import qn
import docx.opc.constants

# ==========================================
# PART 1: é…ç½®åŒºåŸŸ
# ==========================================

# å­—æ®µå±•ç¤ºåç§°æ˜ å°„ (ä¸­æ–‡ Key)
REPORT_MAPPING = {
    "spend": "èŠ±è´¹ ($)", "roas": "ROAS", "purchases": "è´­ä¹°æ¬¡æ•°", "purchase_value": "è´­ä¹°æ€»ä»·å€¼",
    "cpa": "CPA ($)", "ctr": "CTR (%)", "cpm": "CPM ($)", "cpc": "CPC ($)", "aov": "å®¢å•ä»·",
    "impressions": "å±•ç°é‡", "clicks_all": "ç‚¹å‡»é‡ (All)", "clicks": "ç‚¹å‡»é‡ (All)", "ctr_all": "ç‚¹å‡»ç‡ (All)",
    "landing_page_views": "è½åœ°é¡µè®¿é—®é‡", "add_to_cart": "åŠ è´­æ¬¡æ•°", "initiate_checkout": "ç»“è´¦å‘èµ·æ•° (IC)",
    "rate_click_to_lp": "ç‚¹å‡» â†’ è½åœ°é¡µè®¿é—®è½¬åŒ–ç‡", "rate_lp_to_atc": "è½åœ°é¡µ â†’ åŠ è´­è½¬åŒ–ç‡",
    "rate_atc_to_ic": "åŠ è´­ â†’ è´­ä¹°è½¬åŒ–ç‡", "rate_ic_to_pur": "è´­ä¹°è½¬åŒ–ç‡",
    "cvr_purchase": "ç‚¹å‡» â†’ è´­ä¹°è½¬åŒ–ç‡", "cvr_lp_to_pur": "CVR (å…¨ç«™è½¬åŒ–ç‡)",
    "date_range": "æ—¥æœŸ/æ—¶æ®µ", "campaign_type": "æŠ•æ”¾æ¨¡å¼", "adset_name": "å¹¿å‘Šç»„ID", "adset_id": "å¹¿å‘Šç»„ID",
    "custom_audience_settings": "è‡ªå®šä¹‰å—ä¼—æº", "converting_keywords": "é«˜æ½œå…´è¶£è¯", "audience_type": "å—ä¼—ç­–ç•¥",
    "country": "å›½å®¶", "age_group": "å¹´é¾„", "gender": "æ€§åˆ«", "creative_name": "ç´ æåç§°", "placement": "ç‰ˆä½",
    "landing_page_url": "é¡µé¢ URL", "mom_change": "ç¯æ¯”æ³¢åŠ¨", "anomaly_metric_name": "å¼‚å¸¸é¡¹",
    "converting_countries": "äº§ç”Ÿæˆæ•ˆçš„å›½å®¶", "converting_genders": "äº§ç”Ÿæˆæ•ˆçš„æ€§åˆ«", "converting_ages": "äº§ç”Ÿæˆæ•ˆçš„å¹´é¾„",
    "dimension_item": "ç»´åº¦åç§°", "content_item": "å†…å®¹åç§°"
}

COMMON_METRICS = {
    "spend": ["èŠ±è´¹é‡‘é¢(USD)", "èŠ±è´¹é‡‘é¢ ï¼ˆUSDï¼‰", "èŠ±è´¹é‡‘é¢ (USD)", "èŠ±è´¹é‡‘é¢", "Amount Spent"],
    "roas": ["å¹¿å‘ŠèŠ±è´¹å›æŠ¥ (ROAS) - è´­ç‰©", "å¹¿å‘ŠèŠ±è´¹å›æŠ¥ï¼ˆROASï¼‰-è´­ç‰©", "ROAS", "Purchase ROAS"],
    "purchases": ["è´­ä¹°æ¬¡æ•°", "æˆæ•ˆæ•°é‡", "æˆæ•ˆ", "Purchases", "Results"],
    "cpa": ["å•æ¬¡è´­ä¹°è´¹ç”¨", "å•æ¬¡è´­ç‰©æˆæœ¬", "å•æ¬¡æˆæ•ˆæˆæœ¬", "å•æ¬¡æˆæ•ˆè´¹ç”¨", "Cost per Purchase"],
    "ctr": ["é“¾æ¥ç‚¹å‡»ç‡", "é“¾æ¥ç‚¹å‡»ç‡ï¼ˆ%)", "é“¾æ¥ç‚¹å‡»ç‡ï¼ˆ%ï¼‰", "CTR"],
    "cpm": ["åƒæ¬¡å±•ç¤ºè´¹ç”¨", "CPM"],
    "clicks": ["ç‚¹å‡»", "é“¾æ¥ç‚¹å‡»", "Clicks (All)"],
    "impressions": ["æ›å…‰", "å±•ç¤ºæ¬¡æ•°", "Impressions"],
    "purchase_value": ["è´­ä¹°ä»·å€¼", "è´­ç‰©ä»·å€¼", "Purchase Conversion Value"],
    "aov": ["å•æ¬¡è´­ä¹°ä»·å€¼", "å•æ¬¡è´­ç‰©ä»·å€¼"]
}

SHEET_MAPPINGS = {
    "æ•´ä½“æ•°æ®": {
        **COMMON_METRICS,
        "date_range": ["æ—¶é—´èŒƒå›´", "Date", "Day"],
        "clicks_all": ["ç‚¹å‡»", "Clicks"],
        "landing_page_views": ["è½åœ°é¡µæµè§ˆé‡", "Landing Page Views"],
        "add_to_cart": ["åŠ å…¥è´­ç‰©è½¦", "Adds to Cart"],
        "initiate_checkout": ["ç»“è´¦å‘èµ·æ¬¡æ•°", "Initiate Checkout"],
        "rate_click_to_lp": ["ç‚¹å‡»-è½åœ°é¡µæµè§ˆè½¬åŒ–ç‡"],
        "rate_lp_to_atc": ["è½åœ°é¡µæµè§ˆ-åŠ è´­è½¬åŒ–ç‡"],
        "rate_atc_to_ic": ["åŠ è´­-ç»“è´¦è½¬åŒ–ç‡"],
        "rate_ic_to_pur": ["ç»“è´¦-è´­ä¹°è½¬åŒ–ç‡"]
    },
    "åˆ†æ—¶æ®µæ•°æ®": {
        **COMMON_METRICS,
        "date_range": ["æ—¶é—´èŒƒå›´", "Time of Day", "Hourly"],
        "landing_page_views": ["è½åœ°é¡µæµè§ˆé‡"],
        "add_to_cart": ["åŠ å…¥è´­ç‰©è½¦"],
        "initiate_checkout": ["ç»“è´¦å‘èµ·æ¬¡æ•°"],
    },
    "å¼‚å¸¸æŒ‡æ ‡": {
        "anomaly_metric_name": ["å¼‚å¸¸æŒ‡æ ‡"],
        "mom_change": ["ç¯æ¯”"]
    },
    "å¹¿å‘Šæ¶æ„": {**COMMON_METRICS, "dimension_item": ["å¹¿å‘Šç±»å‹", "Campaign Name"]},
    "å—ä¼—ç»„": {
        **COMMON_METRICS,
        "dimension_item": ["å¹¿å‘Šç»„", "å¹¿å‘Šç»„Id", "Ad Set Name"],
        "custom_audience_settings": ["è®¾ç½®çš„è‡ªå®šä¹‰å—ä¼—"],
        "converting_keywords": ["äº§ç”Ÿæˆæ•ˆçš„å…³é”®è¯"]
    },
    "å—ä¼—ç±»å‹": {**COMMON_METRICS, "dimension_item": ["å—ä¼—ç±»å‹"]},
    "å›½å®¶": {**COMMON_METRICS, "dimension_item": ["å›½å®¶/åœ°åŒº", "å›½å®¶", "Country", "Region"]},
    "å¹´é¾„": {**COMMON_METRICS, "dimension_item": ["å¹´é¾„", "Age"]},
    "æ€§åˆ«": {**COMMON_METRICS, "dimension_item": ["æ€§åˆ«", "Gender"]},
    "å¹³å°&ç‰ˆä½": {**COMMON_METRICS, "dimension_item": ["å¹³å°&ç‰ˆä½", "Placement", "Platform"]},
    "ç´ æ": {
        **COMMON_METRICS,
        "content_item": ["ç´ æ", "Ad Name", "Creative Name"],
        "cvr_lp_to_pur": ["è½åœ°é¡µæµè§ˆ-è´­ä¹°è½¬åŒ–ç‡"]
    },
    "è½åœ°é¡µ": {
        **COMMON_METRICS,
        "content_item": ["è½åœ°é¡µurl", "è½åœ°é¡µ", "Website URL"],
        "ctr_all": ["æ›å…‰-ç‚¹å‡»è½¬åŒ–ç‡"],
        "rate_lp_to_atc": ["è½åœ°é¡µæµè§ˆ-åŠ è´­è½¬åŒ–ç‡", "è½åœ°é¡µæµè§ˆ-è´­ç‰©è½¬åŒ–ç‡"]
    }
}

GROUP_CONFIG = {
    "Master_Overview": ["æ•´ä½“æ•°æ®", "åˆ†æ—¶æ®µæ•°æ®", "å¼‚å¸¸æŒ‡æ ‡"],
    "Master_Breakdown": ["å¹¿å‘Šæ¶æ„", "å—ä¼—ç»„", "å—ä¼—ç±»å‹", "å›½å®¶", "å¹´é¾„", "æ€§åˆ«", "å¹³å°&ç‰ˆä½"],
    "Master_Creative": ["ç´ æ", "è½åœ°é¡µ"]
}

FIELD_ALIASES = {
    "adset_id": ["adset_id", "ad set id", "adset id", "å¹¿å‘Šç»„ç¼–å·", "å¹¿å‘Šç»„id", "adset_name", "ad set name"],
    "converting_countries": ["converting_countries", "country", "region", "å›½å®¶", "åœ°åŒº"],
    "converting_genders": ["converting_genders", "gender", "æ€§åˆ«"],
    "converting_ages": ["converting_ages", "age", "å¹´é¾„", "age_group"],
    "converting_keywords": ["converting_keywords", "keywords", "interests", "å…´è¶£", "å…³é”®è¯"],
    "spend": ["spend", "amount spent", "cost", "èŠ±è´¹", "æ¶ˆè€—"],
    "purchases": ["purchases", "results", "result", "æˆæ•ˆ", "è´­ä¹°"],
    "roas": ["roas", "return on ad spend", "purchase roas"],
    "purchase_value": ["purchase_value", "conversion value", "value", "æ€»ä»·å€¼", "gmv", "è´­ä¹°æ€»ä»·å€¼"],
    "clicks": ["clicks", "clicks (all)", "ç‚¹å‡»é‡", "clicks_all"],
    "impressions": ["impressions", "å±•ç¤º", "å±•ç°"],
    "ctr_all": ["ctr_all", "ctr (all)", "ç‚¹å‡»ç‡ (all)"]
}

# ==========================================
# PART 2: æ ¸å¿ƒå·¥å…·å‡½æ•°
# ==========================================

def clean_numeric(val):
    if pd.isna(val): return 0.0
    if isinstance(val, (int, float)): return float(val)
    val_str = str(val).strip().replace('$', '').replace('Â¥', '').replace(',', '')
    if '%' in val_str: 
        try: return float(val_str.replace('%', '')) / 100.0
        except: return 0.0
    try: return float(val_str)
    except: return 0.0

def clean_numeric_strict(val):
    if pd.isna(val): return 0.0
    val_str = str(val).strip().replace('$', '').replace('Â¥', '').replace(',', '')
    if '%' in val_str: val_str = val_str.replace('%', '')
    try: return float(val_str)
    except: return 0.0

def find_column_fuzzy(df, keywords):
    for kw in keywords:
        if kw in df.columns: return kw
    df_cols_norm = {c.lower().replace(' ', '').replace('_', ''): c for c in df.columns}
    for kw in keywords:
        kw_norm = kw.lower().replace(' ', '').replace('_', '')
        if kw_norm in df_cols_norm: return df_cols_norm[kw_norm]
    for col in df.columns:
        col_lower = col.lower()
        for kw in keywords:
            if kw.lower() in col_lower: return col
    return None

def calc_metrics_dict(df_chunk):
    res = {}
    if df_chunk.empty: return res
    sums = {}
    targets = ['spend', 'clicks', 'impressions', 'purchases', 'purchase_value',
               'landing_page_views', 'add_to_cart', 'initiate_checkout']
    for t in targets:
        aliases = FIELD_ALIASES.get(t, [t])
        if t == 'purchase_value' and 'value' not in aliases: aliases.append('value')
        col = find_column_fuzzy(df_chunk, aliases)
        if col:
             sums[t] = df_chunk[col].apply(clean_numeric_strict).sum()
        else:
             sums[t] = 0.0

    eps = 1e-9
    res['spend'] = sums['spend']
    res['roas'] = sums['purchase_value'] / (sums['spend'] + eps)
    res['cpm'] = (sums['spend'] / (sums['impressions'] + eps)) * 1000
    res['cpc'] = sums['spend'] / (sums['clicks'] + eps)
    res['ctr'] = sums['clicks'] / (sums['impressions'] + eps)
    res['cpa'] = sums['spend'] / (sums['purchases'] + eps)
    res['cvr_purchase'] = sums['purchases'] / (sums['clicks'] + eps)
    res['rate_click_to_lp'] = sums['landing_page_views'] / (sums['clicks'] + eps)
    res['rate_lp_to_atc'] = sums['add_to_cart'] / (sums['landing_page_views'] + eps)
    res['rate_atc_to_ic'] = sums['initiate_checkout'] / (sums['add_to_cart'] + eps)
    res['rate_ic_to_pur'] = sums['purchases'] / (sums['initiate_checkout'] + eps)
    res['aov'] = sums['purchase_value'] / (sums['purchases'] + eps)

    date_col = find_column_fuzzy(df_chunk, ['date', 'time', 'range'])
    if date_col:
        try:
            dates = pd.to_datetime(df_chunk[date_col], errors='coerce').dropna()
            if not dates.empty: res['date_range'] = f"{dates.min():%Y-%m-%d} ~ {dates.max():%Y-%m-%d}"
            else: res['date_range'] = "-"
        except: res['date_range'] = "-"
    else: res['date_range'] = "-"
    return res

def format_cell(key, val, is_mom=False):
    if isinstance(val, str): return val
    if is_mom:
        if key == 'date_range': return val
        return f"{val:+.2%}"
    k = str(key).lower()
    if 'roas' in k: return f"{val:.2f}"
    if any(x in k for x in ['rate', 'ctr', 'cvr', 'ç‚¹å‡»ç‡', 'è½¬åŒ–ç‡']): return f"{val:.2%}"
    if any(x in k for x in ['spend', 'cpm', 'cpc', 'value', 'aov', 'cpa', 'èŠ±è´¹', 'é‡‘é¢', 'å®¢å•ä»·', 'gmv']): return f"{val:,.2f}"
    if any(x in k for x in ['purchases', 'cart', 'click', 'æ¬¡æ•°', 'å•é‡', 'ç‚¹å‡»', 'å±•ç°', 'è®¿é—®é‡', 'å‘èµ·æ•°']): return f"{val:,.0f}"
    return f"{val}"

def extract_benchmark_values(df_bench):
    targets = {'roas': (['roas'], True), 'cpm': (['cpm'], False), 'ctr': (['ctr'], True), 'cpc': (['cpc'], False), 'cpa': (['cpa_purchase', 'cpa'], False)}
    extracted = {}
    for metric, (aliases, higher_better) in targets.items():
        found_col = None
        for alias in aliases:
            found_col = find_column_fuzzy(df_bench, [alias])
            if found_col: break
        if found_col:
            try:
                s = df_bench[found_col].apply(clean_numeric_strict)
                v = s[s>0].mean()
                if not pd.isna(v): extracted[metric] = [v, higher_better]
            except: pass
    return extracted

def remap_json_keys(obj, mapping):
    """é€’å½’å°† JSON å¯¹è±¡ä¸­çš„è‹±æ–‡ Key æ›¿æ¢ä¸ºä¸­æ–‡å±•ç¤ºå"""
    if isinstance(obj, dict):
        new_dict = {}
        for k, v in obj.items():
            new_key = mapping.get(k, k)
            new_dict[new_key] = remap_json_keys(v, mapping)
        return new_dict
    elif isinstance(obj, list):
        return [remap_json_keys(i, mapping) for i in obj]
    else:
        return obj

def add_hyperlink(paragraph, url, text, color="0000FF", underline=True):
    try:
        part = paragraph.part
        r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)
        hyperlink = OxmlElement('w:hyperlink')
        hyperlink.set(qn('r:id'), r_id)
        new_run = OxmlElement('w:r')
        rPr = OxmlElement('w:rPr')
        if color:
            c = OxmlElement('w:color')
            c.set(qn('w:val'), color)
            rPr.append(c)
        if underline:
            u = OxmlElement('w:u')
            u.set(qn('w:val'), 'single')
            rPr.append(u)
        new_run.append(rPr)
        new_run.text = text
        hyperlink.append(new_run)
        paragraph._p.append(hyperlink)
        return hyperlink
    except: return None

def apply_report_labels(df, custom_mapping=None):
    if df.empty: return df
    mapping = REPORT_MAPPING.copy()
    if custom_mapping: mapping.update(custom_mapping)
    return df.rename(columns=mapping)

def add_df_to_word(doc, df, title, level=1):
    if df.empty: return
    doc.add_heading(title, level=level)
    t = doc.add_table(rows=df.shape[0]+1, cols=df.shape[1])
    t.style = 'Table Grid'
    is_creative = "ç´ æ" in title
    is_landing = "è½åœ°é¡µ" in title
    link_col_idx = -1
    for j, col in enumerate(df.columns):
        cell = t.cell(0, j)
        cell.text = str(col)
        if any(x in str(col).lower() for x in ["url", "link", "ç´ æ", "å†…å®¹", "content"]):
            link_col_idx = j
        for p in cell.paragraphs:
            for r in p.runs:
                r.font.bold = True
                r.font.size = Pt(8)
    for i in range(df.shape[0]):
        label_prefix = "ç´ æ" if is_creative else ("è½åœ°é¡µ" if is_landing else "")
        label_char = chr(65 + (i % 26))
        if i >= 26: label_char += str(i // 26)
        label_text = f"{label_prefix}{label_char}"
        for j in range(df.shape[1]):
            val = df.iat[i, j]
            cell = t.cell(i+1, j)
            if (is_creative or is_landing) and j == link_col_idx:
                try:
                    p = cell.paragraphs[0]
                    url = str(val).strip()
                    if len(url) > 5: add_hyperlink(p, url, label_text)
                    else: cell.text = label_text
                except: cell.text = label_text
            else:
                cell.text = str(val)
                if "ç»“è®º" in str(df.columns[j]):
                    if "âœ…" in str(val): cell.paragraphs[0].runs[0].font.color.rgb = RGBColor(0, 128, 0)
                    if "âš ï¸" in str(val): cell.paragraphs[0].runs[0].font.color.rgb = RGBColor(255, 0, 0)
            for p in cell.paragraphs:
                for r in p.runs: r.font.size = Pt(8)
    doc.add_paragraph("\n")

# ==========================================
# PART 3: ä¸»é€»è¾‘ç±»
# ==========================================

class AdReportProcessor:
    def __init__(self, raw_file, bench_file=None):
        self.raw_file = raw_file
        self.bench_file = bench_file
        self.processed_dfs = {}
        self.merged_dfs = {}
        self.final_json = {}
        self.doc = Document()

    def process_etl(self):
        xls = pd.ExcelFile(self.raw_file)
        for sheet_name, mapping in SHEET_MAPPINGS.items():
            if sheet_name in xls.sheet_names:
                df = pd.read_excel(xls, sheet_name=sheet_name)
                final_cols = {}
                for std_col, raw_col_options in mapping.items():
                    matched_col = None
                    for option in raw_col_options:
                        if option in df.columns: matched_col = option; break
                        if not matched_col:
                            for df_col in df.columns:
                                if option.replace(" ", "") == df_col.replace(" ", ""): matched_col = df_col; break
                        if matched_col: break
                    if matched_col: final_cols[std_col] = matched_col

                if final_cols:
                    df_clean = df[list(final_cols.values())].rename(columns={v: k for k, v in final_cols.items()})
                    text_cols = ['date_range', 'anomaly_metric_name', 'converting_keywords',
                                 'custom_audience_settings', 'dimension_item', 'content_item']
                    for col in df_clean.columns:
                        if col not in text_cols: df_clean[col] = df_clean[col].apply(clean_numeric)

                    if sheet_name in ["ç´ æ", "è½åœ°é¡µ","å—ä¼—ç»„"]:
                        if "spend" in df_clean.columns:
                            df_clean = df_clean.sort_values("spend", ascending=False).head(10)

                    df_clean["Source_Sheet"] = sheet_name
                    self.processed_dfs[sheet_name] = df_clean

        for master_name, source_sheets in GROUP_CONFIG.items():
            dfs_to_merge = [self.processed_dfs[src] for src in source_sheets if src in self.processed_dfs]
            if dfs_to_merge:
                merged_df = pd.concat(dfs_to_merge, ignore_index=True)
                cols = list(merged_df.columns)
                priority_cols = ['Source_Sheet', 'date_range', 'dimension_item', 'content_item',
                                 'spend', 'roas', 'purchases', 'cpa']
                new_order = [c for c in priority_cols if c in cols] + [c for c in cols if c not in priority_cols]
                self.merged_dfs[master_name] = merged_df[new_order]

    def generate_report(self):
        benchmark_targets = {'roas': [2.0, True], 'cpm': [20.0, False], 'ctr': [0.015, True], 'cpc': [1.5, False], 'cpa': [30.0, False]}
        if self.bench_file:
            try:
                df_b = pd.read_excel(self.bench_file)
                benchmark_targets = extract_benchmark_values(df_b)
            except: pass

        self.doc.add_heading('å¹¿å‘ŠæŠ•æ”¾æ·±åº¦åˆ†ææŠ¥å‘Š', 0).alignment = WD_ALIGN_PARAGRAPH.CENTER
        self.final_json = {"report_title": "å¹¿å‘ŠæŠ•æ”¾æ·±åº¦åˆ†ææŠ¥å‘Š", "generated_at": pd.Timestamp.now().strftime("%Y-%m-%d")}

        # 1. å¤§ç›˜æ€»è§ˆ
        df_ov = pd.DataFrame()
        if "Master_Overview" in self.merged_dfs:
            df_src = self.merged_dfs["Master_Overview"]
            mask = df_src['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["åˆ†æ—¶", "Time"]))
            df_ov = df_src[mask].copy() if not df_src[mask].empty else df_src.copy()

        if not df_ov.empty:
            date_col = find_column_fuzzy(df_ov, ['date', 'time', 'æ—¶é—´'])
            if date_col:
                try:
                    df_ov['temp_date'] = pd.to_datetime(df_ov[date_col], errors='coerce')
                    df_clean = df_ov.dropna(subset=['temp_date']).sort_values('temp_date')
                    dates = df_clean['temp_date'].unique()
                    raw_overall = calc_metrics_dict(df_clean)

                    if len(dates) >= 2:
                        mid_date = dates[len(dates)//2]
                        raw_prev = calc_metrics_dict(df_clean[df_clean['temp_date'] < mid_date])
                        raw_curr = calc_metrics_dict(df_clean[df_clean['temp_date'] >= mid_date])
                        raw_mom = {}
                        for k, v_curr in raw_curr.items():
                            if k == 'date_range': raw_mom[k] = "-"
                            else:
                                v_prev = raw_prev.get(k, 0)
                                raw_mom[k] = (v_curr - v_prev) / v_prev if v_prev > 0 else 0.0
                    else:
                        raw_prev = {k: "-" for k in raw_overall}; raw_curr = raw_overall; raw_mom = {k: "-" for k in raw_overall}

                    col_order = ["date_range", "spend", "roas", "cpa", "cpm", "cpc", "ctr", "cvr_purchase",
                                 "rate_click_to_lp", "rate_lp_to_atc", "rate_ic_to_pur", "aov", "add_to_cart", "purchases", "purchase_value"]
                    final_data = []
                    for label, r in zip(["æ•´ä½“æ•°æ®", "ä¸Šå‘¨æœŸå€¼", "æœ¬å‘¨æœŸ", "ç¯æ¯”"], [raw_overall, raw_prev, raw_curr, raw_mom]):
                        row = {"Label": label}
                        is_m = (label == "ç¯æ¯”")
                        for c in col_order: row[c] = format_cell(c, r.get(c, 0), is_mom=is_m)
                        row['date_range'] = label
                        final_data.append(row)

                    df_f = pd.DataFrame(final_data, columns=col_order)
                    df_f_display = apply_report_labels(df_f)
                    add_df_to_word(self.doc, df_f_display, "1. æ•°æ®å¤§ç›˜æ€»è§ˆ", level=1)
                    self.final_json['1_data_overview'] = df_f.to_dict(orient='records')

                    # Benchmark
                    raw_current = calc_metrics_dict(df_clean)
                    bench_data = []
                    for metric_key in ['roas', 'cpm', 'ctr', 'cpc', 'cpa']:
                        curr_val = raw_current.get(metric_key, 0)
                        bench_val, higher_is_better = benchmark_targets.get(metric_key, [0, True])
                        conclusion = "-"
                        if curr_val != 0:
                            diff = curr_val - bench_val
                            if higher_is_better: conclusion = "âœ… ä¼˜äºå¤§ç›˜" if diff > 0 else ("âš ï¸ ä½äºå¤§ç›˜" if diff < 0 else "æŒå¹³")
                            else: conclusion = "âœ… ä¼˜äºå¤§ç›˜" if diff < 0 else ("âš ï¸ é«˜äºå¤§ç›˜" if diff > 0 else "æŒå¹³")

                        bench_data.append({
                            "æŒ‡æ ‡": REPORT_MAPPING.get(metric_key, metric_key.upper()),
                            "å½“å‰è´¦æˆ·": format_cell(metric_key, curr_val),
                            "è¡Œä¸šåŸºå‡†": format_cell(metric_key, bench_val),
                            "å¯¹æ¯”ç»“è®º": conclusion
                        })
                    df_b = pd.DataFrame(bench_data)
                    add_df_to_word(self.doc, df_b, "2. è¡Œä¸š Benchmark å¯¹æ¯”", level=1)
                    self.final_json['2_industry_benchmark'] = df_b.to_dict(orient='records')
                except Exception as e: st.warning(f"å¤§ç›˜è®¡ç®—è­¦å‘Š: {e}")

        # 3. å—ä¼—ç»„
        self.doc.add_heading("3. å—ä¼—ç»„åˆ†æ", level=1)
        self.final_json['3_audience_analysis'] = {}
        audience_configs = [
            ("3.1 å›½å®¶åˆ†æ", ["å›½å®¶", "Country"], True, "å›½å®¶"),
            ("3.2 æ€§åˆ«åˆ†æ", ["æ€§åˆ«", "Gender"], False, "æ€§åˆ«"),
            ("3.3 å¹´é¾„åˆ†æ", ["å¹´é¾„", "Age"], False, "å¹´é¾„æ®µ"),
            ("3.4 å—ä¼—ç»„åˆ†æè¡¨", ["å—ä¼—", "Audience"], True, "å—ä¼—ç»„åç§°"),
        ]

        if "Master_Breakdown" in self.merged_dfs:
            df_bd = self.merged_dfs["Master_Breakdown"]
            for title, keywords, top10, dim_label in audience_configs:
                mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in keywords))
                df_curr = df_bd[mask].copy()
                if not df_curr.empty:
                    if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan) if 'clicks' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['cpm']): df_curr['cpm'] = (df_curr['spend'] / df_curr['impressions'].replace(0, np.nan)) * 1000 if 'impressions' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan) if 'impressions' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan) if 'purchases' in df_curr else 0

                    req_cols = ["dimension_item", "spend", "ctr", "cpc", "cpm", "cpa", "roas"]
                    if "å—ä¼—" in title: req_cols += ["converting_countries", "converting_keywords"]
                    rename_map = {}; valid_cols = []
                    for req in req_cols:
                        aliases = FIELD_ALIASES.get(req, [req])
                        found = find_column_fuzzy(df_curr, aliases)
                        if found: valid_cols.append(found); rename_map[found] = req
                        else: df_curr[req] = 0.0 if "converting" not in req else "-"; valid_cols.append(req)

                    df_final = df_curr[valid_cols].rename(columns=rename_map)
                    if "dimension_item" in df_final.columns:
                          df_final = df_final[~df_final['dimension_item'].astype(str).str.lower().str.contains('unknow', na=False)]
                    if top10 and 'spend' in df_final.columns: df_final = df_final.sort_values('spend', ascending=False).head(10)
                    
                    df_clean = df_final.round(2)
                    df_display = apply_report_labels(df_clean, custom_mapping={'dimension_item': dim_label})
                    add_df_to_word(self.doc, df_display, title, level=2)
                    self.final_json['3_audience_analysis'][title] = df_clean.to_dict(orient='records')

        # 4. ç´ æä¸è½åœ°é¡µ
        if "Master_Creative" in self.merged_dfs:
            df_cr = self.merged_dfs["Master_Creative"]
            for title, keywords, label, json_key in [("4. ç´ æåˆ†æ", ["ç´ æ", "Creative"], "ç´ æåç§°", "4_creative_analysis"), ("6. è½åœ°é¡µåˆ†æ", ["è½åœ°é¡µ", "Landing"], "è½åœ°é¡µ URL", "6_landing_page_analysis")]:
                mask = df_cr['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in keywords))
                df_curr = df_cr[mask].copy()
                if not df_curr.empty:
                    if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan) if 'clicks' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan) if 'purchases' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan) if 'impressions' in df_curr else 0

                    req_cols = ["content_item", "spend", "ctr", "cpc", "cpm", "roas", "cpa"]
                    rename_map = {}; valid_cols = []
                    for req in req_cols:
                        aliases = FIELD_ALIASES.get(req, [req])
                        found = find_column_fuzzy(df_curr, aliases)
                        if found: valid_cols.append(found); rename_map[found] = req
                        else: df_curr[req] = 0.0; valid_cols.append(req)

                    df_final = df_curr[valid_cols].rename(columns=rename_map)
                    if 'spend' in df_final.columns: df_final = df_final.sort_values('spend', ascending=False).head(10)
                    df_clean = df_final.round(2)
                    df_display = apply_report_labels(df_clean, custom_mapping={'content_item': label})
                    add_df_to_word(self.doc, df_display, title, level=1)
                    self.final_json[json_key] = df_clean.to_dict(orient='records')

        # 5. ç‰ˆä½
        if "Master_Breakdown" in self.merged_dfs:
             self.doc.add_heading("5. ç‰ˆä½åˆ†æ", level=1)
             df_bd = self.merged_dfs["Master_Breakdown"]
             mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["ç‰ˆä½", "Placement"]))
             df_curr = df_bd[mask].copy()
             if not df_curr.empty:
                 if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan) if 'clicks' in df_curr else 0
                 if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan) if 'purchases' in df_curr else 0
                 if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan) if 'impressions' in df_curr else 0
                 if not find_column_fuzzy(df_curr, ['cpm']): df_curr['cpm'] = (df_curr['spend'] / df_curr['impressions'].replace(0, np.nan)) * 1000 if 'impressions' in df_curr else 0

                 req_cols = ['dimension_item', 'spend', 'ctr', 'cpc', 'cpm', 'roas', 'cpa']
                 rename_map = {}; valid_cols = []
                 for c in req_cols:
                      aliases = FIELD_ALIASES.get(c, [c])
                      f = find_column_fuzzy(df_curr, aliases)
                      if f: valid_cols.append(f); rename_map[f] = c
                      else: df_curr[c] = 0.0; valid_cols.append(c)

                 df_clean = df_curr[valid_cols].rename(columns=rename_map).round(2)
                 df_top5 = df_clean.sort_values('spend', ascending=False).head(5)
                 add_df_to_word(self.doc, apply_report_labels(df_top5, {'dimension_item': 'ç‰ˆä½'}), "5.1 ç‰ˆä½èŠ±è´¹ TOP 5", level=2)

                 mean_ctr = df_clean['ctr'].mean(); mean_cpm = df_clean['cpm'].mean()
                 mask_pot = (df_clean['ctr'] > mean_ctr) & (df_clean['cpm'] < mean_cpm)
                 df_pot = df_clean[mask_pot].sort_values('ctr', ascending=False).head(5)
                 if df_pot.empty: df_pot = df_clean.sort_values('ctr', ascending=False).head(5)
                 add_df_to_word(self.doc, apply_report_labels(df_pot, {'dimension_item': 'ç‰ˆä½'}), "5.2 ç‰ˆä½é«˜æ½œåŠ›", level=2)
                 self.final_json['5_placement_analysis'] = {"top_spend": df_top5.to_dict('records'), "high_potential": df_pot.to_dict('records')}

        # 7. æ¶æ„è¯Šæ–­
        rows = []
        if "Master_Overview" in self.merged_dfs:
             metrics = calc_metrics_dict(self.merged_dfs["Master_Overview"])
             rows.append({"æ¨¡å—": "é¢„ç®—ç»“æ„", "å½“å‰ç»“æ„æ•°æ®è¡¨ç°": f"æ€»èŠ±è´¹: ${metrics.get('spend',0):,.2f}\nCPA: ${metrics.get('cpa',0):.2f}\nROAS: {metrics.get('roas',0):.2f}", "å­˜åœ¨çš„é—®é¢˜": ""})

        if "Master_Breakdown" in self.merged_dfs:
            df_bd = self.merged_dfs["Master_Breakdown"]
            mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["å—ä¼—", "Audience"]))
            df_aud = df_bd[mask]
            s_col = find_column_fuzzy(df_aud, ['spend']); active_count = len(df_aud[df_aud[s_col] > 0]) if s_col else 0
            top_share = "0%"
            if not df_aud.empty and s_col:
                total_s = df_aud[s_col].sum()
                if total_s > 0: top_share = f"{df_aud[s_col].max()/total_s:.1%}"
            rows.append({"æ¨¡å—": "å—ä¼—ç»“æ„", "å½“å‰ç»“æ„æ•°æ®è¡¨ç°": f"æ´»è·ƒå—ä¼—ç»„æ•°: {active_count}\nTop1 èŠ±è´¹å æ¯”: {top_share}", "å­˜åœ¨çš„é—®é¢˜": ""})

        if "Master_Creative" in self.merged_dfs:
             df_cr = self.merged_dfs["Master_Creative"]
             mask = df_cr['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["ç´ æ", "Creative"]))
             df_mat = df_cr[mask]
             s_col = find_column_fuzzy(df_mat, ['spend']); active_count = len(df_mat[df_mat[s_col] > 0]) if s_col else 0
             rows.append({"æ¨¡å—": "ç´ æç»“æ„", "å½“å‰ç»“æ„æ•°æ®è¡¨ç°": f"æ´»è·ƒç´ ææ•°: {active_count}", "å­˜åœ¨çš„é—®é¢˜": ""})

        df_struct = pd.DataFrame(rows)
        add_df_to_word(self.doc, df_struct, "7. å¹¿å‘Šæ¶æ„åˆ†æ", level=1)
        self.final_json['7_structure_audit'] = df_struct.to_dict(orient='records')


# ==========================================
# PART 4: Streamlit UI
# ==========================================
def main():
    st.set_page_config(page_title="Auto-Merge & Analysis V20.10", layout="wide")
    st.title("ğŸ“Š å¹¿å‘Šæ•°æ®æ¸…æ´—ä¸é™ç»´åˆå¹¶ (Auto-Merge V20.10)")
    st.markdown("""
    **åŠŸèƒ½è¯´æ˜ï¼š**
    1. **ETLé˜¶æ®µ**ï¼šè‡ªåŠ¨è¿›è¡Œå­—æ®µæ˜ å°„ã€æ•°å€¼æ¸…æ´—ã€ä»¥åŠç´ æ/è½åœ°é¡µ Top10 æˆªæ–­ã€‚
    2. **æŠ¥å‘Šé˜¶æ®µ**ï¼šç”Ÿæˆæ¶æ„è¯Šæ–­è¡¨ã€Word æŠ¥å‘ŠåŠ Gemini åˆ†æç”¨ JSONã€‚
    """)

    col1, col2 = st.columns(2)
    with col1:
        raw_file = st.file_uploader("1. ä¸Šä¼  [æ•°æ®æŠ¥è¡¨] (Excel)", type=["xlsx", "xls"])
    with col2:
        bench_file = st.file_uploader("2. ä¸Šä¼  [è¡Œä¸šBenchmark] (Excel, å¯é€‰)", type=["xlsx", "xls"])

    if st.button("ğŸš€ å¼€å§‹æ‰§è¡Œå…¨æµç¨‹"):
        if not raw_file:
            st.error("è¯·è‡³å°‘ä¸Šä¼ æ•°æ®æŠ¥è¡¨ï¼")
            return

        processor = AdReportProcessor(raw_file, bench_file)

        try:
            with st.spinner("é˜¶æ®µ 1/2: æ•°æ®æ¸…æ´—ã€Top10æˆªæ–­ã€é™ç»´åˆå¹¶..."):
                processor.process_etl()
                st.success("âœ… é˜¶æ®µ 1 å®Œæˆï¼šMaster Tables å·²ç”Ÿæˆ")
                
                with st.expander("æŸ¥çœ‹é™ç»´åˆå¹¶åçš„æ•°æ® (Master Tables)"):
                    tabs = st.tabs(processor.merged_dfs.keys())
                    for i, (k, v) in enumerate(processor.merged_dfs.items()):
                        with tabs[i]: st.dataframe(v.head(20))

            with st.spinner("é˜¶æ®µ 2/2: ç”Ÿæˆæ¶æ„è¯Šæ–­ã€WordæŠ¥å‘Š & JSON..."):
                processor.generate_report()
                st.success("âœ… é˜¶æ®µ 2 å®Œæˆï¼šæŠ¥å‘Šå·²ç”Ÿæˆ")

            st.divider()

            c1, c2, c3 = st.columns(3)

            # 1. JSON (ä¸­æ–‡å±•ç¤ºå Key)
            final_json_display = remap_json_keys(processor.final_json, REPORT_MAPPING)
            json_str = json.dumps(final_json_display, indent=4, ensure_ascii=False, default=str)
            c1.download_button("ğŸ“¥ ä¸‹è½½ JSON (Geminiç”¨ - ä¸­æ–‡Key)", json_str, "Ad_Report_Data.json", "application/json")

            # 2. Excel (ä¸­æ–‡å±•ç¤ºå Header)
            output_xls = io.BytesIO()
            with pd.ExcelWriter(output_xls, engine='xlsxwriter') as writer:
                for name, df in processor.merged_dfs.items(): 
                    # é‡å‘½ååˆ—å¤´ä¸ºä¸­æ–‡
                    df_display = df.rename(columns=REPORT_MAPPING)
                    df_display.to_excel(writer, sheet_name=name, index=False)
            c2.download_button("ğŸ“¥ ä¸‹è½½ Excel (åˆå¹¶åæ•°æ® - ä¸­æ–‡å¤´)", output_xls.getvalue(), "Merged_Ad_Report_Final.xlsx", "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")

            # 3. Word
            output_doc = io.BytesIO()
            processor.doc.save(output_doc)
            c3.download_button("ğŸ“¥ ä¸‹è½½ Word (æœ€ç»ˆæŠ¥å‘Š)", output_doc.getvalue(), "Ad_Report_Final_V20_10.docx", "application/vnd.openxmlformats-officedocument.wordprocessingml.document")

        except Exception as e:
            st.error(f"å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
            st.exception(e)

if __name__ == "__main__":
    main()
