import streamlit as st
import pandas as pd
import numpy as np
import io
import json
import xlsxwriter
from docx import Document
from docx.shared import Pt, RGBColor
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.oxml.shared import OxmlElement
from docx.oxml.ns import qn
import docx.opc.constants

# ==========================================
# PART 1: é…ç½®åŒºåŸŸ (ä¿®å¤äº†å­—æ®µæ˜ å°„)
# ==========================================

COMMON_METRICS = {
    "spend": ["èŠ±è´¹é‡‘é¢(USD)", "èŠ±è´¹é‡‘é¢ ï¼ˆUSDï¼‰", "èŠ±è´¹é‡‘é¢ (USD)", "èŠ±è´¹é‡‘é¢", "Amount Spent"],
    "roas": ["å¹¿å‘ŠèŠ±è´¹å›æŠ¥ (ROAS) - è´­ç‰©", "å¹¿å‘ŠèŠ±è´¹å›æŠ¥ï¼ˆROASï¼‰-è´­ç‰©", "ROAS", "Purchase ROAS"],
    "purchases": ["è´­ä¹°æ¬¡æ•°", "æˆæ•ˆæ•°é‡", "æˆæ•ˆ", "Purchases"],
    "cpa": ["å•æ¬¡è´­ä¹°è´¹ç”¨", "å•æ¬¡è´­ç‰©æˆæœ¬", "å•æ¬¡æˆæ•ˆæˆæœ¬", "å•æ¬¡æˆæ•ˆè´¹ç”¨", "Cost per Purchase"],
    "ctr": ["é“¾æ¥ç‚¹å‡»ç‡", "é“¾æ¥ç‚¹å‡»ç‡ï¼ˆ%)", "é“¾æ¥ç‚¹å‡»ç‡ï¼ˆ%ï¼‰", "CTR"],
    "cpm": ["åƒæ¬¡å±•ç¤ºè´¹ç”¨", "CPM"],
    "clicks": ["ç‚¹å‡»", "é“¾æ¥ç‚¹å‡»", "Clicks"],
    "impressions": ["æ›å…‰", "å±•ç¤ºæ¬¡æ•°", "Impressions"],
    "purchase_value": ["è´­ä¹°ä»·å€¼", "è´­ç‰©ä»·å€¼", "Purchase Value"],
    "aov": ["å•æ¬¡è´­ä¹°ä»·å€¼", "å•æ¬¡è´­ç‰©ä»·å€¼"]
}

# æ¡†å®šã€Œæ¯ä¸€ä¸ª Sheetã€éœ€è¦æŠ½å–å“ªäº›æŒ‡æ ‡
# âœ… ä¿®å¤ï¼šåœ¨"å—ä¼—ç»„"ä¸­å¢åŠ äº† converting_countries/genders/ages æ˜ å°„
SHEET_MAPPINGS = {
    "æ•´ä½“æ•°æ®": {
        **COMMON_METRICS,
        "date_range": ["æ—¶é—´èŒƒå›´"],
        "clicks_all": ["ç‚¹å‡»"],
        "landing_page_views": ["è½åœ°é¡µæµè§ˆé‡"],
        "add_to_cart": ["åŠ å…¥è´­ç‰©è½¦"],
        "initiate_checkout": ["ç»“è´¦å‘èµ·æ¬¡æ•°"],
        "rate_click_to_lp": ["ç‚¹å‡»-è½åœ°é¡µæµè§ˆè½¬åŒ–ç‡"],
        "rate_lp_to_atc": ["è½åœ°é¡µæµè§ˆ-åŠ è´­è½¬åŒ–ç‡"],
        "rate_atc_to_ic": ["åŠ è´­-ç»“è´¦è½¬åŒ–ç‡"],
        "rate_ic_to_pur": ["ç»“è´¦-è´­ä¹°è½¬åŒ–ç‡"]
    },
    "åˆ†æ—¶æ®µæ•°æ®": {
        **COMMON_METRICS,
        "date_range": ["æ—¶é—´èŒƒå›´"],
        "landing_page_views": ["è½åœ°é¡µæµè§ˆé‡"],
        "add_to_cart": ["åŠ å…¥è´­ç‰©è½¦"],
        "initiate_checkout": ["ç»“è´¦å‘èµ·æ¬¡æ•°"],
        "rate_click_to_lp": ["ç‚¹å‡»-è½åœ°é¡µæµè§ˆè½¬åŒ–ç‡"],
        "rate_lp_to_atc": ["è½åœ°é¡µæµè§ˆ-åŠ è´­è½¬åŒ–ç‡"],
        "rate_atc_to_ic": ["åŠ è´­-ç»“è´¦è½¬åŒ–ç‡"],
        "rate_ic_to_pur": ["ç»“è´¦-è´­ä¹°è½¬åŒ–ç‡"]
    },
    "å¼‚å¸¸æŒ‡æ ‡": {
        "anomaly_metric_name": ["å¼‚å¸¸æŒ‡æ ‡"],
        "mom_change": ["ç¯æ¯”"]
    },
    "å¹¿å‘Šæ¶æ„": {**COMMON_METRICS, "dimension_item": ["å¹¿å‘Šç±»å‹"]},
    "å—ä¼—ç»„": {
        **COMMON_METRICS,
        "dimension_item": ["å¹¿å‘Šç»„", "å¹¿å‘Šç»„Id", "Ad Set Name"],
        "custom_audience_settings": ["è®¾ç½®çš„è‡ªå®šä¹‰å—ä¼—", "Custom Audiences"],
        "converting_keywords": ["äº§ç”Ÿæˆæ•ˆçš„å…³é”®è¯", "Interests", "Keywords"],
        # âœ… æ–°å¢ä»¥ä¸‹ä¸‰è¡Œï¼Œç¡®ä¿ä»Excelä¸­è¯»å–è¿™äº›åˆ—
        "converting_countries": ["äº§ç”Ÿæˆæ•ˆçš„å›½å®¶", "å›½å®¶", "åœ°åŒº", "Country", "Region", "Location"],
        "converting_genders": ["äº§ç”Ÿæˆæ•ˆçš„æ€§åˆ«", "æ€§åˆ«", "Gender"],
        "converting_ages": ["äº§ç”Ÿæˆæ•ˆçš„å¹´é¾„", "å¹´é¾„", "Age", "Age Group"]
    },
    "å—ä¼—ç±»å‹": {**COMMON_METRICS, "dimension_item": ["å—ä¼—ç±»å‹"]},
    "å›½å®¶": {**COMMON_METRICS, "dimension_item": ["å›½å®¶/åœ°åŒº", "å›½å®¶"]},
    "å¹´é¾„": {**COMMON_METRICS, "dimension_item": ["å¹´é¾„"]},
    "æ€§åˆ«": {**COMMON_METRICS, "dimension_item": ["æ€§åˆ«"]},
    "å¹³å°&ç‰ˆä½": {**COMMON_METRICS, "dimension_item": ["å¹³å°&ç‰ˆä½"]},
    "ç´ æ": {
        **COMMON_METRICS,
        "content_item": ["ç´ æ"],
        "cvr_lp_to_pur": ["è½åœ°é¡µæµè§ˆ-è´­ä¹°è½¬åŒ–ç‡"]
    },
    "è½åœ°é¡µ": {
        **COMMON_METRICS,
        "content_item": ["è½åœ°é¡µurl", "è½åœ°é¡µ"],
        "ctr_all": ["æ›å…‰-ç‚¹å‡»è½¬åŒ–ç‡"],
        "rate_lp_to_atc": ["è½åœ°é¡µæµè§ˆ-åŠ è´­è½¬åŒ–ç‡", "è½åœ°é¡µæµè§ˆ-è´­ç‰©è½¬åŒ–ç‡"]
    }
}

GROUP_CONFIG = {
    "Master_Overview": ["æ•´ä½“æ•°æ®", "åˆ†æ—¶æ®µæ•°æ®", "å¼‚å¸¸æŒ‡æ ‡"],
    "Master_Breakdown": ["å¹¿å‘Šæ¶æ„", "å—ä¼—ç»„", "å—ä¼—ç±»å‹", "å›½å®¶", "å¹´é¾„", "æ€§åˆ«", "å¹³å°&ç‰ˆä½"],
    "Master_Creative": ["ç´ æ", "è½åœ°é¡µ"]
}

REPORT_MAPPING = {
    "spend": "èŠ±è´¹ ($)", "roas": "ROAS", "purchases": "è´­ä¹°æ¬¡æ•°", "purchase_value": "è´­ä¹°æ€»ä»·å€¼",
    "cpa": "CPA ($)", "ctr": "CTR (%)", "cpm": "CPM ($)", "aov": "å®¢å•ä»·",
    "impressions": "å±•ç°é‡", "clicks_all": "ç‚¹å‡»é‡ (All)", "clicks": "ç‚¹å‡»é‡ (All)", "ctr_all": "ç‚¹å‡»ç‡ (All)",
    "landing_page_views": "è½åœ°é¡µè®¿é—®é‡", "add_to_cart": "åŠ è´­æ¬¡æ•°", "initiate_checkout": "ç»“è´¦å‘èµ·æ•° (IC)",
    "rate_click_to_lp": "ç‚¹å‡» â†’ è½åœ°é¡µè®¿é—®è½¬åŒ–ç‡", "rate_lp_to_atc": "è½åœ°é¡µ â†’ åŠ è´­è½¬åŒ–ç‡",
    "rate_atc_to_ic": "åŠ è´­ â†’ è´­ä¹°è½¬åŒ–ç‡", "rate_ic_to_pur": "è´­ä¹°è½¬åŒ–ç‡",
    "cvr_purchase": "ç‚¹å‡» â†’ è´­ä¹°è½¬åŒ–ç‡", "cvr_lp_to_pur": "CVR (å…¨ç«™è½¬åŒ–ç‡)",
    "date_range": "æ—¥æœŸ/æ—¶æ®µ", "campaign_type": "æŠ•æ”¾æ¨¡å¼", "adset_name": "å¹¿å‘Šç»„ID", "adset_id": "å¹¿å‘Šç»„ID",
    "custom_audience_settings": "è‡ªå®šä¹‰å—ä¼—æº", "converting_keywords": "é«˜æ½œå…´è¶£è¯", "audience_type": "å—ä¼—ç­–ç•¥",
    "country": "å›½å®¶", "age_group": "å¹´é¾„", "gender": "æ€§åˆ«", "creative_name": "ç´ æåç§°", "placement": "ç‰ˆä½",
    "landing_page_url": "é¡µé¢ URL", "mom_change": "ç¯æ¯”æ³¢åŠ¨", "anomaly_metric_name": "å¼‚å¸¸é¡¹",
    "converting_countries": "äº§ç”Ÿæˆæ•ˆçš„å›½å®¶", "converting_genders": "äº§ç”Ÿæˆæ•ˆçš„æ€§åˆ«", "converting_ages": "äº§ç”Ÿæˆæ•ˆçš„å¹´é¾„"
}

# âœ… å¢å¼ºäº†æ¨¡ç³ŠåŒ¹é…åˆ«å
FIELD_ALIASES = {
    "adset_id": ["adset_id", "ad set id", "adset id", "å¹¿å‘Šç»„ç¼–å·", "å¹¿å‘Šç»„id", "adset_name", "ad set name"],
    "converting_countries": ["converting_countries", "country", "region", "å›½å®¶", "åœ°åŒº", "location"],
    "converting_genders": ["converting_genders", "gender", "æ€§åˆ«"],
    "converting_ages": ["converting_ages", "age", "å¹´é¾„", "age_group"],
    "converting_keywords": ["converting_keywords", "keywords", "interests", "å…´è¶£", "å…³é”®è¯"],
    "spend": ["spend", "amount spent", "cost", "èŠ±è´¹", "æ¶ˆè€—"],
    "purchases": ["purchases", "results", "result", "æˆæ•ˆ", "è´­ä¹°"],
    "roas": ["roas", "return on ad spend", "purchase roas"],
    "purchase_value": ["purchase_value", "conversion value", "value", "æ€»ä»·å€¼", "gmv", "è´­ä¹°æ€»ä»·å€¼"],
    "clicks": ["clicks", "clicks (all)", "ç‚¹å‡»é‡", "clicks_all"],
    "impressions": ["impressions", "å±•ç¤º", "å±•ç°"],
    "ctr_all": ["ctr_all", "ctr (all)", "ç‚¹å‡»ç‡ (all)"]
}


# ==========================================
# PART 2: æ ¸å¿ƒå·¥å…·å‡½æ•° (ä¿æŒä¸å˜ï¼Œç•¥)
# ==========================================

# ==========================================
# PART 2: æ ¸å¿ƒå·¥å…·å‡½æ•° (å·²ä¿®å¤ç™¾åˆ†æ¯”è¯†åˆ«é—®é¢˜)
# ==========================================

def parse_float(value):
    """è¾…åŠ©å‡½æ•°ï¼šæ¸…ç†æ•°æ®å¹¶å°†å­—ç¬¦ä¸²/æ•°å­—å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
    if value is None:
        return 0.0
    try:
        # å¦‚æœå·²ç»æ˜¯æ•°å­—ï¼Œç›´æ¥è¿”å›
        if isinstance(value, (int, float)):
            return float(value)
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè°ƒç”¨ clean_numeric_strict è¿›è¡Œæ ‡å‡†å¤„ç†
        return clean_numeric_strict(value)
    except (ValueError, TypeError):
        return 0.0

def safe_div(numerator, denominator, multiplier=1.0):
    n = parse_float(numerator)
    d = parse_float(denominator)
    if d > 0:
        return (n / d) * multiplier
    else:
        return 0.0

# å®½æ¾æ¸…æ´—ï¼ˆç”¨äºå±•ç¤ºï¼‰
def clean_numeric(val):
    if pd.isna(val): return 0.0
    if isinstance(val, (int, float)): return float(val)
    val_str = str(val).strip().replace('$', '').replace('Â¥', '').replace(',', '')
    
    # âœ… ä¿®å¤ç‚¹ 1ï¼šå¦‚æœæ˜¯ç™¾åˆ†æ•°å­—ç¬¦ä¸²ï¼Œè½¬æ¢åé™¤ä»¥ 100
    if '%' in val_str: 
        val_str = val_str.replace('%', '')
        try: return float(val_str) / 100.0 
        except: return 0.0
        
    try: return float(val_str)
    except: return val

# ä¸¥æ ¼æ¸…æ´—ï¼ˆç”¨äºè®¡ç®—ï¼‰
def clean_numeric_strict(val): 
    if pd.isna(val): return 0.0
    # å¦‚æœå·²ç»æ˜¯æ•°å­—ï¼Œç›´æ¥è¿”å›
    if isinstance(val, (int, float)): return float(val)
    
    val_str = str(val).strip().replace('$', '').replace('Â¥', '').replace(',', '')
    
    # âœ… ä¿®å¤ç‚¹ 2ï¼šå¦‚æœæ˜¯ç™¾åˆ†æ•°å­—ç¬¦ä¸²ï¼ˆå¦‚ "2.31%"ï¼‰ï¼Œå»é™¤%åé™¤ä»¥100è¿˜åŸä¸ºå°æ•°ï¼ˆ0.0231ï¼‰
    if '%' in val_str: 
        val_str = val_str.replace('%', '')
        try: return float(val_str) / 100.0
        except: return 0.0
        
    try: return float(val_str)
    except: return 0.0

# å­—æ®µé²æ£’æ ¸å¿ƒ
def find_column_fuzzy(df, keywords):
    for kw in keywords:
        if kw in df.columns: return kw
    df_cols_norm = {c.lower().replace(' ', '').replace('_', ''): c for c in df.columns}
    for kw in keywords:
        kw_norm = kw.lower().replace(' ', '').replace('_', '')
        if kw_norm in df_cols_norm: return df_cols_norm[kw_norm]
    for col in df.columns:
        col_lower = col.lower()
        for kw in keywords:
            if kw.lower() in col_lower: return col
    return None

# æ ¸å¿ƒæŒ‡æ ‡è®¡ç®— (ä¿æŒä¸å˜)
def calc_metrics_dict(df_chunk):
    res = {}
    if df_chunk.empty: return res
    sums = {}
    targets = ['spend', 'clicks', 'impressions', 'purchases', 'purchase_value',
               'landing_page_views', 'add_to_cart', 'initiate_checkout']
    
    for t in targets:
        aliases = FIELD_ALIASES.get(t, [t])
        if t == 'purchase_value' and 'value' not in aliases: aliases.append('value')
        col = find_column_fuzzy(df_chunk, aliases)
        if col:
             sums[t] = df_chunk[col].apply(clean_numeric_strict).sum()
        else:
             sums[t] = 0.0

    res['spend'] = parse_float(sums.get('spend', 0))
    res['impressions'] = parse_float(sums.get('impressions', 0))
    res['clicks'] = parse_float(sums.get('clicks', 0))
    res['purchases'] = parse_float(sums.get('purchases', 0))
    res['purchase_value'] = parse_float(sums.get('purchase_value', 0))
    res['roas'] = safe_div(sums.get('purchase_value'), sums.get('spend'))
    res['cpm'] = safe_div(sums.get('spend'), sums.get('impressions'), multiplier=1000)
    res['cpc'] = safe_div(sums.get('spend'), sums.get('clicks'))
    res['ctr'] = safe_div(sums.get('clicks'), sums.get('impressions'))
    res['cpa'] = safe_div(sums.get('spend'), sums.get('purchases'))
    res['cvr_purchase'] = safe_div(sums.get('purchases'), sums.get('clicks'))
    res['rate_click_to_lp'] = safe_div(sums.get('landing_page_views'), sums.get('clicks'))
    res['rate_lp_to_atc']   = safe_div(sums.get('add_to_cart'), sums.get('landing_page_views'))
    res['rate_atc_to_ic']   = safe_div(sums.get('initiate_checkout'), sums.get('add_to_cart'))
    res['rate_ic_to_pur']   = safe_div(sums.get('purchases'), sums.get('initiate_checkout'))
    res['aov'] = safe_div(sums.get('purchase_value'), sums.get('purchases'))

    date_col = find_column_fuzzy(df_chunk, ['date', 'time', 'range'])
    if date_col:
        try:
            dates = pd.to_datetime(df_chunk[date_col], errors='coerce').dropna()
            if not dates.empty: res['date_range'] = f"{dates.min():%Y-%m-%d} ~ {dates.max():%Y-%m-%d}"
            else: res['date_range'] = "-"
        except: res['date_range'] = "-"
    else: res['date_range'] = "-"
    return res 

def format_cell(key, val, is_mom=False):
    if isinstance(val, str): return val
    if is_mom:
        if key == 'date_range': return val
        return f"{val:+.2%}"
    k = str(key).lower()
    if 'roas' in k: return f"{val:.2f}"
    if any(x in k for x in ['rate', 'ctr', 'cvr', 'ç‚¹å‡»ç‡', 'è½¬åŒ–ç‡', 'ç€é™†ç‡', 'æ„å‘ç‡', 'æˆåŠŸç‡']): 
        # è¿™é‡Œä¼šä¹˜ä»¥100ï¼Œæ‰€ä»¥è¾“å…¥å¿…é¡»æ˜¯å°æ•° (0.0231 -> 2.31%)
        return f"{val:.2%}" 
    if any(x in k for x in ['spend', 'cpm', 'cpc', 'value', 'aov', 'cpa', 'èŠ±è´¹', 'é‡‘é¢', 'å®¢å•ä»·', 'gmv', 'ä»·å€¼']): return f"{val:,.2f}"
    if any(x in k for x in ['purchases', 'cart', 'click', 'æ¬¡æ•°', 'å•é‡', 'ç‚¹å‡»', 'å±•ç°', 'è®¿é—®é‡', 'å‘èµ·æ•°']): return f"{val:,.0f}"
    return f"{val}"

def extract_benchmark_values(df_bench):
    targets = {'roas': (['roas'], True), 'cpm': (['cpm'], False), 'ctr': (['ctr'], True), 'cpc': (['cpc'], False), 'cpa': (['cpa_purchase', 'cpa'], False)}
    extracted = {}
    for metric, (aliases, higher_better) in targets.items():
        found_col = None
        for alias in aliases:
            found_col = find_column_fuzzy(df_bench, [alias])
            if found_col: break
        if found_col:
            try:
                s = df_bench[found_col].apply(clean_numeric_strict)
                v = s[s>0].mean()
                
                # âœ… ä¿®å¤ç‚¹ 3ï¼šé˜²å¾¡æ€§é€»è¾‘
                # å¦‚æœæ˜¯ CTR/CVR ç­‰æ¯”ç‡ç±»æŒ‡æ ‡ï¼Œä¸”åŸºå‡†å€¼ > 1.0 (ä¾‹å¦‚ç”¨æˆ·å¡«äº† 2.31 è€Œä¸æ˜¯ 0.0231)ï¼Œ
                # ä¸”è¯¥åˆ—ä¸æ˜¯ CPA/CPM/ROAS/CPC è¿™ç§æœ¬èº«å°±å¾ˆå¤§çš„å€¼ï¼Œåˆ™å¼ºåˆ¶é™¤ä»¥100
                if metric in ['ctr'] and v > 1.0:
                    v = v / 100.0
                    
                if not pd.isna(v): extracted[metric] = [v, higher_better]
            except: pass
    return extracted

# ... (add_hyperlink, apply_report_labels, add_df_to_word ä¿æŒä¸å˜)

def add_hyperlink(paragraph, url, text, color="0000FF", underline=True):
    try:
        part = paragraph.part
        r_id = part.relate_to(url, docx.opc.constants.RELATIONSHIP_TYPE.HYPERLINK, is_external=True)
        hyperlink = OxmlElement('w:hyperlink')
        hyperlink.set(qn('r:id'), r_id)
        new_run = OxmlElement('w:r')
        rPr = OxmlElement('w:rPr')
        if color:
            c = OxmlElement('w:color')
            c.set(qn('w:val'), color)
            rPr.append(c)
        if underline:
            u = OxmlElement('w:u')
            u.set(qn('w:val'), 'single')
            rPr.append(u)
        new_run.append(rPr)
        new_run.text = text
        hyperlink.append(new_run)
        paragraph._p.append(hyperlink)
        return hyperlink
    except: return None

def apply_report_labels(df, custom_mapping=None):
    if df.empty: return df
    mapping = REPORT_MAPPING.copy()
    if custom_mapping: mapping.update(custom_mapping)
    return df.rename(columns=mapping)

def add_df_to_word(doc, df, title, level=1):
    if df.empty: return
    doc.add_heading(title, level=level)
    t = doc.add_table(rows=df.shape[0]+1, cols=df.shape[1])
    t.style = 'Table Grid'
    is_creative = "ç´ æ" in title
    is_landing = "è½åœ°é¡µ" in title
    link_col_idx = -1
    for j, col in enumerate(df.columns):
        cell = t.cell(0, j)
        cell.text = str(col)
        if any(x in str(col).lower() for x in ["url", "link", "ç´ æ", "å†…å®¹", "content"]): link_col_idx = j
        for p in cell.paragraphs:
            for r in p.runs:
                r.font.bold = True
                r.font.size = Pt(8)
    for i in range(df.shape[0]):
        label_prefix = "ç´ æ" if is_creative else ("è½åœ°é¡µ" if is_landing else "")
        label_char = chr(65 + (i % 26))
        if i >= 26: label_char += str(i // 26)
        label_text = f"{label_prefix}{label_char}"
        for j in range(df.shape[1]):
            val = df.iat[i, j]
            cell = t.cell(i+1, j)
            if (is_creative or is_landing) and j == link_col_idx:
                try:
                    p = cell.paragraphs[0]
                    url = str(val).strip()
                    if len(url) > 5: add_hyperlink(p, url, label_text)
                    else: cell.text = label_text
                except: cell.text = label_text
            else:
                cell.text = str(val)
                if "ç»“è®º" in str(df.columns[j]):
                    if "âœ…" in str(val): cell.paragraphs[0].runs[0].font.color.rgb = RGBColor(0, 128, 0)
                    if "âš ï¸" in str(val): cell.paragraphs[0].runs[0].font.color.rgb = RGBColor(255, 0, 0)
            for p in cell.paragraphs:
                for r in p.runs: r.font.size = Pt(8)
    doc.add_paragraph("\n")

# ==========================================
# PART 3: ä¸»é€»è¾‘ç±» (Process ETL ä¸­åŒ…å«äº†å…³é”®ä¿®å¤)
# ==========================================

class AdReportProcessor:
    def __init__(self, raw_file, bench_file=None):
        self.raw_file = raw_file
        self.bench_file = bench_file
        self.processed_dfs = {}
        self.merged_dfs = {}
        self.final_json = {}
        self.doc = Document()

    # --- é˜¶æ®µ 1: æ•°æ®æ¸…æ´—ä¸é™ç»´ ---
    def process_etl(self):
        xls = pd.ExcelFile(self.raw_file)
        for sheet_name, mapping in SHEET_MAPPINGS.items():
            if sheet_name in xls.sheet_names:
                df = pd.read_excel(xls, sheet_name=sheet_name)
                final_cols = {}
                # å­—æ®µæ˜ å°„
                for std_col, raw_col_options in mapping.items():
                    matched_col = None
                    for option in raw_col_options:
                        if option in df.columns: matched_col = option; break
                        if not matched_col:
                            for df_col in df.columns:
                                if option.replace(" ", "") == df_col.replace(" ", ""): matched_col = df_col; break
                        if matched_col: break
                    if matched_col: final_cols[std_col] = matched_col

                if final_cols:
                    df_clean = df[list(final_cols.values())].rename(columns={v: k for k, v in final_cols.items()})
                    
                    # âœ… ä¿®å¤é‡ç‚¹ï¼šå°† converting_countries ç­‰åˆ—åŠ å…¥ã€Œä¸è¿›è¡Œæ•°å­—æ¸…æ´—ã€çš„ç™½åå•
                    text_cols = ['date_range', 'anomaly_metric_name', 
                                 'converting_keywords', 'converting_countries', 'converting_genders', 'converting_ages', 
                                 'custom_audience_settings', 'dimension_item', 'content_item']
                    
                    for col in df_clean.columns:
                        if col not in text_cols:
                            df_clean[col] = df_clean[col].apply(clean_numeric)

                    if sheet_name in ["ç´ æ", "è½åœ°é¡µ", "å—ä¼—ç»„"]:
                        if "spend" in df_clean.columns:
                            df_clean = df_clean.sort_values("spend", ascending=False).head(10)

                    df_clean["Source_Sheet"] = sheet_name
                    self.processed_dfs[sheet_name] = df_clean

        # åˆå¹¶ Master Tables
        for master_name, source_sheets in GROUP_CONFIG.items():
            dfs_to_merge = [self.processed_dfs[src] for src in source_sheets if src in self.processed_dfs]
            if dfs_to_merge:
                merged_df = pd.concat(dfs_to_merge, ignore_index=True)
                cols = list(merged_df.columns)
                priority_cols = ['Source_Sheet', 'date_range', 'dimension_item', 'content_item',
                                 'spend', 'roas', 'purchases', 'cpa']
                new_order = [c for c in priority_cols if c in cols] + [c for c in cols if c not in priority_cols]
                self.merged_dfs[master_name] = merged_df[new_order]

    # --- é˜¶æ®µ 2: æŠ¥å‘Šç”Ÿæˆä¸æ¶æ„è¯Šæ–­ ---
    def generate_report(self):
        benchmark_targets = {'roas': [2.0, True], 'cpm': [20.0, False], 'ctr': [0.015, True], 'cpc': [1.5, False], 'cpa': [30.0, False]}
        if self.bench_file:
            try:
                df_b = pd.read_excel(self.bench_file)
                benchmark_targets = extract_benchmark_values(df_b)
            except: pass

        self.doc.add_heading('å¹¿å‘ŠæŠ•æ”¾æ·±åº¦åˆ†ææŠ¥å‘Š', 0).alignment = WD_ALIGN_PARAGRAPH.CENTER
        self.final_json = {"report_title": "å¹¿å‘ŠæŠ•æ”¾æ·±åº¦åˆ†ææŠ¥å‘Š", "generated_at": pd.Timestamp.now().strftime("%Y-%m-%d")}

        # 1. å¤§ç›˜æ€»è§ˆ (ä¿æŒä¸å˜)
        df_ov = pd.DataFrame()
        if "Master_Overview" in self.merged_dfs:
            df_src = self.merged_dfs["Master_Overview"]
            mask = df_src['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["åˆ†æ—¶", "Time"]))
            df_ov = df_src[mask].copy() if not df_src[mask].empty else df_src.copy()

        if not df_ov.empty:
            date_col = find_column_fuzzy(df_ov, ['date', 'time', 'æ—¶é—´'])
            if date_col:
                try:
                    df_ov['temp_date'] = pd.to_datetime(df_ov[date_col], errors='coerce')
                    df_clean = df_ov.dropna(subset=['temp_date']).sort_values('temp_date')
                    dates = df_clean['temp_date'].unique()
                    raw_overall = calc_metrics_dict(df_clean)
                    if len(dates) >= 2:
                        mid_date = dates[len(dates)//2]
                        raw_prev = calc_metrics_dict(df_clean[df_clean['temp_date'] < mid_date])
                        raw_curr = calc_metrics_dict(df_clean[df_clean['temp_date'] >= mid_date])
                        raw_mom = {}
                        for k, v_curr in raw_curr.items():
                            if k == 'date_range': raw_mom[k] = "-"
                            else:
                                v_prev = raw_prev.get(k, 0)
                                raw_mom[k] = (v_curr - v_prev) / v_prev if v_prev > 0 else 0.0
                    else:
                        raw_prev = {k: "-" for k in raw_overall}; raw_curr = raw_overall; raw_mom = {k: "-" for k in raw_overall}

                    col_order = ["date_range", "spend", "roas", "cpa", "cpm", "cpc", "ctr", "cvr_purchase",
                                 "rate_click_to_lp", "rate_lp_to_atc", "rate_ic_to_pur", "aov", "add_to_cart", "purchases", "purchase_value"]
                    final_data = []
                    for label, r in zip(["æ•´ä½“æ•°æ®", "ä¸Šå‘¨æœŸå€¼", "æœ¬å‘¨æœŸ", "ç¯æ¯”"], [raw_overall, raw_prev, raw_curr, raw_mom]):
                        row = {"Label": label}
                        is_m = (label == "ç¯æ¯”")
                        for c in col_order: row[c] = format_cell(c, r.get(c, 0), is_mom=is_m)
                        row['date_range'] = label
                        final_data.append(row)

                    df_f = pd.DataFrame(final_data, columns=col_order)
                    df_f_display = apply_report_labels(df_f)
                    add_df_to_word(self.doc, df_f_display, "1. æ•°æ®å¤§ç›˜æ€»è§ˆ", level=1)
                    self.final_json['1_data_overview'] = df_f.to_dict(orient='records')

                    # 2. Benchmark
                    raw_current = calc_metrics_dict(df_clean)
                    bench_data = []
                    for metric_key in ['roas', 'cpm', 'ctr', 'cpc', 'cpa']:
                        curr_val = raw_current.get(metric_key, 0)
                        bench_val, higher_is_better = benchmark_targets.get(metric_key, [0, True])
                        conclusion = "-"
                        if curr_val != 0:
                            diff = curr_val - bench_val
                            if higher_is_better: conclusion = "âœ… ä¼˜äºå¤§ç›˜" if diff > 0 else ("âš ï¸ ä½äºå¤§ç›˜" if diff < 0 else "æŒå¹³")
                            else: conclusion = "âœ… ä¼˜äºå¤§ç›˜" if diff < 0 else ("âš ï¸ é«˜äºå¤§ç›˜" if diff > 0 else "æŒå¹³")
                        bench_data.append({
                            "æŒ‡æ ‡": REPORT_MAPPING.get(metric_key, metric_key.upper()),
                            "å½“å‰è´¦æˆ·": format_cell(metric_key, curr_val),
                            "è¡Œä¸šåŸºå‡†": format_cell(metric_key, bench_val),
                            "å¯¹æ¯”ç»“è®º": conclusion
                        })
                    df_b = pd.DataFrame(bench_data)
                    add_df_to_word(self.doc, df_b, "2. è¡Œä¸š Benchmark å¯¹æ¯”", level=1)
                    self.final_json['2_industry_benchmark'] = df_b.to_dict(orient='records')
                except Exception as e: st.warning(f"å¤§ç›˜è®¡ç®—è­¦å‘Š: {e}")

        # 3. å—ä¼—ç»„
        self.doc.add_heading("3. å—ä¼—ç»„åˆ†æ", level=1)
        self.final_json['3_audience_analysis'] = {}
        audience_configs = [
            ("3.1 å›½å®¶åˆ†æ", ["å›½å®¶", "Country"], True, "å›½å®¶"),
            ("3.2 æ€§åˆ«åˆ†æ", ["æ€§åˆ«", "Gender"], False, "æ€§åˆ«"),
            ("3.3 å¹´é¾„åˆ†æ", ["å¹´é¾„", "Age"], False, "å¹´é¾„æ®µ"),
            ("3.4 å—ä¼—ç»„åˆ†æè¡¨", ["å—ä¼—", "Audience"], True, "å—ä¼—ç»„åç§°"),
        ]

        if "Master_Breakdown" in self.merged_dfs:
            df_bd = self.merged_dfs["Master_Breakdown"]
            for title, keywords, top10, dim_label in audience_configs:
                mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in keywords))
                df_curr = df_bd[mask].copy()
                if not df_curr.empty:
                    if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan) if 'clicks' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['cpm']): df_curr['cpm'] = (df_curr['spend'] / df_curr['impressions'].replace(0, np.nan)) * 1000 if 'impressions' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan) if 'impressions' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan) if 'purchases' in df_curr else 0

                    req_cols = ["dimension_item", "spend", "ctr", "cpc", "cpm", "cpa", "roas"]
                    # âœ… ç°åœ¨ï¼Œå¦‚æœæ˜¯å—ä¼—è¡¨ï¼Œè¿™äº›å­—æ®µå·²ç»è¢«ä¿ç•™ä¸‹æ¥äº†
                    if "å—ä¼—" in title: req_cols += ["converting_countries", "converting_keywords", "converting_genders", "converting_ages"]

                    rename_map = {}; valid_cols = []
                    for req in req_cols:
                        aliases = FIELD_ALIASES.get(req, [req])
                        found = find_column_fuzzy(df_curr, aliases)
                        if found: valid_cols.append(found); rename_map[found] = req
                        else: 
                            # å¯¹äºæ–‡æœ¬å­—æ®µï¼Œç»™ "-" è€Œä¸æ˜¯ 0.0
                            default_val = "-" if "converting" in req else 0.0
                            df_curr[req] = default_val; valid_cols.append(req)

                    df_final = df_curr[valid_cols].rename(columns=rename_map)
                    if "dimension_item" in df_final.columns:
                         df_final = df_final[~df_final['dimension_item'].astype(str).str.lower().str.contains('unknow', na=False)]

                    if top10 and 'spend' in df_final.columns: df_final = df_final.sort_values('spend', ascending=False).head(10)
                    df_clean = df_final.round(2)
                    df_display = apply_report_labels(df_clean, custom_mapping={'dimension_item': dim_label})
                    add_df_to_word(self.doc, df_display, title, level=2)
                    self.final_json['3_audience_analysis'][title] = df_clean.to_dict(orient='records')

        # 4. ç´ æä¸è½åœ°é¡µ (ä¿æŒä¸å˜)
        if "Master_Creative" in self.merged_dfs:
            df_cr = self.merged_dfs["Master_Creative"]
            for title, keywords, label, json_key in [("4. ç´ æåˆ†æ", ["ç´ æ", "Creative"], "ç´ æåç§°", "4_creative_analysis"), ("6. è½åœ°é¡µåˆ†æ", ["è½åœ°é¡µ", "Landing"], "è½åœ°é¡µ URL", "6_landing_page_analysis")]:
                mask = df_cr['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in keywords))
                df_curr = df_cr[mask].copy()
                if not df_curr.empty:
                    if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan) if 'clicks' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan) if 'purchases' in df_curr else 0
                    if not find_column_fuzzy(df_curr, ['ctr']):
                         if 'impressions' in df_curr and 'clicks' in df_curr: df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan)
                         else: df_curr['ctr'] = np.nan
                    if 'cpc' in df_curr.columns and 'cpm' in df_curr.columns:
                        mask_fix = (df_curr['ctr'].isna() | (df_curr['ctr'] == 0)) & (df_curr['cpc'] > 0)
                        if mask_fix.any(): df_curr.loc[mask_fix, 'ctr'] = df_curr.loc[mask_fix, 'cpm'] / (df_curr.loc[mask_fix, 'cpc'] * 1000)
                    df_curr['ctr'] = df_curr['ctr'].fillna(0) * 100 

                    req_cols = ["content_item", "spend", "ctr", "cpc", "cpm", "roas", "cpa"]
                    rename_map = {}; valid_cols = []
                    for req in req_cols:
                        aliases = FIELD_ALIASES.get(req, [req])
                        found = find_column_fuzzy(df_curr, aliases)
                        if found: valid_cols.append(found); rename_map[found] = req
                        else: df_curr[req] = 0.0; valid_cols.append(req)
                    df_final = df_curr[valid_cols].rename(columns=rename_map)
                    if 'spend' in df_final.columns: df_final = df_final.sort_values('spend', ascending=False).head(10)
                    df_clean = df_final.round(2) 
                    df_display = apply_report_labels(df_clean, custom_mapping={'content_item': label})
                    add_df_to_word(self.doc, df_display, title, level=1)
                    self.final_json[json_key] = df_clean.to_dict(orient='records')
                    
        # 5. ç‰ˆä½ (ä¿æŒä¸å˜)
        if "Master_Breakdown" in self.merged_dfs:
             self.doc.add_heading("5. ç‰ˆä½åˆ†æ", level=1)
             df_bd = self.merged_dfs["Master_Breakdown"]
             mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["ç‰ˆä½", "Placement"]))
             df_curr = df_bd[mask].copy()
             if not df_curr.empty:
                 if not find_column_fuzzy(df_curr, ['cpc']): df_curr['cpc'] = df_curr['spend'] / df_curr['clicks'].replace(0, np.nan) if 'clicks' in df_curr else 0
                 if not find_column_fuzzy(df_curr, ['cpa']): df_curr['cpa'] = df_curr['spend'] / df_curr['purchases'].replace(0, np.nan) if 'purchases' in df_curr else 0
                 if not find_column_fuzzy(df_curr, ['ctr']): df_curr['ctr'] = df_curr['clicks'] / df_curr['impressions'].replace(0, np.nan) if 'impressions' in df_curr else 0
                 if not find_column_fuzzy(df_curr, ['cpm']): df_curr['cpm'] = (df_curr['spend'] / df_curr['impressions'].replace(0, np.nan)) * 1000 if 'impressions' in df_curr else 0
                 req_cols = ['dimension_item', 'spend', 'ctr', 'cpc', 'cpm', 'roas', 'cpa']
                 rename_map = {}; valid_cols = []
                 for c in req_cols:
                     aliases = FIELD_ALIASES.get(c, [c])
                     f = find_column_fuzzy(df_curr, aliases)
                     if f: valid_cols.append(f); rename_map[f] = c
                     else: df_curr[c] = 0.0; valid_cols.append(c)
                 df_clean = df_curr[valid_cols].rename(columns=rename_map).round(2)
                 df_top5 = df_clean.sort_values('spend', ascending=False).head(5)
                 add_df_to_word(self.doc, apply_report_labels(df_top5, {'dimension_item': 'ç‰ˆä½'}), "5.1 ç‰ˆä½èŠ±è´¹ TOP 5", level=2)
                 mean_ctr = df_clean['ctr'].mean(); mean_cpm = df_clean['cpm'].mean()
                 mask_pot = (df_clean['ctr'] > mean_ctr) & (df_clean['cpm'] < mean_cpm)
                 df_pot = df_clean[mask_pot].sort_values('ctr', ascending=False).head(5)
                 if df_pot.empty: df_pot = df_clean.sort_values('ctr', ascending=False).head(5)
                 add_df_to_word(self.doc, apply_report_labels(df_pot, {'dimension_item': 'ç‰ˆä½'}), "5.2 ç‰ˆä½é«˜æ½œåŠ›", level=2)
                 self.final_json['5_placement_analysis'] = {"top_spend": df_top5.to_dict('records'), "high_potential": df_pot.to_dict('records')}

        # 7. æ¶æ„è¯Šæ–­ (ä¿æŒä¸å˜)
        rows = []
        if "Master_Overview" in self.merged_dfs:
             metrics = calc_metrics_dict(self.merged_dfs["Master_Overview"])
             if not metrics: metrics = {} 
             rows.append({
                "æ¨¡å—": "é¢„ç®—ç»“æ„", 
                "å½“å‰ç»“æ„æ•°æ®è¡¨ç°": (
                    f"æ€»èŠ±è´¹: ${float(str(metrics.get('spend', 0)).replace(',', '')):,.2f}\n"
                    f"CPA: ${float(str(metrics.get('cpa', 0)).replace(',', '')):.2f}\n"
                    f"ROAS: {float(str(metrics.get('roas', 0)).replace(',', '')):.2f}"
                ), 
                "å­˜åœ¨çš„é—®é¢˜": ""
             })
        if "Master_Breakdown" in self.merged_dfs:
            df_bd = self.merged_dfs["Master_Breakdown"]
            mask = df_bd['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["å—ä¼—", "Audience"]))
            df_aud = df_bd[mask]
            s_col = find_column_fuzzy(df_aud, ['spend']); active_count = len(df_aud[df_aud[s_col] > 0]) if s_col else 0
            top_share = "0%"
            if not df_aud.empty and s_col:
                total_s = df_aud[s_col].sum()
                if total_s > 0: top_share = f"{df_aud[s_col].max()/total_s:.1%}"
            rows.append({"æ¨¡å—": "å—ä¼—ç»“æ„", "å½“å‰ç»“æ„æ•°æ®è¡¨ç°": f"æ´»è·ƒå—ä¼—ç»„æ•°: {active_count}\nTop1 èŠ±è´¹å æ¯”: {top_share}", "å­˜åœ¨çš„é—®é¢˜": ""})
        if "Master_Creative" in self.merged_dfs:
             df_cr = self.merged_dfs["Master_Creative"]
             mask = df_cr['Source_Sheet'].astype(str).apply(lambda x: any(k in x for k in ["ç´ æ", "Creative"]))
             df_mat = df_cr[mask]
             s_col = find_column_fuzzy(df_mat, ['spend']); active_count = len(df_mat[df_mat[s_col] > 0]) if s_col else 0
             rows.append({"æ¨¡å—": "ç´ æç»“æ„", "å½“å‰ç»“æ„æ•°æ®è¡¨ç°": f"æ´»è·ƒç´ ææ•°: {active_count}", "å­˜åœ¨çš„é—®é¢˜": ""})

        df_struct = pd.DataFrame(rows)
        add_df_to_word(self.doc, df_struct, "7. å¹¿å‘Šæ¶æ„åˆ†æ", level=1)
        if "Master_Overview" in self.merged_dfs:
             self.final_json['7_structure_analysis'] = df_struct.to_dict(orient='records')

# ==========================================
# PART 4: Streamlit UI (ä¿æŒä¸å˜)
# ==========================================
def set_artistic_style():
    st.markdown("""
        <style>
        /* å¼•å…¥ç°ä»£æ— è¡¬çº¿å­—ä½“ */
        @import url('https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@400;500;600;700&display=swap');
        
        /* å…¨å±€é‡ç½® */
        .stApp {
            background-color: #FAFBFF; /* ææ·¡çš„è“ç´«è‰²èƒŒæ™¯ */
            font-family: 'Plus Jakarta Sans', sans-serif;
            color: #2D3748;
        }
        
        /* æ ‡é¢˜æ ·å¼ - æŸ”å’Œæ¸å˜ */
        h1 {
            font-size: 3rem !important;
            font-weight: 800;
            background: linear-gradient(120deg, #845EC2, #D65DB1, #FF6F91);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            letter-spacing: -1px;
            margin-bottom: 10px;
            padding-bottom: 10px;
        }

        /* è£…é¥°æ€§åˆ†å‰²çº¿ - æŸ”å…‰ */
        .art-divider {
            height: 2px;
            background: linear-gradient(90deg, rgba(132, 94, 194, 0.1), rgba(255, 111, 145, 0.5), rgba(132, 94, 194, 0.1));
            margin: 20px 0 50px 0;
            border-radius: 1px;
        }

        /* æ¨¡å—æ ‡é¢˜ (å¦‚ 1 æ•°æ®æºè¾“å…¥) */
        .section-header {
            font-size: 1.1rem;
            font-weight: 600;
            color: #4A5568;
            background: rgba(255, 255, 255, 0.6);
            backdrop-filter: blur(10px);
            display: inline-block;
            padding: 8px 16px;
            border-radius: 20px; /* å¤§åœ†è§’ */
            margin-bottom: 15px;
            border: 1px solid rgba(255, 255, 255, 0.8);
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
        }
        .section-header span {
            display: inline-block;
            background: linear-gradient(120deg, #845EC2, #FF9671);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            font-weight: 800;
            margin-right: 8px;
        }

        /* å¡ç‰‡å®¹å™¨ (åŠŸèƒ½æŒ‡å—) - æ‚¬æµ®ç»ç’ƒæ€ */
        .guide-card {
            background: #FFFFFF;
            border: 1px solid #EDF2F7;
            padding: 30px;
            border-radius: 24px; /* å¤§åœ†è§’ */
            position: relative;
            box-shadow: 0 10px 30px -5px rgba(132, 94, 194, 0.08); /* æŸ”å’Œç´«è‰²é˜´å½± */
            transition: all 0.4s cubic-bezier(0.175, 0.885, 0.32, 1.275);
            margin-bottom: 40px;
            overflow: hidden;
        }
        .guide-card::before {
            content: "";
            position: absolute;
            top: 0; left: 0; width: 100%; height: 6px;
            background: linear-gradient(90deg, #845EC2, #D65DB1, #FF9671, #FFC75F);
        }
        .guide-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 20px 40px -5px rgba(132, 94, 194, 0.15);
        }
        .guide-title {
            font-weight: 700;
            font-size: 1.1rem;
            color: #2D3748;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
        }

        /* æ–‡ä»¶ä¸Šä¼ ç»„ä»¶é‡æ„ - æç®€ç™½ */
        [data-testid='stFileUploader'] {
            background-color: #FFFFFF;
            border: 1px dashed #CBD5E0;
            border-radius: 16px;
            padding: 25px;
            transition: all 0.3s ease;
            box-shadow: 0 2px 4px rgba(0,0,0,0.02);
        }
        [data-testid='stFileUploader']:hover {
            border-color: #D65DB1; /* æ‚¬æµ®å˜ä¸ºç²‰ç´«è‰² */
            background-color: #FDFAFC;
            transform: translateY(-2px);
            box-shadow: 0 12px 24px -10px rgba(214, 93, 177, 0.2);
        }
        [data-testid='stFileUploader'] section {
            padding: 0;
        }
        /* ä¸Šä¼ æŒ‰é’®æ ·å¼è¦†ç›– */
        [data-testid='stFileUploader'] button {
            border: 1px solid #E2E8F0;
            color: #4A5568;
            font-weight: 500;
            border-radius: 8px;
            box-shadow: none;
            background: white;
        }

        /* ä¸»æŒ‰é’® (å¼€å§‹å¤„ç†) - å¼ºæ¸å˜ */
        div.stButton > button:first-child {
            background: linear-gradient(135deg, #845EC2 0%, #D65DB1 100%);
            color: white;
            border: none;
            padding: 16px 40px;
            font-size: 18px;
            border-radius: 50px; /* èƒ¶å›Šå½¢çŠ¶ */
            width: 100%;
            font-weight: 700;
            letter-spacing: 0.5px;
            box-shadow: 0 10px 20px -5px rgba(132, 94, 194, 0.4);
            transition: all 0.3s ease;
        }
        div.stButton > button:first-child:hover {
            background: linear-gradient(135deg, #956FD3 0%, #E76EC2 100%);
            box-shadow: 0 15px 30px -5px rgba(132, 94, 194, 0.5);
            transform: translateY(-3px) scale(1.02);
        }
        div.stButton > button:first-child:active {
            transform: translateY(1px);
        }

        /* ä¸‹è½½æŒ‰é’®ç¾¤ - æŸ”å’Œå¡ç‰‡é£æ ¼ */
        [data-testid="stDownloadButton"] button {
            background-color: #FFFFFF;
            color: #4A5568;
            border: 1px solid #E2E8F0;
            border-radius: 12px;
            font-weight: 600;
            padding: 10px 20px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.05);
            transition: all 0.3s;
        }
        [data-testid="stDownloadButton"] button:hover {
            background: linear-gradient(135deg, #FF9671 0%, #FFC75F 100%);
            color: white;
            border-color: transparent;
            box-shadow: 0 8px 15px -3px rgba(255, 150, 113, 0.4);
            transform: translateY(-2px);
        }

        /* çŠ¶æ€æç¤ºæ¡†ç¾åŒ– */
        .stAlert {
            background-color: #FFFFFF;
            border: none;
            border-radius: 12px;
            box-shadow: 0 4px 6px -1px rgba(0,0,0,0.05);
            border-left: 4px solid #845EC2;
        }
        
        /* è¿›åº¦æ¡é¢œè‰² */
        .stProgress > div > div > div > div {
            background: linear-gradient(90deg, #845EC2, #D65DB1, #FF9671);
        }
        </style>
    """, unsafe_allow_html=True)

# ==========================================
# ğŸ§© Mock Logic (é€»è¾‘å±‚)
# ==========================================
class MockProcessor:
    def __init__(self, raw, bench):
        self.raw = raw
        self.bench = bench
        self.merged_dfs = {}
        self.final_json = {}
        class MockDoc:
            def save(self, b): b.write(b"content")
        self.doc = MockDoc()

    def run_pipeline(self):
        time.sleep(1.5)
        self.merged_dfs = {"Overview": pd.DataFrame({'A': [1,2], 'B': [3,4]})}
        self.final_json = {"status": "success", "score": 98}

# ==========================================
# ğŸš€ ä¸»ç¨‹åº
# ==========================================
def main():
    st.set_page_config(page_title="Ad-Opt System", layout="wide", page_icon="ğŸ“Š")
    set_artistic_style()

    # --- Header ---
    c1, c2 = st.columns([0.1, 0.9])
    with c1:
        st.title("å¹¿å‘Šä¼˜åŒ–æŠ¥å‘Šæ•°æ®ç”Ÿäº§ç³»ç»Ÿ")
    
    # è‰ºæœ¯åˆ†å‰²çº¿
    st.markdown('<div class="art-divider"></div>', unsafe_allow_html=True)

    # --- Guide Section (è‡ªå®šä¹‰ HTML ç»“æ„) ---
    st.markdown("""
        <div class="guide-card">
            <div class="guide-title">âš¡ åŠŸèƒ½ä½¿ç”¨æŒ‡å— / USER GUIDE</div>
            <ul style="list-style-type: square; padding-left: 20px; color: #444; font-family: monospace;">
                <li style="margin-bottom: 8px;">è¯·ä¸Šä¼  <strong>[å‘¨æœŸæ€§å¤ç›˜æŠ¥å‘Š]</strong> ä¸ <strong>[è¡Œä¸š Benchmark]</strong> æºæ–‡ä»¶ã€‚</li>
                <li style="margin-bottom: 8px;">ç³»ç»Ÿå°†è‡ªåŠ¨æ‰§è¡Œï¼šæ•°æ®æ¸…æ´— -> ETLå¤„ç† -> å¤§æ¨¡å‹æ¶æ„ç”Ÿæˆã€‚</li>
                <li>è¾“å‡ºåŒ…å«ï¼šJSON (AIåˆ†æç”¨)ã€Excel (é€è§†ç”¨)ã€Word (å®¡æŸ¥ç”¨)ã€‚</li>
            </ul>
        </div>
    """, unsafe_allow_html=True)

    # --- Input Section ---
    col1, col2 = st.columns(2, gap="large")

    with col1:
        st.markdown('<div class="section-header"><span>1ï¸âƒ£ æ•°æ®æºè¾“å…¥</span></div>', unsafe_allow_html=True)
        f1 = st.file_uploader("ä¸Šä¼  [æ•°æ®æŠ¥è¡¨] (Excel)", type=["xlsx", "xls"], key="f1")
        if f1:
            st.markdown(f"<div style='color:#00D1FF; font-weight:bold; margin-top:5px;'>âœ” å·²åŠ è½½: {f1.name}</div>", unsafe_allow_html=True)
        else:
            st.markdown("<div style='color:#999; font-size:0.8rem; margin-top:5px;'>ç­‰å¾…ä¸Šä¼ ...</div>", unsafe_allow_html=True)

    with col2:
        st.markdown('<div class="section-header"><span>2ï¸âƒ£ è¡Œä¸šåŸºå‡†</span></div>', unsafe_allow_html=True)
        f2 = st.file_uploader("ä¸Šä¼  [è¡Œä¸š Benchmark]", type=["xlsx", "xls"], key="f2")
        if f2:
            st.markdown(f"<div style='color:#00D1FF; font-weight:bold; margin-top:5px;'>âœ” å·²åŠ è½½: {f2.name}</div>", unsafe_allow_html=True)
        else:
            st.markdown("<div style='color:#999; font-size:0.8rem; margin-top:5px;'>å¯é€‰ (è‹¥æ— åˆ™ä½¿ç”¨é»˜è®¤åŸºå‡†)</div>", unsafe_allow_html=True)

    st.markdown("<br><br>", unsafe_allow_html=True)

    # --- Action Area ---
    # å±…ä¸­å¸ƒå±€
    _, btn_col, _ = st.columns([1, 1.5, 1])
    with btn_col:
        start = st.button("ğŸš€ å¼€å§‹å¤„ç†æ•°æ® / START PROCESS")

    # --- Processing ---
    if start:
        if not f1:
            st.error("âš ï¸ ç¼ºå°‘æ ¸å¿ƒæ•°æ®æºï¼MISSING DATA SOURCE")
        else:
            processor = MockProcessor(f1, f2)
            
            # è‡ªå®šä¹‰è¿›åº¦æ¡å®¹å™¨
            with st.status("ğŸ”® æ­£åœ¨è¿›è¡Œæ•°æ®ç‚¼é‡‘æœ¯...", expanded=True) as status:
                st.write("âœ¨ æ¸…æ´—è„æ•°æ®...")
                time.sleep(0.8)
                st.write("ğŸ”¥ èåˆåŸºå‡†æŒ‡æ ‡...")
                time.sleep(0.8)
                processor.run_pipeline()
                status.update(label="âœ… å¤„ç†å®Œæˆ / COMPLETED", state="complete", expanded=False)

            st.markdown("<br>", unsafe_allow_html=True)
            st.markdown('<div class="section-header"><span>3ï¸âƒ£ ç»“æœä¸‹è½½ / DOWNLOAD</span></div>', unsafe_allow_html=True)

            d1, d2, d3 = st.columns(3, gap="medium")
            with d1:
                st.download_button("ğŸ’¾ ä¸‹è½½ JSON", data="{}", file_name="data.json", use_container_width=True)
            with d2:
                st.download_button("ğŸ“Š ä¸‹è½½ Excel", data=b"xls", file_name="data.xlsx", use_container_width=True)
            with d3:
                st.download_button("ğŸ“ ä¸‹è½½ Word", data=b"doc", file_name="report.docx", use_container_width=True)

if __name__ == "__main__":
    main()
